<h1 id="torch-quantization">torch.quantization</h1> <p id="module-torch.quantization">This module implements the functions you call directly to convert your model from FP32 to quantized form. For example the <a class="reference internal" href="#torch.quantization.prepare" title="torch.quantization.prepare"><code>prepare()</code></a> is used in post training quantization to prepares your model for the calibration step and <a class="reference internal" href="#torch.quantization.convert" title="torch.quantization.convert"><code>convert()</code></a> actually converts the weights to int8 and replaces the operations with their quantized counterparts. There are other helper functions for things like quantizing the input to your model and performing critical fusions like conv+relu.</p>  <h2 id="top-level-quantization-apis">Top-level quantization APIs</h2> <dl class="function"> <dt id="torch.quantization.quantize">
<code>torch.quantization.quantize(model, run_fn, run_args, mapping=None, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#quantize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Quantize the input float model with post training static quantization.</p> <p>First it will prepare the model for calibration, then it calls <code>run_fn</code> which will run the calibration step, after that we will convert the model to a quantized model.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – input float model</li> <li>
<strong>run_fn</strong> – a calibration function for calibrating the prepared model</li> <li>
<strong>run_args</strong> – positional arguments for <code>run_fn</code>
</li> <li>
<strong>inplace</strong> – carry out model transformations in-place, the original module is mutated</li> <li>
<strong>mapping</strong> – correspondence between original module types and quantized counterparts</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Quantized model.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.quantize_dynamic">
<code>torch.quantization.quantize_dynamic(model, qconfig_spec=None, dtype=torch.qint8, mapping=None, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#quantize_dynamic"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Converts a float model to dynamic (i.e. weights-only) quantized model.</p> <p>Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.</p> <p>For simplest usage provide <code>dtype</code> argument that can be float16 or qint8. Weight-only quantization by default is performed for layers with large weights size - i.e. Linear and RNN variants.</p> <p>Fine grained control is possible with <code>qconfig</code> and <code>mapping</code> that act similarly to <code>quantize()</code>. If <code>qconfig</code> is provided, the <code>dtype</code> argument is ignored.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – input model</li> <li>
<p><strong>qconfig_spec</strong> – </p>
<p>Either:</p> <ul> <li>A dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute). Entries in the dictionary need to be QConfigDynamic instances.</li> <li>A set of types and/or submodule names to apply dynamic quantization to, in which case the <code>dtype</code> argument is used to specify the bit-width</li> </ul> </li> <li>
<strong>inplace</strong> – carry out model transformations in-place, the original module is mutated</li> <li>
<strong>mapping</strong> – maps type of a submodule to a type of corresponding dynamically quantized version with which the submodule needs to be replaced</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.quantize_qat">
<code>torch.quantization.quantize_qat(model, run_fn, run_args, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#quantize_qat"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Do quantization aware training and output a quantized model</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – input model</li> <li>
<strong>run_fn</strong> – a function for evaluating the prepared model, can be a function that simply runs the prepared model or a training loop</li> <li>
<strong>run_args</strong> – positional arguments for <code>run_fn</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Quantized model.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.prepare">
<code>torch.quantization.prepare(model, inplace=False, allow_list=None, observer_non_leaf_module_list=None, prepare_custom_config_dict=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#prepare"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Prepares a copy of the model for quantization calibration or quantization-aware training.</p> <p>Quantization configuration should be assigned preemptively to individual submodules in <code>.qconfig</code> attribute.</p> <p>The model will be attached with observer or fake quant modules, and qconfig will be propagated.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – input model to be modified in-place</li> <li>
<strong>inplace</strong> – carry out model transformations in-place, the original module is mutated</li> <li>
<strong>allow_list</strong> – list of quantizable modules</li> <li>
<strong>observer_non_leaf_module_list</strong> – list of non-leaf modules we want to add observer</li> <li>
<strong>prepare_custom_config_dict</strong> – customization configuration dictionary for prepare function</li> </ul> </dd> </dl> <pre data-language="python"># Example of prepare_custom_config_dict:
prepare_custom_config_dict = {
    # user will manually define the corresponding observed
    # module class which has a from_float class method that converts
    # float custom module to observed custom module
    "float_to_observed_custom_module_class": {
        CustomModule: ObservedCustomModule
    }
 }
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.prepare_qat">
<code>torch.quantization.prepare_qat(model, mapping=None, inplace=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#prepare_qat"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Prepares a copy of the model for quantization calibration or quantization-aware training and converts it to quantized version.</p> <p>Quantization configuration should be assigned preemptively to individual submodules in <code>.qconfig</code> attribute.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – input model to be modified in-place</li> <li>
<strong>mapping</strong> – dictionary that maps float modules to quantized modules to be replaced.</li> <li>
<strong>inplace</strong> – carry out model transformations in-place, the original module is mutated</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.convert">
<code>torch.quantization.convert(module, mapping=None, inplace=False, remove_qconfig=True, convert_custom_config_dict=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#convert"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Converts submodules in input module to a different module according to <code>mapping</code> by calling <code>from_float</code> method on the target module class. And remove qconfig at the end if remove_qconfig is set to True.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> – prepared and calibrated module</li> <li>
<strong>mapping</strong> – a dictionary that maps from source module type to target module type, can be overwritten to allow swapping user defined Modules</li> <li>
<strong>inplace</strong> – carry out model transformations in-place, the original module is mutated</li> <li>
<strong>convert_custom_config_dict</strong> – custom configuration dictionary for convert function</li> </ul> </dd> </dl> <pre data-language="python"># Example of convert_custom_config_dict:
convert_custom_config_dict = {
    # user will manually define the corresponding quantized
    # module class which has a from_observed class method that converts
    # observed custom module to quantized custom module
    "observed_to_quantized_custom_module_class": {
        ObservedCustomModule: QuantizedCustomModule
    }
}
</pre> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.QConfig">
<code>class torch.quantization.QConfig</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/qconfig.html#QConfig"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Describes how to quantize a layer or a part of the network by providing settings (observer classes) for activations and weights respectively.</p> <p>Note that QConfig needs to contain observer <strong>classes</strong> (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization preparation function will instantiate observers multiple times for each of the layers.</p> <p>Observer classes have usually reasonable default arguments, but they can be overwritten with <code>with_args</code> method (that behaves like functools.partial):</p>  <p>my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8), weight=default_observer.with_args(dtype=torch.qint8))</p>  </dd>
</dl> <dl class="class"> <dt id="torch.quantization.QConfigDynamic">
<code>class torch.quantization.QConfigDynamic</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/qconfig.html#QConfigDynamic"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Describes how to dynamically quantize a layer or a part of the network by providing settings (observer classes) for weights.</p> <p>It’s like QConfig, but for dynamic quantization.</p> <p>Note that QConfigDynamic needs to contain observer <strong>classes</strong> (like MinMaxObserver) or a callable that returns instances on invocation, not the concrete observer instances themselves. Quantization function will instantiate observers multiple times for each of the layers.</p> <p>Observer classes have usually reasonable default arguments, but they can be overwritten with <code>with_args</code> method (that behaves like functools.partial):</p>  <p>my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))</p>  </dd>
</dl>   <h2 id="preparing-model-for-quantization">Preparing model for quantization</h2> <dl class="function"> <dt id="torch.quantization.fuse_modules">
<code>torch.quantization.fuse_modules(model, modules_to_fuse, inplace=False, fuser_func=&lt;function fuse_known_modules&gt;, fuse_custom_config_dict=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/fuse_modules.html#fuse_modules"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Fuses a list of modules into a single module</p> <p>Fuses only the following sequence of modules: conv, bn conv, bn, relu conv, relu linear, relu bn, relu All other sequences are left unchanged. For these sequences, replaces the first item in the list with the fused module, replacing the rest of the modules with identity.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – Model containing the modules to be fused</li> <li>
<strong>modules_to_fuse</strong> – list of list of module names to fuse. Can also be a list of strings if there is only a single list of modules to fuse.</li> <li>
<strong>inplace</strong> – bool specifying if fusion happens in place on the model, by default a new model is returned</li> <li>
<strong>fuser_func</strong> – Function that takes in a list of modules and outputs a list of fused modules of the same length. For example, fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()] Defaults to torch.quantization.fuse_known_modules</li> <li>
<strong>fuse_custom_config_dict</strong> – custom configuration for fusion</li> </ul> </dd> </dl> <pre data-language="python"># Example of fuse_custom_config_dict
fuse_custom_config_dict = {
    # Additional fuser_method mapping
    "additional_fuser_method_mapping": {
        (torch.nn.Conv2d, torch.nn.BatchNorm2d): fuse_conv_bn
    },
}
</pre> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>model with fused modules. A new copy is created if inplace=True.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = myModel()
&gt;&gt;&gt; # m is a module containing  the sub-modules below
&gt;&gt;&gt; modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]
&gt;&gt;&gt; fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)
&gt;&gt;&gt; output = fused_m(input)

&gt;&gt;&gt; m = myModel()
&gt;&gt;&gt; # Alternately provide a single list of modules to fuse
&gt;&gt;&gt; modules_to_fuse = ['conv1', 'bn1', 'relu1']
&gt;&gt;&gt; fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)
&gt;&gt;&gt; output = fused_m(input)
</pre> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.QuantStub">
<code>class torch.quantization.QuantStub(qconfig=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/stubs.html#QuantStub"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Quantize stub module, before calibration, this is same as an observer, it will be swapped as <code>nnq.Quantize</code> in <code>convert</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>qconfig</strong> – quantization configuration for the tensor, if qconfig is not provided, we will get qconfig from parent modules</p> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.DeQuantStub">
<code>class torch.quantization.DeQuantStub</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/stubs.html#DeQuantStub"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dequantize stub module, before calibration, this is same as identity, this will be swapped as <code>nnq.DeQuantize</code> in <code>convert</code>.</p> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.QuantWrapper">
<code>class torch.quantization.QuantWrapper(module)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/stubs.html#QuantWrapper"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A wrapper class that wraps the input module, adds QuantStub and DeQuantStub and surround the call to module with call to quant and dequant modules.</p> <p>This is used by the <code>quantization</code> utility functions to add the quant and dequant modules, before <code>convert</code> function <code>QuantStub</code> will just be observer, it observes the input tensor, after <code>convert</code>, <code>QuantStub</code> will be swapped to <code>nnq.Quantize</code> which does actual quantization. Similarly for <code>DeQuantStub</code>.</p> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.add_quant_dequant">
<code>torch.quantization.add_quant_dequant(module)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#add_quant_dequant"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrap the leaf child module in QuantWrapper if it has a valid qconfig Note that this function will modify the children of module inplace and it can return a new module which wraps the input module as well.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> – input module with qconfig attributes for all the leaf modules</li> <li>
<strong>we want to quantize</strong> (<em>that</em>) – </li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Either the inplace modified module with submodules wrapped in <code>QuantWrapper</code> based on qconfig or a new <code>QuantWrapper</code> module which wraps the input module, the latter case only happens when the input module is a leaf module and we want to quantize it.</p> </dd> </dl> </dd>
</dl>   <h2 id="utility-functions">Utility functions</h2> <dl class="function"> <dt id="torch.quantization.add_observer_">
<code>torch.quantization.add_observer_(module, qconfig_propagation_list=None, non_leaf_module_list=None, device=None, custom_module_class_mapping=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#add_observer_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Add observer for the leaf child of the module.</p> <p>This function insert observer module to all leaf child module that has a valid qconfig attribute.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> – input module with qconfig attributes for all the leaf modules that we want to quantize</li> <li>
<strong>device</strong> – parent device, if any</li> <li>
<strong>non_leaf_module_list</strong> – list of non-leaf modules we want to add observer</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>None, module is modified inplace with added observer modules and forward_hooks</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.swap_module">
<code>torch.quantization.swap_module(mod, mapping, custom_module_class_mapping)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#swap_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Swaps the module if it has a quantized counterpart and it has an <code>observer</code> attached.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mod</strong> – input module</li> <li>
<strong>mapping</strong> – a dictionary that maps from nn module to nnq module</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>The corresponding quantized module of <code>mod</code></p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.propagate_qconfig_">
<code>torch.quantization.propagate_qconfig_(module, qconfig_dict=None, allow_list=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#propagate_qconfig_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Propagate qconfig through the module hierarchy and assign <code>qconfig</code> attribute on each leaf module</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> – input module</li> <li>
<strong>qconfig_dict</strong> – dictionary that maps from name or type of submodule to quantization configuration, qconfig applies to all submodules of a given module unless qconfig for the submodules are specified (when the submodule already has qconfig attribute)</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>None, module is modified inplace with qconfig attached</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.quantization.default_eval_fn">
<code>torch.quantization.default_eval_fn(model, calib_data)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization.html#default_eval_fn"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Default evaluation function takes a torch.utils.data.Dataset or a list of input Tensors and run the model on the dataset</p> </dd>
</dl>   <h2 id="observers">Observers</h2> <dl class="class"> <dt id="torch.quantization.ObserverBase">
<code>class torch.quantization.ObserverBase(dtype)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#ObserverBase"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Base observer Module. Any observer implementation should derive from this class.</p> <p>Concrete observers should follow the same API. In forward, they will update the statistics of the observed Tensor. And they should provide a <code>calculate_qparams</code> function that computes the quantization parameters given the collected statistics.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>dtype</strong> – Quantized data type</p> </dd> </dl> <dl class="method"> <dt id="torch.quantization.ObserverBase.with_args">
<code>classmethod with_args(**kwargs)</code> </dt> <dd>
<p>Wrapper that allows creation of class factories.</p> <p>This can be useful when there is a need to create classes with the same constructor arguments, but different instances.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; Foo.with_args = classmethod(_with_args)
&gt;&gt;&gt; foo_builder = Foo.with_args(a=3, b=4).with_args(answer=42)
&gt;&gt;&gt; foo_instance1 = foo_builder()
&gt;&gt;&gt; foo_instance2 = foo_builder()
&gt;&gt;&gt; id(foo_instance1) == id(foo_instance2)
False
</pre> </dd>
</dl> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.MinMaxObserver">
<code>class torch.quantization.MinMaxObserver(dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#MinMaxObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Observer module for computing the quantization parameters based on the running min and max values.</p> <p>This observer uses the tensor min/max statistics to compute the quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> <li>
<strong>quant_min</strong> – Minimum quantization value. If unspecified, it will follow the 8-bit setup.</li> <li>
<strong>quant_max</strong> – Maximum quantization value. If unspecified, it will follow the 8-bit setup.</li> </ul> </dd> </dl> <p>Given running min/max as <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext>min</mtext></msub></mrow><annotation encoding="application/x-tex">x_\text{min}</annotation></semantics></math></span></span> </span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">x_\text{max}</annotation></semantics></math></span></span> </span>, scale <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> </span> and zero point <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span></span> </span> are computed as:</p> <p>The running minimum/maximum <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext>min/max</mtext></msub></mrow><annotation encoding="application/x-tex">x_\text{min/max}</annotation></semantics></math></span></span> </span> is computed as:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mtext>min</mtext></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><msub><mi>x</mi><mtext>min</mtext></msub><mo>=</mo><mtext>None</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>min</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo separator="true">,</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mtext>max</mtext></msub></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><msub><mi>x</mi><mtext>max</mtext></msub><mo>=</mo><mtext>None</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>max</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo separator="true">,</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo><mo fence="true">)</mo></mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{ll} x_\text{min} &amp;= \begin{cases} \min(X) &amp; \text{if~}x_\text{min} = \text{None} \\ \min\left(x_\text{min}, \min(X)\right) &amp; \text{otherwise} \end{cases}\\ x_\text{max} &amp;= \begin{cases} \max(X) &amp; \text{if~}x_\text{max} = \text{None} \\ \max\left(x_\text{max}, \max(X)\right) &amp; \text{otherwise} \end{cases}\\ \end{array}</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> is the observed tensor.</p> <p>The scale <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> </span> and zero point <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>z</mi></mrow><annotation encoding="application/x-tex">z</annotation></semantics></math></span></span> </span> are then computed as:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right left" columnspacing="0em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>if Symmetric:</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>s</mi><mo>=</mo><mn>2</mn><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">∣</mi><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">∣</mi><mo separator="true">,</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>z</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>if dtype is qint8</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>128</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mtext>Otherwise:</mtext></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>s</mi><mo>=</mo><mrow><mo fence="true">(</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow><mi mathvariant="normal">/</mi><mrow><mo fence="true">(</mo><msub><mi>Q</mi><mtext>max</mtext></msub><mo>−</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo fence="true">)</mo></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>z</mi><mo>=</mo><msub><mi>Q</mi><mtext>min</mtext></msub><mo>−</mo><mtext>round</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mtext>min</mtext></msub><mi mathvariant="normal">/</mi><mi>s</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} \text{if Symmetric:}&amp;\\ &amp;s = 2 \max(|x_\text{min}|, x_\text{max}) / \left( Q_\text{max} - Q_\text{min} \right) \\ &amp;z = \begin{cases} 0 &amp; \text{if dtype is qint8} \\ 128 &amp; \text{otherwise} \end{cases}\\ \text{Otherwise:}&amp;\\ &amp;s = \left( x_\text{max} - x_\text{min} \right ) / \left( Q_\text{max} - Q_\text{min} \right ) \\ &amp;z = Q_\text{min} - \text{round}(x_\text{min} / s) \end{aligned}</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>min</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{min}</annotation></semantics></math></span></span> </span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Q</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">Q_\text{max}</annotation></semantics></math></span></span> </span> are the minimum and maximum of the quantized data type.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Only works with <code>torch.per_tensor_symmetric</code> quantization scheme</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>dtype</code> can only take <code>torch.qint8</code> or <code>torch.quint8</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.</p> </div> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.MovingAverageMinMaxObserver">
<code>class torch.quantization.MovingAverageMinMaxObserver(averaging_constant=0.01, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False, quant_min=None, quant_max=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#MovingAverageMinMaxObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Observer module for computing the quantization parameters based on the moving average of the min and max values.</p> <p>This observer computes the quantization parameters based on the moving averages of minimums and maximums of the incoming tensors. The module records the average minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>averaging_constant</strong> – Averaging constant for min/max.</li> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> <li>
<strong>quant_min</strong> – Minimum quantization value. If unspecified, it will follow the 8-bit setup.</li> <li>
<strong>quant_max</strong> – Maximum quantization value. If unspecified, it will follow the 8-bit setup.</li> </ul> </dd> </dl> <p>The moving average min/max is computed as follows</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.15999999999999992em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>x</mi><mtext>min</mtext></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><msub><mi>x</mi><mtext>min</mtext></msub><mo>=</mo><mtext>None</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo><msub><mi>x</mi><mtext>min</mtext></msub><mo>+</mo><mi>c</mi><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><msub><mi>x</mi><mtext>max</mtext></msub><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.3599999999999999em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if </mtext><msub><mi>x</mi><mtext>max</mtext></msub><mo>=</mo><mtext>None</mtext></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>c</mi><mo stretchy="false">)</mo><msub><mi>x</mi><mtext>max</mtext></msub><mo>+</mo><mi>c</mi><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mtext>otherwise</mtext></mstyle></mtd></mtr></mtable></mrow></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{array}{ll} x_\text{min} = \begin{cases} \min(X) &amp; \text{if~}x_\text{min} = \text{None} \\ (1 - c) x_\text{min} + c \min(X) &amp; \text{otherwise} \end{cases}\\ x_\text{max} = \begin{cases} \max(X) &amp; \text{if~}x_\text{max} = \text{None} \\ (1 - c) x_\text{max} + c \max(X) &amp; \text{otherwise} \end{cases}\\ \end{array}</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mtext>min/max</mtext></msub></mrow><annotation encoding="application/x-tex">x_\text{min/max}</annotation></semantics></math></span></span> </span> is the running average min/max, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> is is the incoming tensor, and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span></span> </span> is the <code>averaging_constant</code>.</p> <p>The scale and zero point are then computed as in <code>MinMaxObserver</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only works with <code>torch.per_tensor_affine</code> quantization scheme.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the running minimum equals to the running maximum, the scale and zero_point are set to 1.0 and 0.</p> </div> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.PerChannelMinMaxObserver">
<code>class torch.quantization.PerChannelMinMaxObserver(ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#PerChannelMinMaxObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p> <p>This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>ch_axis</strong> – Channel axis</li> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> <li>
<strong>quant_min</strong> – Minimum quantization value. If unspecified, it will follow the 8-bit setup.</li> <li>
<strong>quant_max</strong> – Maximum quantization value. If unspecified, it will follow the 8-bit setup.</li> </ul> </dd> </dl> <p>The quantization parameters are computed the same way as in <code>MinMaxObserver</code>, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.</p> </div> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.MovingAveragePerChannelMinMaxObserver">
<code>class torch.quantization.MovingAveragePerChannelMinMaxObserver(averaging_constant=0.01, ch_axis=0, dtype=torch.quint8, qscheme=torch.per_channel_affine, reduce_range=False, quant_min=None, quant_max=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#MovingAveragePerChannelMinMaxObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Observer module for computing the quantization parameters based on the running per channel min and max values.</p> <p>This observer uses the tensor min/max statistics to compute the per channel quantization parameters. The module records the running minimum and maximum of incoming tensors, and uses this statistic to compute the quantization parameters.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>averaging_constant</strong> – Averaging constant for min/max.</li> <li>
<strong>ch_axis</strong> – Channel axis</li> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> <li>
<strong>quant_min</strong> – Minimum quantization value. If unspecified, it will follow the 8-bit setup.</li> <li>
<strong>quant_max</strong> – Maximum quantization value. If unspecified, it will follow the 8-bit setup.</li> </ul> </dd> </dl> <p>The quantization parameters are computed the same way as in <code>MovingAverageMinMaxObserver</code>, with the difference that the running min/max values are stored per channel. Scales and zero points are thus computed per channel as well.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the running minimum equals to the running maximum, the scales and zero_points are set to 1.0 and 0.</p> </div> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.HistogramObserver">
<code>class torch.quantization.HistogramObserver(bins=2048, upsample_rate=128, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#HistogramObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The module records the running histogram of tensor values along with min/max values. <code>calculate_qparams</code> will calculate scale and zero_point.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>bins</strong> – Number of bins to use for the histogram</li> <li>
<strong>upsample_rate</strong> – Factor by which the histograms are upsampled, this is used to interpolate histograms with varying ranges across observations</li> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> </ul> </dd> </dl> <p>The scale and zero point are computed as follows:</p> <ol class="arabic simple"> <li>
<dl class="simple"> <dt>Create the histogram of the incoming inputs.</dt>
<dd>
<p>The histogram is computed continuously, and the ranges per bin change with every new tensor observed.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>Search the distribution in the histogram for optimal min/max values.</dt>
<dd>
<p>The search for the min/max values ensures the minimization of the quantization error with respect to the floating point model.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>Compute the scale and zero point the same way as in the</dt>
<dd>
<p><a class="reference internal" href="#torch.quantization.MinMaxObserver" title="torch.quantization.MinMaxObserver"><code>MinMaxObserver</code></a></p> </dd> </dl> </li> </ol> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.FakeQuantize">
<code>class torch.quantization.FakeQuantize(observer=&lt;class 'torch.quantization.observer.MovingAverageMinMaxObserver'&gt;, quant_min=0, quant_max=255, **observer_kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/fake_quantize.html#FakeQuantize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Simulate the quantize and dequantize operations in training time. The output of this module is given by</p> <p>x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale</p> <ul class="simple"> <li>
<code>scale</code> defines the scale factor used for quantization.</li> <li>
<code>zero_point</code> specifies the quantized value to which 0 in floating point maps to</li> <li>
<code>quant_min</code> specifies the minimum allowable quantized value.</li> <li>
<code>quant_max</code> specifies the maximum allowable quantized value.</li> <li>
<code>fake_quant_enable</code> controls the application of fake quantization on tensors, note that statistics can still be updated.</li> <li>
<code>observer_enable</code> controls statistics collection on tensors</li> <li>
<dl class="simple"> <dt>
<code>dtype specifies the quantized dtype that is being emulated with fake-quantization,</code> </dt>
<dd>
<p>allowable values are torch.qint8 and torch.quint8. The values of quant_min and quant_max should be chosen to be consistent with the dtype</p> </dd> </dl> </li> </ul> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>observer</strong> (<em>module</em>) – Module for observing statistics on input tensors and calculating scale and zero-point.</li> <li>
<strong>quant_min</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – The minimum allowable quantized value.</li> <li>
<strong>quant_max</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – The maximum allowable quantized value.</li> <li>
<strong>observer_kwargs</strong> (<em>optional</em>) – Arguments for the observer module</li> </ul> </dd> <dt class="field-even">Variables</dt> <dd class="field-even">
<p><strong>~FakeQuantize.observer</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – User provided module that collects statistics on the input tensor and provides a method to calculate scale and zero-point.</p> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.NoopObserver">
<code>class torch.quantization.NoopObserver(dtype=torch.float16, custom_op_name='')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#NoopObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Observer that doesn’t do anything and just passes its configuration to the quantized module’s <code>.from_float()</code>.</p> <p>Primarily used for quantization to float16 which doesn’t require determining ranges.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>custom_op_name</strong> – (temporary) specify this observer for an operator that doesn’t require any observation (Can be used in Graph Mode Passes for special case ops).</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="debugging-utilities">Debugging utilities</h2> <dl class="function"> <dt id="torch.quantization.get_observer_dict">
<code>torch.quantization.get_observer_dict(mod, target_dict, prefix='')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/quantize.html#get_observer_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Traverse the modules and save all observers into dict. This is mainly used for quantization accuracy debug :param mod: the top module we want to save all observers :param prefix: the prefix for the current module :param target_dict: the dictionary used to save all the observers</p> </dd>
</dl> <dl class="class"> <dt id="torch.quantization.RecordingObserver">
<code>class torch.quantization.RecordingObserver(**kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/quantization/observer.html#RecordingObserver"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The module is mainly for debug and records the tensor values during runtime.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dtype</strong> – Quantized data type</li> <li>
<strong>qscheme</strong> – Quantization scheme to be used</li> <li>
<strong>reduce_range</strong> – Reduces the range of the quantized data type by 1 bit</li> </ul> </dd> </dl> </dd>
</dl> <table class="longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="torch.nn.intrinsic#module-torch.nn.intrinsic" title="torch.nn.intrinsic"><code>nn.intrinsic</code></a></p></td> <td></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/torch.quantization.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/torch.quantization.html</a>
  </p>
</div>
