<h1 id="multiheadattention">MultiheadAttention</h1> <dl class="class"> <dt id="torch.nn.MultiheadAttention">
<code>class torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=0.0, bias=True, add_bias_kv=False, add_zero_attn=False, kdim=None, vdim=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/activation.html#MultiheadAttention"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Allows the model to jointly attend to information from different representation subspaces. See <a class="reference external" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">\text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O </annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mi>e</mi><mi>a</mi><msub><mi>d</mi><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)</annotation></semantics></math></span></span> </span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>embed_dim</strong> – total dimension of the model.</li> <li>
<strong>num_heads</strong> – parallel attention heads.</li> <li>
<strong>dropout</strong> – a Dropout layer on attn_output_weights. Default: 0.0.</li> <li>
<strong>bias</strong> – add bias as module parameter. Default: True.</li> <li>
<strong>add_bias_kv</strong> – add bias to the key and value sequences at dim=0.</li> <li>
<strong>add_zero_attn</strong> – add a new batch of zeros to the key and value sequences at dim=1.</li> <li>
<strong>kdim</strong> – total number of features in key. Default: None.</li> <li>
<strong>vdim</strong> – total number of features in value. Default: None.</li> </ul> </dd> </dl> <p>Note that if <code>kdim</code> and <code>vdim</code> are None, they will be set to <code>embed_dim</code> such that query, key, and value have the same number of features.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
&gt;&gt;&gt; attn_output, attn_output_weights = multihead_attn(query, key, value)
</pre> <dl class="method"> <dt id="torch.nn.MultiheadAttention.forward">
<code>forward(query, key, value, key_padding_mask=None, need_weights=True, attn_mask=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/activation.html#MultiheadAttention.forward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>key, value</strong> (<em>query</em><em>,</em>) – map a query and a set of key-value pairs to an output. See “Attention Is All You Need” for more details.</li> <li>
<strong>key_padding_mask</strong> – if provided, specified padding elements in the key will be ignored by the attention. When given a binary mask and a value is True, the corresponding value on the attention layer will be ignored. When given a byte mask and a value is non-zero, the corresponding value on the attention layer will be ignored</li> <li>
<strong>need_weights</strong> – output attn_output_weights.</li> <li>
<strong>attn_mask</strong> – 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all the batches while a 3D mask allows to specify a different mask for the entries of each batch.</li> </ul> </dd> </dl> <dl> <dt>Shapes for inputs:</dt>
<dd>
<ul> <li>query: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, N, E)</annotation></semantics></math></span></span> </span> where L is the target sequence length, N is the batch size, E is the embedding dimension.</li> <li>key: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, N, E)</annotation></semantics></math></span></span> </span>, where S is the source sequence length, N is the batch size, E is the embedding dimension.</li> <li>value: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, N, E)</annotation></semantics></math></span></span> </span> where S is the source sequence length, N is the batch size, E is the embedding dimension.</li> <li>key_padding_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span> </span> where N is the batch size, S is the source sequence length. If a ByteTensor is provided, the non-zero positions will be ignored while the position with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.</li> <li>
<p>attn_mask: if a 2D mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, S)</annotation></semantics></math></span></span> </span> where L is the target sequence length, S is the source sequence length.</p> <p>If a 3D mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>⋅</mo><mtext>num_heads</mtext><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N\cdot\text{num\_heads}, L, S)</annotation></semantics></math></span></span> </span> where N is the batch size, L is the target sequence length, S is the source sequence length. <code>attn_mask</code> ensure that position i is allowed to attend the unmasked positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend while the zero positions will be unchanged. If a BoolTensor is provided, positions with <code>True</code> is not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight.</p> </li> </ul> </dd> <dt>Shapes for outputs:</dt>
<dd>
<ul class="simple"> <li>attn_output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>L</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(L, N, E)</annotation></semantics></math></span></span> </span> where L is the target sequence length, N is the batch size, E is the embedding dimension.</li> <li>attn_output_weights: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>L</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, L, S)</annotation></semantics></math></span></span> </span> where N is the batch size, L is the target sequence length, S is the source sequence length.</li> </ul> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.MultiheadAttention.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.nn.MultiheadAttention.html</a>
  </p>
</div>
