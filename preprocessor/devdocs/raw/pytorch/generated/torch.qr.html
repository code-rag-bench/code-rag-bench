<h1 id="torch-qr">torch.qr</h1> <dl class="function"> <dt id="torch.qr">
<code>torch.qr(input, some=True, *, out=None) -&gt; (Tensor, Tensor)</code> </dt> <dd>
<p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>, and returns a namedtuple (Q, R) of tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span></span> </span> with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span> </span> being an orthogonal matrix or batch of orthogonal matrices and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span></span> </span> being an upper triangular matrix or batch of upper triangular matrices.</p> <p>If <code>some</code> is <code>True</code>, then this function returns the thin (reduced) QR factorization. Otherwise, if <code>some</code> is <code>False</code>, this function returns the complete QR factorization.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>torch.qr</code> is deprecated. Please use <a class="reference internal" href="../linalg#torch.linalg.qr" title="torch.linalg.qr"><code>torch.linalg.qr()</code></a> instead.</p> <p><strong>Differences with</strong> <code>torch.linalg.qr</code>:</p> <ul class="simple"> <li>
<p><code>torch.linalg.qr</code> takes a string parameter <code>mode</code> instead of <code>some</code>:</p> <ul> <li>
<code>some=True</code> is equivalent of <code>mode='reduced'</code>: both are the default</li> <li>
<code>some=False</code> is equivalent of <code>mode='complete'</code>.</li> </ul> </li> </ul> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If you plan to backpropagate through QR, note that the current backward implementation is only well-defined when the first <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">.</mi><mi>s</mi><mi>i</mi><mi>z</mi><mi>e</mi><mo stretchy="false">(</mo><mo>−</mo><mn>2</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\min(input.size(-1), input.size(-2))</annotation></semantics></math></span></span> </span> columns of <code>input</code> are linearly independent. This behavior will propably change once QR supports pivoting.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function uses LAPACK for CPU inputs and MAGMA for CUDA inputs, and may produce different (valid) decompositions on different device types or different platforms.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span> where <code>*</code> is zero or more batch dimensions consisting of matrices of dimension <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>×</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">m \times n</annotation></semantics></math></span></span> </span>.</li> <li>
<p><strong>some</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – </p>
<p>Set to <code>True</code> for reduced QR decomposition and <code>False</code> for complete QR decomposition. If <code>k = min(m, n)</code> then:</p>  <ul> <li>
<code>some=True</code> : returns <code>(Q, R)</code> with dimensions (m, k), (k, n) (default)</li> <li>
<code>'some=False'</code>: returns <code>(Q, R)</code> with dimensions (m, m), (m, n)</li> </ul>  </li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – tuple of <code>Q</code> and <code>R</code> tensors. The dimensions of <code>Q</code> and <code>R</code> are detailed in the description of <code>some</code> above.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.tensor([[12., -51, 4], [6, 167, -68], [-4, 24, -41]])
&gt;&gt;&gt; q, r = torch.qr(a)
&gt;&gt;&gt; q
tensor([[-0.8571,  0.3943,  0.3314],
        [-0.4286, -0.9029, -0.0343],
        [ 0.2857, -0.1714,  0.9429]])
&gt;&gt;&gt; r
tensor([[ -14.0000,  -21.0000,   14.0000],
        [   0.0000, -175.0000,   70.0000],
        [   0.0000,    0.0000,  -35.0000]])
&gt;&gt;&gt; torch.mm(q, r).round()
tensor([[  12.,  -51.,    4.],
        [   6.,  167.,  -68.],
        [  -4.,   24.,  -41.]])
&gt;&gt;&gt; torch.mm(q.t(), q).round()
tensor([[ 1.,  0.,  0.],
        [ 0.,  1., -0.],
        [ 0., -0.,  1.]])
&gt;&gt;&gt; a = torch.randn(3, 4, 5)
&gt;&gt;&gt; q, r = torch.qr(a, some=False)
&gt;&gt;&gt; torch.allclose(torch.matmul(q, r), a)
True
&gt;&gt;&gt; torch.allclose(torch.matmul(q.transpose(-2, -1), q), torch.eye(5))
True
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.qr.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.qr.html</a>
  </p>
</div>
