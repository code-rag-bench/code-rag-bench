<h1 id="module">Module</h1> <dl class="class"> <dt id="torch.nn.Module">
<code>class torch.nn.Module</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Base class for all neural network modules.</p> <p>Your models should also subclass this class.</p> <p>Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes:</p> <pre data-language="python">import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</pre> <p>Submodules assigned in this way will be registered, and will have their parameters converted too when you call <a class="reference internal" href="#torch.nn.Module.to" title="torch.nn.Module.to"><code>to()</code></a>, etc.</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<p><strong>training</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – Boolean represents whether this module is in training or evaluation mode.</p> </dd> </dl> <dl class="method"> <dt id="torch.nn.Module.add_module">
<code>add_module(name, module)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.add_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the child module. The child module can be accessed from this module using the given name</li> <li>
<strong>module</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a>) – child module to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.apply">
<code>apply(fn)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.apply"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a class="reference internal" href="../nn.init#nn-init-doc"><span class="std std-ref">torch.nn.init</span></a>).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>fn</strong> (<a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> -&gt; None) – function to be applied to each submodule</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.bfloat16">
<code>bfloat16()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.bfloat16"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.buffers">
<code>buffers(recurse=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.buffers"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over module buffers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>torch.Tensor</em> – module buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.children">
<code>children()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.children"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over immediate children modules.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a child module</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.cpu">
<code>cpu()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.cpu"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Moves all model parameters and buffers to the CPU.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.cuda">
<code>cuda(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.cuda"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.double">
<code>double()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.double"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="attribute"> <dt id="torch.nn.Module.dump_patches">
<code>dump_patches: bool = False</code> </dt> <dd>
<p>This allows better BC support for <a class="reference internal" href="#torch.nn.Module.load_state_dict" title="torch.nn.Module.load_state_dict"><code>load_state_dict()</code></a>. In <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a>, the version number will be saved as in the attribute <code>_metadata</code> of the returned state dict, and thus pickled. <code>_metadata</code> is a dictionary with keys that follow the naming convention of state dict. See <code>_load_from_state_dict</code> on how to use this information in loading.</p> <p>If new parameters/buffers are added/removed from a module, this number shall be bumped, and the module’s <code>_load_from_state_dict</code> method can compare the version number and do appropriate changes if the state dict is from before the change.</p> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.eval">
<code>eval()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.eval"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <p>This is equivalent with <a class="reference internal" href="#torch.nn.Module.train" title="torch.nn.Module.train"><code>self.train(False)</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.extra_repr">
<code>extra_repr()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.extra_repr"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the extra representation of the module</p> <p>To print customized extra information, you should re-implement this method in your own modules. Both single-line and multi-line strings are acceptable.</p> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.float">
<code>float()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts all floating point parameters and buffers to float datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.forward">
<code>forward(*input)</code> </dt> <dd>
<p>Defines the computation performed at every call.</p> <p>Should be overridden by all subclasses.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Although the recipe for forward pass needs to be defined within this function, one should call the <a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them.</p> </div> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.half">
<code>half()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.half"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.load_state_dict">
<code>load_state_dict(state_dict, strict=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.load_state_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> into this module and its descendants. If <code>strict</code> is <code>True</code>, then the keys of <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> must exactly match the keys returned by this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a>) – a dict containing parameters and persistent buffers.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys in <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a> match the keys returned by this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function. Default: <code>True</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<ul class="simple"> <li>
<strong>missing_keys</strong> is a list of str containing the missing keys</li> <li>
<strong>unexpected_keys</strong> is a list of str containing the unexpected keys</li> </ul> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.modules">
<code>modules()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.modules"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over all modules in the network.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a module in the network</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.named_buffers">
<code>named_buffers(prefix='', recurse=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.named_buffers"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all buffer names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.named_children">
<code>named_children()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.named_children"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple containing a name and child module</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.named_modules">
<code>named_modules(memo=None, prefix='')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.named_modules"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple of name and module</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.named_parameters">
<code>named_parameters(prefix='', recurse=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.named_parameters"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all parameter names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.parameters">
<code>parameters(recurse=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.parameters"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>Parameter</em> – module parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_backward_hook">
<code>register_backward_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_backward_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of <code>nn.Module.register_full_backward_hook()</code> and the behavior of this function will change in future versions.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_buffer">
<code>register_buffer(name, tensor, persistent=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_buffer"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s <code>running_mean</code> is not a parameter, but is part of the module’s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a>.</p> <p>Buffers can be accessed as attributes using given names.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed from this module using the given name</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – buffer to be registered.</li> <li>
<strong>persistent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether the buffer is part of this module’s <a class="reference internal" href="#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict</code></a>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_forward_hook">
<code>register_forward_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_forward_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a forward hook on the module.</p> <p>The hook will be called every time after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> has computed an output. It should have the following signature:</p> <pre data-language="python">hook(module, input, output) -&gt; None or modified output
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> is called.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_forward_pre_hook">
<code>register_forward_pre_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_forward_pre_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before <a class="reference internal" href="#torch.nn.Module.forward" title="torch.nn.Module.forward"><code>forward()</code></a> is invoked. It should have the following signature:</p> <pre data-language="python">hook(module, input) -&gt; None or modified input
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_full_backward_hook">
<code>register_full_backward_hook(hook)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_full_backward_hook"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</p> <pre data-language="python">hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</pre> <p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.register_parameter">
<code>register_parameter(name, param)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.register_parameter"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed from this module using the given name</li> <li>
<strong>param</strong> (<a class="reference internal" href="torch.nn.parameter.parameter#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter">Parameter</a>) – parameter to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.requires_grad_">
<code>requires_grad_(requires_grad=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.requires_grad_"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters’ <code>requires_grad</code> attributes in-place.</p> <p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether autograd should record operations on parameters in this module. Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.state_dict">
<code>state_dict(destination=None, prefix='', keep_vars=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.state_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a dictionary containing a whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a dictionary containing a whole state of the module</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.to">
<code>to(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as</p> <dl class="function"> <dt>
<code>to(device=None, dtype=None, non_blocking=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="function"> <dt>
<code>to(dtype, non_blocking=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="function"> <dt>
<code>to(tensor, non_blocking=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="function"> <dt>
<code>to(memory_format=torch.channels_last)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.to"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <p>Its signature is similar to <a class="reference internal" href="../tensors#torch.Tensor.to" title="torch.Tensor.to"><code>torch.Tensor.to()</code></a>, but only accepts floating point or complex <code>dtype`s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:`dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p> <p>See below for examples.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This method modifies the module in-place.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<code>torch.device</code>) – the desired device of the parameters and buffers in this module</li> <li>
<strong>dtype</strong> (<code>torch.dtype</code>) – the desired floating point or complex dtype of the parameters and buffers in this module</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li> <li>
<strong>memory_format</strong> (<code>torch.memory_format</code>) – the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.train">
<code>train(mode=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.train"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.type">
<code>type(dst_type)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.type"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Casts all parameters and buffers to <code>dst_type</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.9)">type</a><em> or </em><em>string</em>) – the desired type</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.xpu">
<code>xpu(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.xpu"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Module.zero_grad">
<code>zero_grad(set_to_none=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/module.html#Module.zero_grad"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets gradients of all model parameters to zero. See similar function under <a class="reference internal" href="../optim#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>torch.optim.Optimizer</code></a> for more context.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – instead of setting to zero, set the grads to None. See <a class="reference internal" href="../optim#torch.optim.Optimizer.zero_grad" title="torch.optim.Optimizer.zero_grad"><code>torch.optim.Optimizer.zero_grad()</code></a> for details.</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.Module.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.nn.Module.html</a>
  </p>
</div>
