<h1 id="torch-lobpcg">torch.lobpcg</h1> <dl class="function"> <dt id="torch.lobpcg">
<code>torch.lobpcg(A, k=None, B=None, X=None, n=None, iK=None, niter=None, tol=None, largest=None, method=None, tracker=None, ortho_iparams=None, ortho_fparams=None, ortho_bparams=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/_lobpcg.html#lobpcg"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive defined generalized eigenvalue problem using matrix-free LOBPCG methods.</p> <p>This function is a front-end to the following LOBPCG algorithms selectable via <code>method</code> argument:</p>  <p><code>method=”basic”</code> - the LOBPCG method introduced by Andrew Knyazev, see [Knyazev2001]. A less robust method, may fail when Cholesky is applied to singular input.</p> <p><code>method=”ortho”</code> - the LOBPCG method with orthogonal basis selection [StathopoulosEtal2002]. A robust method.</p>  <p>Supported inputs are dense, sparse, and batches of dense matrices.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In general, the basic method spends least time per iteration. However, the robust methods converge much faster and are more stable. So, the usage of the basic method is generally not recommended but there exist cases where the usage of the basic method may be preferred.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The backward method does not support sparse and complex inputs. It works only when <code>B</code> is not provided (i.e. <code>B == None</code>). We are actively working on extensions, and the details of the algorithms are going to be published promptly.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>While it is assumed that <code>A</code> is symmetric, <code>A.grad</code> is not. To make sure that <code>A.grad</code> is symmetric, so that <code>A - t * A.grad</code> is symmetric in first-order optimization routines, prior to running <code>lobpcg</code> we do the following symmetrization map: <code>A -&gt; (A + A.t()) / 2</code>. The map is performed only when the <code>A</code> requires gradients.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, m)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>B</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, m)</annotation></semantics></math></span></span> </span>. When not specified, <code>B</code> is interpereted as identity matrix.</li> <li>
<strong>X</strong> (<em>tensor</em><em>, </em><em>optional</em>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span> where <code>k &lt;= n &lt;= m</code>. When specified, it is used as initial approximation of eigenvectors. X must be a dense tensor.</li> <li>
<strong>iK</strong> (<em>tensor</em><em>, </em><em>optional</em>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, m)</annotation></semantics></math></span></span> </span>. When specified, it will be used as preconditioner.</li> <li>
<strong>k</strong> (<em>integer</em><em>, </em><em>optional</em>) – the number of requested eigenpairs. Default is the number of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> columns (when specified) or <code>1</code>.</li> <li>
<strong>n</strong> (<em>integer</em><em>, </em><em>optional</em>) – if <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> is not specified then <code>n</code> specifies the size of the generated random approximation of eigenvectors. Default value for <code>n</code> is <code>k</code>. If <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> is specified, the value of <code>n</code> (when specified) must be the number of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span></span> </span> columns.</li> <li>
<strong>tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em>, </em><em>optional</em>) – residual tolerance for stopping criterion. Default is <code>feps ** 0.5</code> where <code>feps</code> is smallest non-zero floating-point number of the given input tensor <code>A</code> data type.</li> <li>
<strong>largest</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – when True, solve the eigenproblem for the largest eigenvalues. Otherwise, solve the eigenproblem for smallest eigenvalues. Default is <code>True</code>.</li> <li>
<strong>method</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>, </em><em>optional</em>) – select LOBPCG method. See the description of the function above. Default is “ortho”.</li> <li>
<strong>niter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – maximum number of iterations. When reached, the iteration process is hard-stopped and the current approximation of eigenpairs is returned. For infinite iteration but until convergence criteria is met, use <code>-1</code>.</li> <li>
<p><strong>tracker</strong> (<em>callable</em><em>, </em><em>optional</em>) – </p>
<p>a function for tracing the iteration process. When specified, it is called at each iteration step with LOBPCG instance as an argument. The LOBPCG instance holds the full state of the iteration process in the following attributes:</p>  <p><code>iparams</code>, <code>fparams</code>, <code>bparams</code> - dictionaries of integer, float, and boolean valued input parameters, respectively</p> <p><code>ivars</code>, <code>fvars</code>, <code>bvars</code>, <code>tvars</code> - dictionaries of integer, float, boolean, and Tensor valued iteration variables, respectively.</p> <p><code>A</code>, <code>B</code>, <code>iK</code> - input Tensor arguments.</p> <p><code>E</code>, <code>X</code>, <code>S</code>, <code>R</code> - iteration Tensor variables.</p>  <p>For instance:</p>  <p><code>ivars[“istep”]</code> - the current iteration step <code>X</code> - the current approximation of eigenvectors <code>E</code> - the current approximation of eigenvalues <code>R</code> - the current residual <code>ivars[“converged_count”]</code> - the current number of converged eigenpairs <code>tvars[“rerr”]</code> - the current state of convergence criteria</p>  <p>Note that when <code>tracker</code> stores Tensor objects from the LOBPCG instance, it must make copies of these.</p> <p>If <code>tracker</code> sets <code>bvars[“force_stop”] = True</code>, the iteration process will be hard-stopped.</p> </li> <li>
<strong>ortho_fparams, ortho_bparams</strong> (<em>ortho_iparams</em><em>,</em>) – various parameters to LOBPCG algorithm when using <code>method=”ortho”</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<p>tensor of eigenvalues of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, k)</annotation></semantics></math></span></span> </span></p> <p>X (Tensor): tensor of eigenvectors of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, k)</annotation></semantics></math></span></span> </span></p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>E (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>)</p> </dd> </dl> <h4 class="rubric">References</h4> <p>[Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal Preconditioned Eigensolver: Locally Optimal Block Preconditioned Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2), 517-541. (25 pages) <a class="reference external" href="https://epubs.siam.org/doi/abs/10.1137/S1064827500366124">https://epubs.siam.org/doi/abs/10.1137/S1064827500366124</a></p> <p>[StathopoulosEtal2002] Andreas Stathopoulos and Kesheng Wu. (2002) A Block Orthogonalization Procedure with Constant Synchronization Requirements. SIAM J. Sci. Comput., 23(6), 2165-2182. (18 pages) <a class="reference external" href="https://epubs.siam.org/doi/10.1137/S1064827500370883">https://epubs.siam.org/doi/10.1137/S1064827500370883</a></p> <p>[DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming Gu. (2018) A Robust and Efficient Implementation of LOBPCG. SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages) <a class="reference external" href="https://epubs.siam.org/doi/abs/10.1137/17M1129830">https://epubs.siam.org/doi/abs/10.1137/17M1129830</a></p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.lobpcg.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.lobpcg.html</a>
  </p>
</div>
