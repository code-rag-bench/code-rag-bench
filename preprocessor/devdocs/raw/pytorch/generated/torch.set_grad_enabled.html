<h1 id="set-grad-enabled">set_grad_enabled</h1> <dl class="class"> <dt id="torch.set_grad_enabled">
<code>class torch.set_grad_enabled(mode)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/autograd/grad_mode.html#set_grad_enabled"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that sets gradient calculation to on or off.</p> <p><code>set_grad_enabled</code> will enable or disable grads based on its argument <a class="reference internal" href="torch.mode#torch.mode" title="torch.mode"><code>mode</code></a>. It can be used as a context-manager or as a function.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) â€“ Flag whether to enable grad (<code>True</code>), or disable (<code>False</code>). This can be used to conditionally enable gradients.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)
&gt;&gt;&gt; is_train = False
&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
...   y = x * 2
&gt;&gt;&gt; y.requires_grad
False
&gt;&gt;&gt; torch.set_grad_enabled(True)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
True
&gt;&gt;&gt; torch.set_grad_enabled(False)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
False
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.set_grad_enabled.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.set_grad_enabled.html</a>
  </p>
</div>
