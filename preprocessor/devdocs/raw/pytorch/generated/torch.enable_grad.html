<h1 id="enable-grad">enable_grad</h1> <dl class="class"> <dt id="torch.enable_grad">
<code>class torch.enable_grad</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/autograd/grad_mode.html#enable_grad"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that enables gradient calculation.</p> <p>Enables gradient calculation, if it has been disabled via <a class="reference internal" href="torch.no_grad#torch.no_grad" title="torch.no_grad"><code>no_grad</code></a> or <a class="reference internal" href="torch.set_grad_enabled#torch.set_grad_enabled" title="torch.set_grad_enabled"><code>set_grad_enabled</code></a>.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <p>Also functions as a decorator. (Make sure to instantiate with parenthesis.)</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.tensor([1], requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...   with torch.enable_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
True
&gt;&gt;&gt; y.backward()
&gt;&gt;&gt; x.grad
&gt;&gt;&gt; @torch.enable_grad()
... def doubler(x):
...     return x * 2
&gt;&gt;&gt; with torch.no_grad():
...     z = doubler(x)
&gt;&gt;&gt; z.requires_grad
True
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.enable_grad.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.enable_grad.html</a>
  </p>
</div>
