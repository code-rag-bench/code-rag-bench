<h1 id="flatten">Flatten</h1> <dl class="class"> <dt id="torch.nn.Flatten">
<code>class torch.nn.Flatten(start_dim=1, end_dim=-1)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/flatten.html#Flatten"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Flattens a contiguous range of dims into a tensor. For use with <code>Sequential</code>.</p> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *dims)</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∏</mo><mo>∗</mo><mi>d</mi><mi>i</mi><mi>m</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, \prod *dims)</annotation></semantics></math></span></span> </span> (for the default case).</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>start_dim</strong> – first dim to flatten (default = 1).</li> <li>
<strong>end_dim</strong> – last dim to flatten (default = -1).</li> </ul> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; input = torch.randn(32, 1, 5, 5)
&gt;&gt;&gt; m = nn.Sequential(
&gt;&gt;&gt;     nn.Conv2d(1, 32, 5, 1, 1),
&gt;&gt;&gt;     nn.Flatten()
&gt;&gt;&gt; )
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; output.size()
torch.Size([32, 288])
</pre> </dd> </dl> <dl class="method"> <dt id="torch.nn.Flatten.add_module">
<code>add_module(name, module)</code> </dt> <dd>
<p>Adds a child module to the current module.</p> <p>The module can be accessed as an attribute using the given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the child module. The child module can be accessed from this module using the given name</li> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – child module to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.apply">
<code>apply(fn)</code> </dt> <dd>
<p>Applies <code>fn</code> recursively to every submodule (as returned by <code>.children()</code>) as well as self. Typical use includes initializing the parameters of a model (see also <a class="reference internal" href="../nn.init#nn-init-doc"><span class="std std-ref">torch.nn.init</span></a>).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>fn</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> -&gt; None) – function to be applied to each submodule</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; @torch.no_grad()
&gt;&gt;&gt; def init_weights(m):
&gt;&gt;&gt;     print(m)
&gt;&gt;&gt;     if type(m) == nn.Linear:
&gt;&gt;&gt;         m.weight.fill_(1.0)
&gt;&gt;&gt;         print(m.weight)
&gt;&gt;&gt; net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
&gt;&gt;&gt; net.apply(init_weights)
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Linear(in_features=2, out_features=2, bias=True)
Parameter containing:
tensor([[ 1.,  1.],
        [ 1.,  1.]])
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.bfloat16">
<code>bfloat16()</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>bfloat16</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.buffers">
<code>buffers(recurse=True)</code> </dt> <dd>
<p>Returns an iterator over module buffers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>torch.Tensor</em> – module buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for buf in model.buffers():
&gt;&gt;&gt;     print(type(buf), buf.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.children">
<code>children()</code> </dt> <dd>
<p>Returns an iterator over immediate children modules.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a child module</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.cpu">
<code>cpu()</code> </dt> <dd>
<p>Moves all model parameters and buffers to the CPU.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.cuda">
<code>cuda(device=None)</code> </dt> <dd>
<p>Moves all model parameters and buffers to the GPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on GPU while being optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.double">
<code>double()</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>double</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.eval">
<code>eval()</code> </dt> <dd>
<p>Sets the module in evaluation mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <p>This is equivalent with <a class="reference internal" href="torch.nn.module#torch.nn.Module.train" title="torch.nn.Module.train"><code>self.train(False)</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.float">
<code>float()</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to float datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.half">
<code>half()</code> </dt> <dd>
<p>Casts all floating point parameters and buffers to <code>half</code> datatype.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>self</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.load_state_dict">
<code>load_state_dict(state_dict, strict=True)</code> </dt> <dd>
<p>Copies parameters and buffers from <a class="reference internal" href="#torch.nn.Flatten.state_dict" title="torch.nn.Flatten.state_dict"><code>state_dict</code></a> into this module and its descendants. If <code>strict</code> is <code>True</code>, then the keys of <a class="reference internal" href="#torch.nn.Flatten.state_dict" title="torch.nn.Flatten.state_dict"><code>state_dict</code></a> must exactly match the keys returned by this module’s <a class="reference internal" href="torch.nn.module#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a>) – a dict containing parameters and persistent buffers.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – whether to strictly enforce that the keys in <a class="reference internal" href="#torch.nn.Flatten.state_dict" title="torch.nn.Flatten.state_dict"><code>state_dict</code></a> match the keys returned by this module’s <a class="reference internal" href="torch.nn.module#torch.nn.Module.state_dict" title="torch.nn.Module.state_dict"><code>state_dict()</code></a> function. Default: <code>True</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<ul class="simple"> <li>
<strong>missing_keys</strong> is a list of str containing the missing keys</li> <li>
<strong>unexpected_keys</strong> is a list of str containing the unexpected keys</li> </ul> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><code>NamedTuple</code> with <code>missing_keys</code> and <code>unexpected_keys</code> fields</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.modules">
<code>modules()</code> </dt> <dd>
<p>Returns an iterator over all modules in the network.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>Module</em> – a module in the network</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.modules()):
        print(idx, '-&gt;', m)

0 -&gt; Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
)
1 -&gt; Linear(in_features=2, out_features=2, bias=True)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.named_buffers">
<code>named_buffers(prefix='', recurse=True)</code> </dt> <dd>
<p>Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all buffer names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, torch.Tensor)</em> – Tuple containing the name and buffer</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, buf in self.named_buffers():
&gt;&gt;&gt;    if name in ['running_var']:
&gt;&gt;&gt;        print(buf.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.named_children">
<code>named_children()</code> </dt> <dd>
<p>Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple containing a name and child module</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, module in model.named_children():
&gt;&gt;&gt;     if name in ['conv4', 'conv5']:
&gt;&gt;&gt;         print(module)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.named_modules">
<code>named_modules(memo=None, prefix='')</code> </dt> <dd>
<p>Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p> <dl class="field-list simple"> <dt class="field-odd">Yields</dt> <dd class="field-odd">
<p><em>(string, Module)</em> – Tuple of name and module</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Duplicate modules are returned only once. In the following example, <code>l</code> will be returned only once.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = nn.Linear(2, 2)
&gt;&gt;&gt; net = nn.Sequential(l, l)
&gt;&gt;&gt; for idx, m in enumerate(net.named_modules()):
        print(idx, '-&gt;', m)

0 -&gt; ('', Sequential(
  (0): Linear(in_features=2, out_features=2, bias=True)
  (1): Linear(in_features=2, out_features=2, bias=True)
))
1 -&gt; ('0', Linear(in_features=2, out_features=2, bias=True))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.named_parameters">
<code>named_parameters(prefix='', recurse=True)</code> </dt> <dd>
<p>Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – prefix to prepend to all parameter names.</li> <li>
<strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</li> </ul> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>(string, Parameter)</em> – Tuple containing the name and parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for name, param in self.named_parameters():
&gt;&gt;&gt;    if name in ['bias']:
&gt;&gt;&gt;        print(param.size())
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.parameters">
<code>parameters(recurse=True)</code> </dt> <dd>
<p>Returns an iterator over module parameters.</p> <p>This is typically passed to an optimizer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if True, then yields parameters of this module and all submodules. Otherwise, yields only parameters that are direct members of this module.</p> </dd> <dt class="field-even">Yields</dt> <dd class="field-even">
<p><em>Parameter</em> – module parameter</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; for param in model.parameters():
&gt;&gt;&gt;     print(type(param), param.size())
&lt;class 'torch.Tensor'&gt; (20L,)
&lt;class 'torch.Tensor'&gt; (20L, 1L, 5L, 5L)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_backward_hook">
<code>register_backward_hook(hook)</code> </dt> <dd>
<p>Registers a backward hook on the module.</p> <p>This function is deprecated in favor of <code>nn.Module.register_full_backward_hook()</code> and the behavior of this function will change in future versions.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_buffer">
<code>register_buffer(name, tensor, persistent=True)</code> </dt> <dd>
<p>Adds a buffer to the module.</p> <p>This is typically used to register a buffer that should not to be considered a model parameter. For example, BatchNorm’s <code>running_mean</code> is not a parameter, but is part of the module’s state. Buffers, by default, are persistent and will be saved alongside parameters. This behavior can be changed by setting <code>persistent</code> to <code>False</code>. The only difference between a persistent buffer and a non-persistent buffer is that the latter will not be a part of this module’s <a class="reference internal" href="#torch.nn.Flatten.state_dict" title="torch.nn.Flatten.state_dict"><code>state_dict</code></a>.</p> <p>Buffers can be accessed as attributes using given names.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the buffer. The buffer can be accessed from this module using the given name</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – buffer to be registered.</li> <li>
<strong>persistent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether the buffer is part of this module’s <a class="reference internal" href="#torch.nn.Flatten.state_dict" title="torch.nn.Flatten.state_dict"><code>state_dict</code></a>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; self.register_buffer('running_mean', torch.zeros(num_features))
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_forward_hook">
<code>register_forward_hook(hook)</code> </dt> <dd>
<p>Registers a forward hook on the module.</p> <p>The hook will be called every time after <code>forward()</code> has computed an output. It should have the following signature:</p> <pre data-language="python">hook(module, input, output) -&gt; None or modified output
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the output. It can modify the input inplace but it will not have effect on forward since this is called after <code>forward()</code> is called.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_forward_pre_hook">
<code>register_forward_pre_hook(hook)</code> </dt> <dd>
<p>Registers a forward pre-hook on the module.</p> <p>The hook will be called every time before <code>forward()</code> is invoked. It should have the following signature:</p> <pre data-language="python">hook(module, input) -&gt; None or modified input
</pre> <p>The input contains only the positional arguments given to the module. Keyword arguments won’t be passed to the hooks and only to the <code>forward</code>. The hook can modify the input. User can either return a tuple or a single modified value in the hook. We will wrap the value into a tuple if a single value is returned(unless that value is already a tuple).</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_full_backward_hook">
<code>register_full_backward_hook(hook)</code> </dt> <dd>
<p>Registers a backward hook on the module.</p> <p>The hook will be called every time the gradients with respect to module inputs are computed. The hook should have the following signature:</p> <pre data-language="python">hook(module, grad_input, grad_output) -&gt; tuple(Tensor) or None
</pre> <p>The <code>grad_input</code> and <code>grad_output</code> are tuples that contain the gradients with respect to the inputs and outputs respectively. The hook should not modify its arguments, but it can optionally return a new gradient with respect to the input that will be used in place of <code>grad_input</code> in subsequent computations. <code>grad_input</code> will only correspond to the inputs given as positional arguments and all kwarg arguments are ignored. Entries in <code>grad_input</code> and <code>grad_output</code> will be <code>None</code> for all non-Tensor arguments.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Modifying inputs or outputs inplace is not allowed when using backward hooks and will raise an error.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a handle that can be used to remove the added hook by calling <code>handle.remove()</code></p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><code>torch.utils.hooks.RemovableHandle</code></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.register_parameter">
<code>register_parameter(name, param)</code> </dt> <dd>
<p>Adds a parameter to the module.</p> <p>The parameter can be accessed as an attribute using given name.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> (<em>string</em>) – name of the parameter. The parameter can be accessed from this module using the given name</li> <li>
<strong>param</strong> (<a class="reference internal" href="torch.nn.parameter.parameter#torch.nn.parameter.Parameter" title="torch.nn.parameter.Parameter">Parameter</a>) – parameter to be added to the module.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.requires_grad_">
<code>requires_grad_(requires_grad=True)</code> </dt> <dd>
<p>Change if autograd should record operations on parameters in this module.</p> <p>This method sets the parameters’ <code>requires_grad</code> attributes in-place.</p> <p>This method is helpful for freezing part of the module for finetuning or training parts of a model individually (e.g., GAN training).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether autograd should record operations on parameters in this module. Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.state_dict">
<code>state_dict(destination=None, prefix='', keep_vars=False)</code> </dt> <dd>
<p>Returns a dictionary containing a whole state of the module.</p> <p>Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>a dictionary containing a whole state of the module</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.9)">dict</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; module.state_dict().keys()
['bias', 'weight']
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.to">
<code>to(*args, **kwargs)</code> </dt> <dd>
<p>Moves and/or casts the parameters and buffers.</p> <p>This can be called as</p> <dl class="function"> <dt>
<code>to(device=None, dtype=None, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(dtype, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(tensor, non_blocking=False)</code> </dt> 
</dl> <dl class="function"> <dt>
<code>to(memory_format=torch.channels_last)</code> </dt> 
</dl> <p>Its signature is similar to <a class="reference internal" href="../tensors#torch.Tensor.to" title="torch.Tensor.to"><code>torch.Tensor.to()</code></a>, but only accepts floating point or complex <code>dtype`s. In addition, this method will
only cast the floating point or complex parameters and buffers to :attr:`dtype</code> (if given). The integral parameters and buffers will be moved <code>device</code>, if that is given, but with dtypes unchanged. When <code>non_blocking</code> is set, it tries to convert/move asynchronously with respect to the host if possible, e.g., moving CPU Tensors with pinned memory to CUDA devices.</p> <p>See below for examples.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This method modifies the module in-place.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<code>torch.device</code>) – the desired device of the parameters and buffers in this module</li> <li>
<strong>dtype</strong> (<code>torch.dtype</code>) – the desired floating point or complex dtype of the parameters and buffers in this module</li> <li>
<strong>tensor</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – Tensor whose dtype and device are the desired dtype and device for all parameters and buffers in this module</li> <li>
<strong>memory_format</strong> (<code>torch.memory_format</code>) – the desired memory format for 4D parameters and buffers in this module (keyword only argument)</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; linear = nn.Linear(2, 2)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]])
&gt;&gt;&gt; linear.to(torch.double)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1913, -0.3420],
        [-0.5113, -0.2325]], dtype=torch.float64)
&gt;&gt;&gt; gpu1 = torch.device("cuda:1")
&gt;&gt;&gt; linear.to(gpu1, dtype=torch.half, non_blocking=True)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
&gt;&gt;&gt; cpu = torch.device("cpu")
&gt;&gt;&gt; linear.to(cpu)
Linear(in_features=2, out_features=2, bias=True)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.1914, -0.3420],
        [-0.5112, -0.2324]], dtype=torch.float16)

&gt;&gt;&gt; linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
&gt;&gt;&gt; linear.weight
Parameter containing:
tensor([[ 0.3741+0.j,  0.2382+0.j],
        [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
&gt;&gt;&gt; linear(torch.ones(3, 2, dtype=torch.cdouble))
tensor([[0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j],
        [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
</pre> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.train">
<code>train(mode=True)</code> </dt> <dd>
<p>Sets the module in training mode.</p> <p>This has any effect only on certain modules. See documentations of particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. <a class="reference internal" href="torch.nn.dropout#torch.nn.Dropout" title="torch.nn.Dropout"><code>Dropout</code></a>, <code>BatchNorm</code>, etc.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether to set training mode (<code>True</code>) or evaluation mode (<code>False</code>). Default: <code>True</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.type">
<code>type(dst_type)</code> </dt> <dd>
<p>Casts all parameters and buffers to <code>dst_type</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>dst_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.9)">type</a><em> or </em><em>string</em>) – the desired type</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.xpu">
<code>xpu(device=None)</code> </dt> <dd>
<p>Moves all model parameters and buffers to the XPU.</p> <p>This also makes associated parameters and buffers different objects. So it should be called before constructing optimizer if the module will live on XPU while being optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – if specified, all parameters will be copied to that device</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>self</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a></p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.nn.Flatten.zero_grad">
<code>zero_grad(set_to_none=False)</code> </dt> <dd>
<p>Sets gradients of all model parameters to zero. See similar function under <a class="reference internal" href="../optim#torch.optim.Optimizer" title="torch.optim.Optimizer"><code>torch.optim.Optimizer</code></a> for more context.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – instead of setting to zero, set the grads to None. See <a class="reference internal" href="../optim#torch.optim.Optimizer.zero_grad" title="torch.optim.Optimizer.zero_grad"><code>torch.optim.Optimizer.zero_grad()</code></a> for details.</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.Flatten.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.nn.Flatten.html</a>
  </p>
</div>
