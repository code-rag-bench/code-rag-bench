<h1 id="syncbatchnorm">SyncBatchNorm</h1> <dl class="class"> <dt id="torch.nn.SyncBatchNorm">
<code>class torch.nn.SyncBatchNorm(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True, process_group=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs with additional channel dimension) as described in the paper <a class="reference external" href="https://arxiv.org/abs/1502.03167">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a> .</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mrow><mi>x</mi><mo>−</mo><mi mathvariant="normal">E</mi><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo></mrow><msqrt><mrow><mrow><mi mathvariant="normal">V</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">r</mi></mrow><mo stretchy="false">[</mo><mi>x</mi><mo stretchy="false">]</mo><mo>+</mo><mi>ϵ</mi></mrow></msqrt></mfrac><mo>∗</mo><mi>γ</mi><mo>+</mo><mi>β</mi></mrow><annotation encoding="application/x-tex">y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta</annotation></semantics></math></span></span></span> </div>
<p>The mean and standard-deviation are calculated per-dimension over all mini-batches of the same process groups. <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span></span> </span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span></span> </span> are learnable parameter vectors of size <code>C</code> (where <code>C</code> is the input size). By default, the elements of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>γ</mi></mrow><annotation encoding="application/x-tex">\gamma</annotation></semantics></math></span></span> </span> are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(0, 1)</annotation></semantics></math></span></span> </span> and the elements of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span></span> </span> are set to 0. The standard-deviation is calculated via the biased estimator, equivalent to <code>torch.var(input, unbiased=False)</code>.</p> <p>Also by default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default <code>momentum</code> of 0.1.</p> <p>If <code>track_running_stats</code> is set to <code>False</code>, this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This <code>momentum</code> argument is different from one used in optimizer classes and the conventional notion of momentum. Mathematically, the update rule for running statistics here is <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover accent="true"><mi>x</mi><mo>^</mo></mover><mtext>new</mtext></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mtext>momentum</mtext><mo stretchy="false">)</mo><mo>×</mo><mover accent="true"><mi>x</mi><mo>^</mo></mover><mo>+</mo><mtext>momentum</mtext><mo>×</mo><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t</annotation></semantics></math></span></span> </span>, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat{x}</annotation></semantics></math></span></span> </span> is the estimated statistic and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">x_t</annotation></semantics></math></span></span> </span> is the new observed value.</p> </div> <p>Because the Batch Normalization is done for each channel in the <code>C</code> dimension, computing statistics on <code>(N, +)</code> slices, it’s common terminology to call this Volumetric Batch Normalization or Spatio-temporal Batch Normalization.</p> <p>Currently <a class="reference internal" href="#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code>SyncBatchNorm</code></a> only supports <code>DistributedDataParallel</code> (DDP) with single GPU per process. Use <a class="reference internal" href="#torch.nn.SyncBatchNorm.convert_sync_batchnorm" title="torch.nn.SyncBatchNorm.convert_sync_batchnorm"><code>torch.nn.SyncBatchNorm.convert_sync_batchnorm()</code></a> to convert <code>BatchNorm*D</code> layer to <a class="reference internal" href="#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code>SyncBatchNorm</code></a> before wrapping Network with DDP.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>num_features</strong> – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span></span> </span> from an expected input of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>+</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, +)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>eps</strong> – a value added to the denominator for numerical stability. Default: <code>1e-5</code>
</li> <li>
<strong>momentum</strong> – the value used for the running_mean and running_var computation. Can be set to <code>None</code> for cumulative moving average (i.e. simple average). Default: 0.1</li> <li>
<strong>affine</strong> – a boolean value that when set to <code>True</code>, this module has learnable affine parameters. Default: <code>True</code>
</li> <li>
<strong>track_running_stats</strong> – a boolean value that when set to <code>True</code>, this module tracks the running mean and variance, and when set to <code>False</code>, this module does not track such statistics, and initializes statistics buffers <code>running_mean</code> and <code>running_var</code> as <code>None</code>. When these buffers are <code>None</code>, this module always uses batch statistics. in both training and eval modes. Default: <code>True</code>
</li> <li>
<strong>process_group</strong> – synchronization of stats happen within each process group individually. Default behavior is synchronization across the whole world</li> </ul> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>+</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, +)</annotation></semantics></math></span></span> </span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mo>+</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C, +)</annotation></semantics></math></span></span> </span> (same shape as input)</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With Learnable Parameters
&gt;&gt;&gt; m = nn.SyncBatchNorm(100)
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # ranks is a list of int identifying rank ids.
&gt;&gt;&gt; ranks = list(range(8))
&gt;&gt;&gt; r1, r2 = ranks[:4], ranks[4:]
&gt;&gt;&gt; # Note: every rank calls into new_group for every
&gt;&gt;&gt; # process group created, even if that rank is not
&gt;&gt;&gt; # part of the group.
&gt;&gt;&gt; process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]
&gt;&gt;&gt; process_group = process_groups[0 if dist.get_rank() &lt;= 3 else 1]
&gt;&gt;&gt; # Without Learnable Parameters
&gt;&gt;&gt; m = nn.BatchNorm3d(100, affine=False, process_group=process_group)
&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45, 10)
&gt;&gt;&gt; output = m(input)

&gt;&gt;&gt; # network is nn.BatchNorm layer
&gt;&gt;&gt; sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)
&gt;&gt;&gt; # only single gpu per process is currently supported
&gt;&gt;&gt; ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(
&gt;&gt;&gt;                         sync_bn_network,
&gt;&gt;&gt;                         device_ids=[args.local_rank],
&gt;&gt;&gt;                         output_device=args.local_rank)
</pre> <dl class="method"> <dt id="torch.nn.SyncBatchNorm.convert_sync_batchnorm">
<code>classmethod convert_sync_batchnorm(module, process_group=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/batchnorm.html#SyncBatchNorm.convert_sync_batchnorm"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Helper function to convert all <code>BatchNorm*D</code> layers in the model to <a class="reference internal" href="#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code>torch.nn.SyncBatchNorm</code></a> layers.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">nn.Module</a>) – module containing one or more attr:<code>BatchNorm*D</code> layers</li> <li>
<strong>process_group</strong> (<em>optional</em>) – process group to scope synchronization, default is the whole world</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>The original <code>module</code> with the converted <a class="reference internal" href="#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code>torch.nn.SyncBatchNorm</code></a> layers. If the original <code>module</code> is a <code>BatchNorm*D</code> layer, a new <a class="reference internal" href="#torch.nn.SyncBatchNorm" title="torch.nn.SyncBatchNorm"><code>torch.nn.SyncBatchNorm</code></a> layer object will be returned instead.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; # Network with nn.BatchNorm layer
&gt;&gt;&gt; module = torch.nn.Sequential(
&gt;&gt;&gt;            torch.nn.Linear(20, 100),
&gt;&gt;&gt;            torch.nn.BatchNorm1d(100),
&gt;&gt;&gt;          ).cuda()
&gt;&gt;&gt; # creating process group (optional)
&gt;&gt;&gt; # ranks is a list of int identifying rank ids.
&gt;&gt;&gt; ranks = list(range(8))
&gt;&gt;&gt; r1, r2 = ranks[:4], ranks[4:]
&gt;&gt;&gt; # Note: every rank calls into new_group for every
&gt;&gt;&gt; # process group created, even if that rank is not
&gt;&gt;&gt; # part of the group.
&gt;&gt;&gt; process_groups = [torch.distributed.new_group(pids) for pids in [r1, r2]]
&gt;&gt;&gt; process_group = process_groups[0 if dist.get_rank() &lt;= 3 else 1]
&gt;&gt;&gt; sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)
</pre> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.SyncBatchNorm.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.nn.SyncBatchNorm.html</a>
  </p>
</div>
