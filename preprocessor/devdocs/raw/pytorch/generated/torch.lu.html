<h1 id="torch-lu">torch.lu</h1> <dl class="function"> <dt id="torch.lu">
<code>torch.lu(*args, **kwargs)</code> </dt> <dd>
<p>Computes the LU factorization of a matrix or batches of matrices <code>A</code>. Returns a tuple containing the LU factorization and pivots of <code>A</code>. Pivoting is done if <code>pivot</code> is set to <code>True</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The pivots returned by the function are 1-indexed. If <code>pivot</code> is <code>False</code>, then the returned pivots is a tensor filled with zeros of the appropriate size.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>LU factorization with <code>pivot</code> = <code>False</code> is not available for CPU, and attempting to do so will throw an error. However, LU factorization with <code>pivot</code> = <code>False</code> is available for CUDA.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function does not check if the factorization was successful or not if <code>get_infos</code> is <code>True</code> since the status of the factorization is present in the third element of the return tuple.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p><code>L</code>, <code>U</code>, and <code>P</code> can be derived using <a class="reference internal" href="torch.lu_unpack#torch.lu_unpack" title="torch.lu_unpack"><code>torch.lu_unpack()</code></a>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The LU factorization does have backward support, but only for square inputs of full rank.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the tensor to factor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>pivot</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – controls whether pivoting is done. Default: <code>True</code>
</li> <li>
<strong>get_infos</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – if set to <code>True</code>, returns an info IntTensor. Default: <code>False</code>
</li> <li>
<strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – optional output tuple. If <code>get_infos</code> is <code>True</code>, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If <code>get_infos</code> is <code>False</code>, then the elements in the tuple are Tensor, IntTensor. Default: <code>None</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<p>A tuple of tensors containing</p>  <ul class="simple"> <li>
<strong>factorization</strong> (<em>Tensor</em>): the factorization of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>pivots</strong> (<em>IntTensor</em>): the pivots of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mtext>min</mtext><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, \text{min}(m, n))</annotation></semantics></math></span></span> </span>. <code>pivots</code> stores all the intermediate transpositions of rows. The final permutation <code>perm</code> could be reconstructed by applying <code>swap(perm[i], perm[pivots[i] - 1])</code> for <code>i = 0, ..., pivots.size(-1) - 1</code>, where <code>perm</code> is initially the identity permutation of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span> </span> elements (essentially this is what <a class="reference internal" href="torch.lu_unpack#torch.lu_unpack" title="torch.lu_unpack"><code>torch.lu_unpack()</code></a> is doing).</li> <li>
<strong>infos</strong> (<em>IntTensor</em>, <em>optional</em>): if <code>get_infos</code> is <code>True</code>, this is a tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span> </span> where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed</li> </ul>  </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>(<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>, IntTensor, IntTensor (optional))</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
&gt;&gt;&gt; A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
&gt;&gt;&gt; pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
&gt;&gt;&gt; if info.nonzero().size(0) == 0:
...   print('LU factorization succeeded for all samples!')
LU factorization succeeded for all samples!
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.lu.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.lu.html</a>
  </p>
</div>
