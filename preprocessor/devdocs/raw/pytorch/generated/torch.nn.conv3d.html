<h1 id="conv3d">Conv3d</h1> <dl class="class"> <dt id="torch.nn.Conv3d">
<code>class torch.nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/modules/conv.html#Conv3d"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a 3D convolution over an input signal composed of several input planes.</p> <p>In the simplest case, the output value of the layer with input size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><mi>D</mi><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, D, H, W)</annotation></semantics></math></span></span> </span> and output <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>D</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{out}, D_{out}, H_{out}, W_{out})</annotation></semantics></math></span></span> </span> can be precisely described as:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>o</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><msub><mi>t</mi><mi>j</mi></msub></mrow></msub><mo stretchy="false">)</mo><mo>=</mo><mi>b</mi><mi>i</mi><mi>a</mi><mi>s</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><msub><mi>t</mi><mi>j</mi></msub></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>0</mn></mrow><mrow><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>−</mo><mn>1</mn></mrow></munderover><mi>w</mi><mi>e</mi><mi>i</mi><mi>g</mi><mi>h</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><msub><mi>t</mi><mi>j</mi></msub></mrow></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo><mo>⋆</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mo stretchy="false">(</mo><msub><mi>N</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>k</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">out(N_i, C_{out_j}) = bias(C_{out_j}) + \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k) </annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⋆</mo></mrow><annotation encoding="application/x-tex">\star</annotation></semantics></math></span></span> </span> is the valid 3D <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-correlation">cross-correlation</a> operator</p> <p>This module supports <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#tf32-on-ampere"><span class="std std-ref">TensorFloat32</span></a>.</p> <ul> <li>
<code>stride</code> controls the stride for the cross-correlation.</li> <li>
<code>padding</code> controls the amount of implicit padding on both sides for <code>padding</code> number of points for each dimension.</li> <li>
<code>dilation</code> controls the spacing between the kernel points; also known as the à trous algorithm. It is harder to describe, but this <a class="reference external" href="https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md">link</a> has a nice visualization of what <code>dilation</code> does.</li> <li>
<p><code>groups</code> controls the connections between inputs and outputs. <code>in_channels</code> and <code>out_channels</code> must both be divisible by <code>groups</code>. For example,</p>  <ul class="simple"> <li>At groups=1, all inputs are convolved to all outputs.</li> <li>At groups=2, the operation becomes equivalent to having two conv layers side by side, each seeing half the input channels and producing half the output channels, and both subsequently concatenated.</li> <li>At groups= <code>in_channels</code>, each input channel is convolved with its own set of filters (of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mtext>out_channels</mtext><mtext>in_channels</mtext></mfrac></mrow><annotation encoding="application/x-tex">\frac{\text{out\_channels}}{\text{in\_channels}}</annotation></semantics></math></span></span> </span>).</li> </ul>  </li> </ul> <p>The parameters <code>kernel_size</code>, <code>stride</code>, <code>padding</code>, <code>dilation</code> can either be:</p>  <ul class="simple"> <li>a single <code>int</code> – in which case the same value is used for the depth, height and width dimension</li> <li>a <code>tuple</code> of three ints – in which case, the first <code>int</code> is used for the depth dimension, the second <code>int</code> for the height dimension and the third <code>int</code> for the width dimension</li> </ul>  <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>groups == in_channels</code> and <code>out_channels == K * in_channels</code>, where <code>K</code> is a positive integer, this operation is also known as a “depthwise convolution”.</p> <p>In other words, for an input of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>L</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, L_{in})</annotation></semantics></math></span></span> </span>, a depthwise convolution with a depthwise multiplier <code>K</code> can be performed with the arguments <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo separator="true">,</mo><msub><mi>C</mi><mtext>out</mtext></msub><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo>×</mo><mtext>K</mtext><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mtext>groups</mtext><mo>=</mo><msub><mi>C</mi><mtext>in</mtext></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(C_\text{in}=C_\text{in}, C_\text{out}=C_\text{in} \times \text{K}, ..., \text{groups}=C_\text{in})</annotation></semantics></math></span></span> </span>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>in_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Number of channels in the input image</li> <li>
<strong>out_channels</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Number of channels produced by the convolution</li> <li>
<strong>kernel_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>) – Size of the convolving kernel</li> <li>
<strong>stride</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Stride of the convolution. Default: 1</li> <li>
<strong>padding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Zero-padding added to all three sides of the input. Default: 0</li> <li>
<strong>padding_mode</strong> (<em>string</em><em>, </em><em>optional</em>) – <code>'zeros'</code>, <code>'reflect'</code>, <code>'replicate'</code> or <code>'circular'</code>. Default: <code>'zeros'</code>
</li> <li>
<strong>dilation</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – Spacing between kernel elements. Default: 1</li> <li>
<strong>groups</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – Number of blocked connections from input channels to output channels. Default: 1</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, adds a learnable bias to the output. Default: <code>True</code>
</li> </ul> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>D</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{in}, D_{in}, H_{in}, W_{in})</annotation></semantics></math></span></span> </span>
</li> <li>
<p>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><msub><mi>C</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>D</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo separator="true">,</mo><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, C_{out}, D_{out}, H_{out}, W_{out})</annotation></semantics></math></span></span> </span> where</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>D</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><msub><mi>D</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mn>0</mn><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow></mrow><annotation encoding="application/x-tex">D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor </annotation></semantics></math></span></span></span> </div>
<div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>H</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><msub><mi>H</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mn>1</mn><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow></mrow><annotation encoding="application/x-tex">H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor </annotation></semantics></math></span></span></span> </div>
<div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>W</mi><mrow><mi>o</mi><mi>u</mi><mi>t</mi></mrow></msub><mo>=</mo><mrow><mo fence="true">⌊</mo><mfrac><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>n</mi></mrow></msub><mo>+</mo><mn>2</mn><mo>×</mo><mtext>padding</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>−</mo><mtext>dilation</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>×</mo><mo stretchy="false">(</mo><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></mrow><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mn>2</mn><mo stretchy="false">]</mo></mrow></mfrac><mo>+</mo><mn>1</mn><mo fence="true">⌋</mo></mrow></mrow><annotation encoding="application/x-tex">W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor </annotation></semantics></math></span></span></span> </div>
</li> </ul> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Conv3d.weight</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable weights of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_channels</mtext><mo separator="true">,</mo><mfrac><mtext>in_channels</mtext><mtext>groups</mtext></mfrac><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},</annotation></semantics></math></span></span> </span> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>kernel_size[0]</mtext><mo separator="true">,</mo><mtext>kernel_size[1]</mtext><mo separator="true">,</mo><mtext>kernel_size[2]</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})</annotation></semantics></math></span></span> </span>. The values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>in</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}</annotation></semantics></math></span></span> </span>
</li> <li>
<strong>~Conv3d.bias</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the learnable bias of the module of shape (out_channels). If <code>bias</code> is <code>True</code>, then the values of these weights are sampled from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">U</mi><mo stretchy="false">(</mo><mo>−</mo><msqrt><mi>k</mi></msqrt><mo separator="true">,</mo><msqrt><mi>k</mi></msqrt><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathcal{U}(-\sqrt{k}, \sqrt{k})</annotation></semantics></math></span></span> </span> where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mfrac><mrow><mi>g</mi><mi>r</mi><mi>o</mi><mi>u</mi><mi>p</mi><mi>s</mi></mrow><mrow><msub><mi>C</mi><mtext>in</mtext></msub><mo>∗</mo><msubsup><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mn>2</mn></msubsup><mtext>kernel_size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}</annotation></semantics></math></span></span> </span>
</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # With square kernels and equal stride
&gt;&gt;&gt; m = nn.Conv3d(16, 33, 3, stride=2)
&gt;&gt;&gt; # non-square kernels and unequal stride and with padding
&gt;&gt;&gt; m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
&gt;&gt;&gt; input = torch.randn(20, 16, 10, 50, 100)
&gt;&gt;&gt; output = m(input)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.nn.Conv3d.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.nn.Conv3d.html</a>
  </p>
</div>
