<h1 id="torch-svd">torch.svd</h1> <dl class="function"> <dt id="torch.svd">
<code>torch.svd(input, some=True, compute_uv=True, *, out=None) -&gt; (Tensor, Tensor, Tensor)</code> </dt> <dd>
<p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>. The singular value decomposition is represented as a namedtuple (<code>U,S,V</code>), such that <code>input</code> = <code>U</code> diag(<code>S</code>) <code>Vᴴ</code>, where <code>Vᴴ</code> is the transpose of <code>V</code> for the real-valued inputs, or the conjugate transpose of <code>V</code> for the complex-valued inputs. If <code>input</code> is a batch of tensors, then <code>U</code>, <code>S</code>, and <code>V</code> are also batched with the same batch dimensions as <code>input</code>.</p> <p>If <code>some</code> is <code>True</code> (default), the method returns the reduced singular value decomposition i.e., if the last two dimensions of <code>input</code> are <code>m</code> and <code>n</code>, then the returned <code>U</code> and <code>V</code> matrices will contain only min(<code>n, m</code>) orthonormal columns.</p> <p>If <code>compute_uv</code> is <code>False</code>, the returned <code>U</code> and <code>V</code> will be zero-filled matrices of shape <code>(m × m)</code> and <code>(n × n)</code> respectively, and the same device as <code>input</code>. The <code>some</code> argument has no effect when <code>compute_uv</code> is <code>False</code>.</p> <p>Supports input of float, double, cfloat and cdouble data types. The dtypes of <code>U</code> and <code>V</code> are the same as <code>input</code>’s. <code>S</code> will always be real-valued, even if <code>input</code> is complex.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> is deprecated. Please use <a class="reference internal" href="../linalg#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> instead, which is similar to NumPy’s <code>numpy.linalg.svd</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Differences with <a class="reference internal" href="../linalg#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>:</p> <ul class="simple"> <li>
<code>some</code> is the opposite of <a class="reference internal" href="../linalg#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>’s <code>full_matricies</code>. Note that default value for both is <code>True</code>, so the default behavior is effectively the opposite.</li> <li>
<a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> returns <code>V</code>, whereas <a class="reference internal" href="../linalg#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> returns <code>Vᴴ</code>.</li> <li>If <code>compute_uv=False</code>, <a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> returns zero-filled tensors for <code>U</code> and <code>Vh</code>, whereas <a class="reference internal" href="../linalg#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> returns empty tensors.</li> </ul> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The singular values are returned in descending order. If <code>input</code> is a batch of matrices, then the singular values of each matrix in the batch is returned in descending order.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The implementation of SVD on CPU uses the LAPACK routine <code>?gesdd</code> (a divide-and-conquer algorithm) instead of <code>?gesvd</code> for speed. Analogously, the SVD on GPU uses the cuSOLVER routines <code>gesvdj</code> and <code>gesvdjBatched</code> on CUDA 10.1.243 and later, and uses the MAGMA routine <code>gesdd</code> on earlier versions of CUDA.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The returned matrix <code>U</code> will be transposed, i.e. with strides <code>U.contiguous().transpose(-2, -1).stride()</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Gradients computed using <code>U</code> and <code>V</code> may be unstable if <code>input</code> is not full rank or has non-unique singular values.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>some</code> = <code>False</code>, the gradients on <code>U[..., :, min(m, n):]</code> and <code>V[..., :, min(m, n):]</code> will be ignored in backward as those vectors can be arbitrary bases of the subspaces.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>S</code> tensor can only be used to compute gradients if <code>compute_uv</code> is True.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>With the complex-valued input the backward operation works correctly only for gauge invariant loss functions. Please look at <a class="reference external" href="https://re-ra.xyz/Gauge-Problem-in-Automatic-Differentiation/">Gauge problem in AD</a> for more details.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Since <code>U</code> and <code>V</code> of an SVD is not unique, each vector can be multiplied by an arbitrary phase factor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>ϕ</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{i \phi}</annotation></semantics></math></span></span> </span> while the SVD result is still correct. Different platforms, like Numpy, or inputs on different device types, may produce different <code>U</code> and <code>V</code> tensors.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <code>(*, m, n)</code> where <code>*</code> is zero or more batch dimensions consisting of <code>(m × n)</code> matrices.</li> <li>
<strong>some</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – controls whether to compute the reduced or full decomposition, and consequently the shape of returned <code>U</code> and <code>V</code>. Defaults to True.</li> <li>
<strong>compute_uv</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – option whether to compute <code>U</code> and <code>V</code> or not. Defaults to True.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a><em>, </em><em>optional</em>) – the output tuple of tensors</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
&gt;&gt;&gt; s
tensor([2.3289, 2.0315, 0.7806])
&gt;&gt;&gt; v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, v = torch.svd(a_big)
&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.transpose(-2, -1)))
tensor(2.6503e-06)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/generated/torch.svd.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/generated/torch.svd.html</a>
  </p>
</div>
