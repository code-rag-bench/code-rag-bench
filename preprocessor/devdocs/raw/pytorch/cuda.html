<h1 id="torch-cuda">torch.cuda</h1> <p id="module-torch.cuda">This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</p> <p>It is lazily initialized, so you can always import it, and use <a class="reference internal" href="#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> to determine if your system supports CUDA.</p> <p><a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p> <dl class="function"> <dt id="torch.cuda.can_device_access_peer">
<code>torch.cuda.can_device_access_peer(device, peer_device)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#can_device_access_peer"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Checks if peer access between two devices is possible.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.current_blas_handle">
<code>torch.cuda.current_blas_handle()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_blas_handle"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns cublasHandle_t pointer to current cuBLAS handle</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.current_device">
<code>torch.cuda.current_device()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_device"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the index of a currently selected device.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.current_stream">
<code>torch.cuda.current_stream(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#current_stream"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for a given device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns the currently selected <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.default_stream">
<code>torch.cuda.default_stream(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#default_stream"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for a given device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns the default <a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.cuda.device">
<code>class torch.cuda.device(device)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that changes the selected device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – device index to select. It’s a no-op if this argument is a negative integer or <code>None</code>.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.device_count">
<code>torch.cuda.device_count()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device_count"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the number of GPUs available.</p> </dd>
</dl> <dl class="class"> <dt id="torch.cuda.device_of">
<code>class torch.cuda.device_of(obj)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#device_of"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that changes the current device to that of given object.</p> <p>You can use both tensors and storages as arguments. If a given object is not allocated on a GPU, this is a no-op.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>obj</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><em>Storage</em>) – object allocated on the selected device.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_arch_list">
<code>torch.cuda.get_arch_list()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_arch_list"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns list CUDA architectures this library was compiled for.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_device_capability">
<code>torch.cuda.get_device_capability(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_capability"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gets the cuda capability of a device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – device for which to return the device capability. This function is a no-op if this argument is a negative integer. It uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>the major and minor cuda capability of the device</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.9)">tuple</a>(<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>, <a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>)</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_device_name">
<code>torch.cuda.get_device_name(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_name"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gets the name of a device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – device for which to return the name. This function is a no-op if this argument is a negative integer. It uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>the name of the device</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a></p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_device_properties">
<code>torch.cuda.get_device_properties(device)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_device_properties"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gets the properties of a device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – device for which to return the properties of the device.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>the properties of the device</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>_CudaDeviceProperties</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_gencode_flags">
<code>torch.cuda.get_gencode_flags()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#get_gencode_flags"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns NVCC gencode flags this library were compiled with.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.init">
<code>torch.cuda.init()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#init"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Initialize PyTorch’s CUDA state. You may need to call this explicitly if you are interacting with PyTorch via its C API, as Python bindings for CUDA functionality will not be available until this initialization takes place. Ordinary users should not need this, as all of PyTorch’s CUDA methods automatically initialize CUDA state on-demand.</p> <p>Does nothing if the CUDA state is already initialized.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.ipc_collect">
<code>torch.cuda.ipc_collect()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#ipc_collect"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Force collects GPU memory after it has been released by CUDA IPC.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Checks if any sent CUDA tensors could be cleaned from the memory. Force closes shared memory file used for reference counting if there is no active counters. Useful when the producer process stopped actively sending tensors and want to release unused memory.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.is_available">
<code>torch.cuda.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a bool indicating if CUDA is currently available.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.is_initialized">
<code>torch.cuda.is_initialized()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#is_initialized"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether PyTorch’s CUDA state has been initialized.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.set_device">
<code>torch.cuda.set_device(device)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#set_device"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the current device.</p> <p>Usage of this function is discouraged in favor of <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a>. In most cases it’s better to use <code>CUDA_VISIBLE_DEVICES</code> environmental variable.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – selected device. This function is a no-op if this argument is negative.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.stream">
<code>torch.cuda.stream(stream)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#stream"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that selects a given stream.</p> <p>All CUDA kernels queued within its context will be enqueued on a selected stream.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream">Stream</a>) – selected stream. This manager is a no-op if it’s <code>None</code>.</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Streams are per-device. If the selected stream is not on the current device, this function will also change the current device to match the stream.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.synchronize">
<code>torch.cuda.synchronize(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda.html#synchronize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Waits for all kernels in all streams on a CUDA device to complete.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – device for which to synchronize. It uses the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>
</dl>  <h2 id="random-number-generator">Random Number Generator</h2> <dl class="function"> <dt id="torch.cuda.get_rng_state">
<code>torch.cuda.get_rng_state(device='cuda')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#get_rng_state"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the random number generator state of the specified GPU as a ByteTensor.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – The device to return the RNG state of. Default: <code>'cuda'</code> (i.e., <code>torch.device('cuda')</code>, the current CUDA device).</p> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function eagerly initializes CUDA.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.get_rng_state_all">
<code>torch.cuda.get_rng_state_all()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#get_rng_state_all"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a list of ByteTensor representing the random number states of all devices.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.set_rng_state">
<code>torch.cuda.set_rng_state(new_state, device='cuda')</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#set_rng_state"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the random number generator state of the specified GPU.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>new_state</strong> (<em>torch.ByteTensor</em>) – The desired state</li> <li>
<strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – The device to set the RNG state. Default: <code>'cuda'</code> (i.e., <code>torch.device('cuda')</code>, the current CUDA device).</li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.set_rng_state_all">
<code>torch.cuda.set_rng_state_all(new_states)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#set_rng_state_all"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the random number generator state of all devices.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>new_states</strong> (<em>Iterable of torch.ByteTensor</em>) – The desired state for each device</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.manual_seed">
<code>torch.cuda.manual_seed(seed)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#manual_seed"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the seed for generating random numbers for the current GPU. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – The desired seed.</p> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If you are working with a multi-GPU model, this function is insufficient to get determinism. To seed all GPUs, use <a class="reference internal" href="#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code>manual_seed_all()</code></a>.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.manual_seed_all">
<code>torch.cuda.manual_seed_all(seed)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#manual_seed_all"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the seed for generating random numbers on all GPUs. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – The desired seed.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.seed">
<code>torch.cuda.seed()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#seed"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the seed for generating random numbers to a random number for the current GPU. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If you are working with a multi-GPU model, this function will only initialize the seed on one GPU. To initialize all GPUs, use <a class="reference internal" href="#torch.cuda.seed_all" title="torch.cuda.seed_all"><code>seed_all()</code></a>.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.seed_all">
<code>torch.cuda.seed_all()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#seed_all"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the seed for generating random numbers to a random number on all GPUs. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.initial_seed">
<code>torch.cuda.initial_seed()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/random.html#initial_seed"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the current random seed of the current GPU.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function eagerly initializes CUDA.</p> </div> </dd>
</dl>   <h2 id="communication-collectives">Communication collectives</h2> <dl class="function"> <dt id="torch.cuda.comm.broadcast">
<code>torch.cuda.comm.broadcast(tensor, devices=None, *, out=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#broadcast"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Broadcasts a tensor to specified GPU devices.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensor to broadcast. Can be on CPU or GPU.</li> <li>
<strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em><em>, </em><em>optional</em>) – an iterable of GPU devices, among which to broadcast.</li> <li>
<strong>out</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to store output results.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Exactly one of <code>devices</code> and <code>out</code> must be specified.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<ul class="simple"> <li>
<dl class="simple"> <dt>
<code>If devices is specified,</code> </dt>
<dd>
<p>a tuple containing copies of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>If out is specified,</code> </dt>
<dd>
<p>a tuple containing <code>out</code> tensors, each containing a copy of <code>tensor</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.comm.broadcast_coalesced">
<code>torch.cuda.comm.broadcast_coalesced(tensors, devices, buffer_size=10485760)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#broadcast_coalesced"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Broadcasts a sequence tensors to the specified GPUs. Small tensors are first coalesced into a buffer to reduce the number of synchronizations.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensors</strong> (<em>sequence</em>) – tensors to broadcast. Must be on the same device, either CPU or GPU.</li> <li>
<strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em>) – an iterable of GPU devices, among which to broadcast.</li> <li>
<strong>buffer_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – maximum size of the buffer used for coalescing</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A tuple containing copies of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.comm.reduce_add">
<code>torch.cuda.comm.reduce_add(inputs, destination=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#reduce_add"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sums tensors from multiple GPUs.</p> <p>All inputs should have matching shapes, dtype, and layout. The output tensor will be of the same shape, dtype, and layout.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>inputs</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – an iterable of tensors to add.</li> <li>
<strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – a device on which the output will be placed (default: current device).</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A tensor containing an elementwise sum of all inputs, placed on the <code>destination</code> device.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.comm.scatter">
<code>torch.cuda.comm.scatter(tensor, devices=None, chunk_sizes=None, dim=0, streams=None, *, out=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#scatter"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Scatters tensor across multiple GPUs.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensor</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensor to scatter. Can be on CPU or GPU.</li> <li>
<strong>devices</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em><em>, </em><em>optional</em>) – an iterable of GPU devices, among which to scatter.</li> <li>
<strong>chunk_sizes</strong> (<em>Iterable</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>]</em><em>, </em><em>optional</em>) – sizes of chunks to be placed on each device. It should match <code>devices</code> in length and sums to <code>tensor.size(dim)</code>. If not specified, <code>tensor</code> will be divided into equal chunks.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – A dimension along which to chunk <code>tensor</code>. Default: <code>0</code>.</li> <li>
<strong>streams</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream">Stream</a><em>]</em><em>, </em><em>optional</em>) – an iterable of Streams, among which to execute the scatter. If not specified, the default stream will be utilized.</li> <li>
<strong>out</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the GPU tensors to store output results. Sizes of these tensors must match that of <code>tensor</code>, except for <code>dim</code>, where the total size must sum to <code>tensor.size(dim)</code>.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Exactly one of <code>devices</code> and <code>out</code> must be specified. When <code>out</code> is specified, <code>chunk_sizes</code> must not be specified and will be inferred from sizes of <code>out</code>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<ul class="simple"> <li>
<dl class="simple"> <dt>
<code>If devices is specified,</code> </dt>
<dd>
<p>a tuple containing chunks of <code>tensor</code>, placed on <code>devices</code>.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>If out is specified,</code> </dt>
<dd>
<p>a tuple containing <code>out</code> tensors, each containing a chunk of <code>tensor</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.comm.gather">
<code>torch.cuda.comm.gather(tensors, dim=0, destination=None, *, out=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/parallel/comm.html#gather"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Gathers tensors from multiple GPU devices.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensors</strong> (<em>Iterable</em><em>[</em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – an iterable of tensors to gather. Tensor sizes in all dimensions other than <code>dim</code> have to match.</li> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – a dimension along which the tensors will be concatenated. Default: <code>0</code>.</li> <li>
<strong>destination</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – the output device. Can be CPU or CUDA. Default: the current CUDA device.</li> <li>
<strong>out</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em><em>, </em><em>keyword-only</em>) – the tensor to store gather result. Its sizes must match those of <code>tensors</code>, except for <code>dim</code>, where the size must equal <code>sum(tensor.size(dim) for tensor in tensors)</code>. Can be on CPU or CUDA.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p><code>destination</code> must not be specified when <code>out</code> is specified.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<ul class="simple"> <li>
<dl class="simple"> <dt>
<code>If destination is specified,</code> </dt>
<dd>
<p>a tensor located on <code>destination</code> device, that is a result of concatenating <code>tensors</code> along <code>dim</code>.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>If out is specified,</code> </dt>
<dd>
<p>the <code>out</code> tensor, now containing results of concatenating <code>tensors</code> along <code>dim</code>.</p> </dd> </dl> </li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="streams-and-events">Streams and events</h2> <dl class="class"> <dt id="torch.cuda.Stream">
<code>class torch.cuda.Stream</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrapper around a CUDA stream.</p> <p>A CUDA stream is a linear sequence of execution that belongs to a specific device, independent from other streams. See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> for details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – a device on which to allocate the stream. If <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default) or a negative integer, this will use the current device.</li> <li>
<strong>priority</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – priority of the stream. Can be either -1 (high priority) or 0 (low priority). By default, streams have priority 0.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Although CUDA versions &gt;= 11 support more than two levels of priorities, in PyTorch, we only support two levels of priorities.</p> </div> <dl class="method"> <dt id="torch.cuda.Stream.query">
<code>query()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.query"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Checks if all the work submitted has been completed.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>A boolean indicating if all kernels in this stream are completed.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Stream.record_event">
<code>record_event(event=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.record_event"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Records an event.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event">Event</a><em>, </em><em>optional</em>) – event to record. If not given, a new one will be allocated.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Recorded event.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Stream.synchronize">
<code>synchronize()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.synchronize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wait for all the kernels in this stream to complete.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This is a wrapper around <code>cudaStreamSynchronize()</code>: see <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA Stream documentation</a> for more info.</p> </div> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Stream.wait_event">
<code>wait_event(event)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.wait_event"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Makes all future work submitted to the stream wait for an event.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>event</strong> (<a class="reference internal" href="#torch.cuda.Event" title="torch.cuda.Event">Event</a>) – an event to wait for.</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This is a wrapper around <code>cudaStreamWaitEvent()</code>: see <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html">CUDA Stream documentation</a> for more info.</p> <p>This function returns without waiting for <code>event</code>: only future operations are affected.</p> </div> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Stream.wait_stream">
<code>wait_stream(stream)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Stream.wait_stream"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Synchronizes with another stream.</p> <p>All future work submitted to this stream will wait until all kernels submitted to a given stream at the time of call complete.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>stream</strong> (<a class="reference internal" href="#torch.cuda.Stream" title="torch.cuda.Stream">Stream</a>) – a stream to synchronize.</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function returns without waiting for currently enqueued kernels in <a class="reference internal" href="#torch.cuda.stream" title="torch.cuda.stream"><code>stream</code></a>: only future operations are affected.</p> </div> </dd>
</dl> </dd>
</dl> <dl class="class"> <dt id="torch.cuda.Event">
<code>class torch.cuda.Event</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrapper around a CUDA event.</p> <p>CUDA events are synchronization markers that can be used to monitor the device’s progress, to accurately measure timing, and to synchronize CUDA streams.</p> <p>The underlying CUDA events are lazily initialized when the event is first recorded or exported to another process. After creation, only streams on the same device may record the event. However, streams on any device can wait on the event.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>enable_timing</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – indicates if the event should measure time (default: <code>False</code>)</li> <li>
<strong>blocking</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, <a class="reference internal" href="#torch.cuda.Event.wait" title="torch.cuda.Event.wait"><code>wait()</code></a> will be blocking (default: <code>False</code>)</li> <li>
<strong>interprocess</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – if <code>True</code>, the event can be shared between processes (default: <code>False</code>)</li> </ul> </dd> </dl> <dl class="method"> <dt id="torch.cuda.Event.elapsed_time">
<code>elapsed_time(end_event)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.elapsed_time"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the time elapsed in milliseconds after the event was recorded and before the end_event was recorded.</p> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.from_ipc_handle">
<code>classmethod from_ipc_handle(device, handle)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.from_ipc_handle"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Reconstruct an event from an IPC handle on the given device.</p> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.ipc_handle">
<code>ipc_handle()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.ipc_handle"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns an IPC handle of this event. If not recorded yet, the event will use the current device.</p> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.query">
<code>query()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.query"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Checks if all work currently captured by event has completed.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>A boolean indicating if all work currently captured by event has completed.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.record">
<code>record(stream=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.record"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Records the event in a given stream.</p> <p>Uses <code>torch.cuda.current_stream()</code> if no stream is specified. The stream’s device must match the event’s device.</p> </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.synchronize">
<code>synchronize()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.synchronize"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Waits for the event to complete.</p> <p>Waits until the completion of all work currently captured in this event. This prevents the CPU thread from proceeding until the event completes.</p>  <div class="admonition note"> <p class="admonition-title">Note</p> <p>This is a wrapper around <code>cudaEventSynchronize()</code>: see <a class="reference external" href="https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html">CUDA Event documentation</a> for more info.</p> </div>  </dd>
</dl> <dl class="method"> <dt id="torch.cuda.Event.wait">
<code>wait(stream=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/streams.html#Event.wait"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Makes all future work submitted to the given stream wait for this event.</p> <p>Use <code>torch.cuda.current_stream()</code> if no stream is specified.</p> </dd>
</dl> </dd>
</dl>   <h2 id="memory-management">Memory management</h2> <dl class="function"> <dt id="torch.cuda.empty_cache">
<code>torch.cuda.empty_cache()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#empty_cache"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in <code>nvidia-smi</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code>empty_cache()</code></a> doesn’t increase the amount of GPU memory available for PyTorch. However, it may help reduce fragmentation of GPU memory in certain cases. See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.list_gpu_processes">
<code>torch.cuda.list_gpu_processes(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#list_gpu_processes"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a human-readable printout of the running processes and their GPU memory use for a given device.</p> <p>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns printout for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_stats">
<code>torch.cuda.memory_stats(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_stats"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p> <p>The return value of this function is a dictionary of statistics, each of which is a non-negative integer.</p> <p>Core statistics:</p> <ul class="simple"> <li>
<code>"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: number of allocation requests received by the memory allocator.</li> <li>
<code>"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: amount of allocated memory.</li> <li>
<code>"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: number of reserved segments from <code>cudaMalloc()</code>.</li> <li>
<code>"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: amount of reserved memory.</li> <li>
<code>"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: number of active memory blocks.</li> <li>
<code>"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: amount of active memory.</li> <li>
<code>"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: number of inactive, non-releasable memory blocks.</li> <li>
<code>"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"</code>: amount of inactive, non-releasable memory.</li> </ul> <p>For these core statistics, values are broken down as follows.</p> <p>Pool type:</p> <ul class="simple"> <li>
<code>all</code>: combined statistics across all memory pools.</li> <li>
<code>large_pool</code>: statistics for the large allocation pool (as of October 2019, for size &gt;= 1MB allocations).</li> <li>
<code>small_pool</code>: statistics for the small allocation pool (as of October 2019, for size &lt; 1MB allocations).</li> </ul> <p>Metric type:</p> <ul class="simple"> <li>
<code>current</code>: current value of this metric.</li> <li>
<code>peak</code>: maximum value of this metric.</li> <li>
<code>allocated</code>: historical total increase in this metric.</li> <li>
<code>freed</code>: historical total decrease in this metric.</li> </ul> <p>In addition to the core statistics, we also provide some simple event counters:</p> <ul class="simple"> <li>
<code>"num_alloc_retries"</code>: number of failed <code>cudaMalloc</code> calls that result in a cache flush and retry.</li> <li>
<code>"num_ooms"</code>: number of out-of-memory errors thrown.</li> </ul> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistics for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_summary">
<code>torch.cuda.memory_summary(device=None, abbreviated=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_summary"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a human-readable printout of the current memory allocator statistics for a given device.</p> <p>This can be useful to display periodically during training, or when handling out-of-memory exceptions.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns printout for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</li> <li>
<strong>abbreviated</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a><em>, </em><em>optional</em>) – whether to return an abbreviated summary (default: False).</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_snapshot">
<code>torch.cuda.memory_snapshot()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_snapshot"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a snapshot of the CUDA memory allocator state across all devices.</p> <p>Interpreting the output of this function requires familiarity with the memory allocator internals.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_allocated">
<code>torch.cuda.memory_allocated(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_allocated"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the current GPU memory occupied by tensors in bytes for a given device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This is likely less than the amount shown in <code>nvidia-smi</code> since some unused memory can be held by the caching allocator and some context needs to be created on GPU. See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.max_memory_allocated">
<code>torch.cuda.max_memory_allocated(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_allocated"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p> <p>By default, this returns the peak allocated memory since the beginning of this program. <code>reset_peak_stats()</code> can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak allocated memory usage of each iteration in a training loop.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.reset_max_memory_allocated">
<code>torch.cuda.reset_max_memory_allocated(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#reset_max_memory_allocated"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</p> <p>See <a class="reference internal" href="#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code>max_memory_allocated()</code></a> for details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function now calls <code>reset_peak_memory_stats()</code>, which resets /all/ peak memory stats.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_reserved">
<code>torch.cuda.memory_reserved(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_reserved"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.max_memory_reserved">
<code>torch.cuda.max_memory_reserved(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_reserved"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p> <p>By default, this returns the peak cached memory since the beginning of this program. <code>reset_peak_stats()</code> can be used to reset the starting point in tracking this metric. For example, these two functions can measure the peak cached memory amount of each iteration in a training loop.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.set_per_process_memory_fraction">
<code>torch.cuda.set_per_process_memory_fraction(fraction, device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#set_per_process_memory_fraction"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set memory fraction for a process. The fraction is used to limit an caching allocator to allocated memory on a CUDA device. The allowed value equals the total visible memory multiplied fraction. If trying to allocate more than the allowed value in a process, will raise an out of memory error in allocator.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>fraction</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a>) – Range: 0~1. Allowed memory equals total_memory * fraction.</li> <li>
<strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. If it is <code>None</code> the default CUDA device is used.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In general, the total available free memory is less than the total capacity.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.memory_cached">
<code>torch.cuda.memory_cached(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#memory_cached"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Deprecated; see <a class="reference internal" href="#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code>memory_reserved()</code></a>.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.max_memory_cached">
<code>torch.cuda.max_memory_cached(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#max_memory_cached"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Deprecated; see <a class="reference internal" href="#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code>max_memory_reserved()</code></a>.</p> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.reset_max_memory_cached">
<code>torch.cuda.reset_max_memory_cached(device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/memory.html#reset_max_memory_cached"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</p> <p>See <a class="reference internal" href="#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code>max_memory_cached()</code></a> for details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>device</strong> (<a class="reference internal" href="tensor_attributes#torch.torch.device" title="torch.torch.device">torch.device</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em>, </em><em>optional</em>) – selected device. Returns statistic for the current device, given by <a class="reference internal" href="#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device()</code></a>, if <a class="reference internal" href="#torch.cuda.device" title="torch.cuda.device"><code>device</code></a> is <code>None</code> (default).</p> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function now calls <code>reset_peak_memory_stats()</code>, which resets /all/ peak memory stats.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/1.8.0/notes/cuda.html#cuda-memory-management"><span class="std std-ref">Memory management</span></a> for more details about GPU memory management.</p> </div> </dd>
</dl>   <h2 id="nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</h2> <dl class="function"> <dt id="torch.cuda.nvtx.mark">
<code>torch.cuda.nvtx.mark(msg)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#mark"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Describe an instantaneous event that occurred at some point.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with the event.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.nvtx.range_push">
<code>torch.cuda.nvtx.range_push(msg)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#range_push"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>msg</strong> (<em>string</em>) – ASCII message to associate with range</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.cuda.nvtx.range_pop">
<code>torch.cuda.nvtx.range_pop()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/cuda/nvtx.html#range_pop"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/cuda.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/cuda.html</a>
  </p>
</div>
