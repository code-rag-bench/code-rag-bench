<h1 id="torch-utils-cpp-extension">torch.utils.cpp_extension</h1> <dl class="function"> <dt id="torch.utils.cpp_extension.CppExtension">
<code>torch.utils.cpp_extension.CppExtension(name, sources, *args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#CppExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a <code>setuptools.Extension</code> for C++.</p> <p>Convenience method that creates a <code>setuptools.Extension</code> with the bare minimum (but often sufficient) arguments to build a C++ extension.</p> <p>All arguments are forwarded to the <code>setuptools.Extension</code> constructor.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from setuptools import setup
&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CppExtension
&gt;&gt;&gt; setup(
        name='extension',
        ext_modules=[
            CppExtension(
                name='extension',
                sources=['extension.cpp'],
                extra_compile_args=['-g']),
        ],
        cmdclass={
            'build_ext': BuildExtension
        })
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.CUDAExtension">
<code>torch.utils.cpp_extension.CUDAExtension(name, sources, *args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#CUDAExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Creates a <code>setuptools.Extension</code> for CUDA/C++.</p> <p>Convenience method that creates a <code>setuptools.Extension</code> with the bare minimum (but often sufficient) arguments to build a CUDA/C++ extension. This includes the CUDA include path, library path and runtime library.</p> <p>All arguments are forwarded to the <code>setuptools.Extension</code> constructor.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from setuptools import setup
&gt;&gt;&gt; from torch.utils.cpp_extension import BuildExtension, CUDAExtension
&gt;&gt;&gt; setup(
        name='cuda_extension',
        ext_modules=[
            CUDAExtension(
                    name='cuda_extension',
                    sources=['extension.cpp', 'extension_kernel.cu'],
                    extra_compile_args={'cxx': ['-g'],
                                        'nvcc': ['-O2']})
        ],
        cmdclass={
            'build_ext': BuildExtension
        })
</pre> <p>Compute capabilities:</p> <p>By default the extension will be compiled to run on all archs of the cards visible during the building process of the extension, plus PTX. If down the road a new card is installed the extension may need to be recompiled. If a visible card has a compute capability (CC) that’s newer than the newest version for which your nvcc can build fully-compiled binaries, Pytorch will make nvcc fall back to building kernels with the newest version of PTX your nvcc does support (see below for details on PTX).</p> <p>You can override the default behavior using <code>TORCH_CUDA_ARCH_LIST</code> to explicitly specify which CCs you want the extension to support:</p> <p>TORCH_CUDA_ARCH_LIST=”6.1 8.6” python build_my_extension.py TORCH_CUDA_ARCH_LIST=”5.2 6.0 6.1 7.0 7.5 8.0 8.6+PTX” python build_my_extension.py</p> <p>The +PTX option causes extension kernel binaries to include PTX instructions for the specified CC. PTX is an intermediate representation that allows kernels to runtime-compile for any CC &gt;= the specified CC (for example, 8.6+PTX generates PTX that can runtime-compile for any GPU with CC &gt;= 8.6). This improves your binary’s forward compatibility. However, relying on older PTX to provide forward compat by runtime-compiling for newer CCs can modestly reduce performance on those newer CCs. If you know exact CC(s) of the GPUs you want to target, you’re always better off specifying them individually. For example, if you want your extension to run on 8.0 and 8.6, “8.0+PTX” would work functionally because it includes PTX that can runtime-compile for 8.6, but “8.0 8.6” would be better.</p> <p>Note that while it’s possible to include all supported archs, the more archs get included the slower the building process will be, as it will build a separate kernel image for each arch.</p> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.BuildExtension">
<code>torch.utils.cpp_extension.BuildExtension(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#BuildExtension"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A custom <code>setuptools</code> build extension .</p> <p>This <code>setuptools.build_ext</code> subclass takes care of passing the minimum required compiler flags (e.g. <code>-std=c++14</code>) as well as mixed C++/CUDA compilation (and support for CUDA files in general).</p> <p>When using <a class="reference internal" href="#torch.utils.cpp_extension.BuildExtension" title="torch.utils.cpp_extension.BuildExtension"><code>BuildExtension</code></a>, it is allowed to supply a dictionary for <code>extra_compile_args</code> (rather than the usual list) that maps from languages (<code>cxx</code> or <code>nvcc</code>) to a list of additional compiler flags to supply to the compiler. This makes it possible to supply different flags to the C++ and CUDA compiler during mixed compilation.</p> <p><code>use_ninja</code> (bool): If <code>use_ninja</code> is <code>True</code> (default), then we attempt to build using the Ninja backend. Ninja greatly speeds up compilation compared to the standard <code>setuptools.build_ext</code>. Fallbacks to the standard distutils backend if Ninja is not available.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the <code>MAX_JOBS</code> environment variable to a non-negative number.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.load">
<code>torch.utils.cpp_extension.load(name, sources, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, is_standalone=False, keep_intermediates=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#load"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Loads a PyTorch C++ extension just-in-time (JIT).</p> <p>To load an extension, a Ninja build file is emitted, which is used to compile the given sources into a dynamic library. This library is subsequently loaded into the current Python process as a module and returned from this function, ready for use.</p> <p>By default, the directory to which the build file is emitted and the resulting library compiled to is <code>&lt;tmp&gt;/torch_extensions/&lt;name&gt;</code>, where <code>&lt;tmp&gt;</code> is the temporary folder on the current platform and <code>&lt;name&gt;</code> the name of the extension. This location can be overridden in two ways. First, if the <code>TORCH_EXTENSIONS_DIR</code> environment variable is set, it replaces <code>&lt;tmp&gt;/torch_extensions</code> and all extensions will be compiled into subfolders of this directory. Second, if the <code>build_directory</code> argument to this function is supplied, it overrides the entire path, i.e. the library will be compiled into that folder directly.</p> <p>To compile the sources, the default system compiler (<code>c++</code>) is used, which can be overridden by setting the <code>CXX</code> environment variable. To pass additional arguments to the compilation process, <code>extra_cflags</code> or <code>extra_ldflags</code> can be provided. For example, to compile your extension with optimizations, pass <code>extra_cflags=['-O3']</code>. You can also use <code>extra_cflags</code> to pass further include directories.</p> <p>CUDA support with mixed compilation is provided. Simply pass CUDA source files (<code>.cu</code> or <code>.cuh</code>) along with other sources. Such files will be detected and compiled with nvcc rather than the C++ compiler. This includes passing the CUDA lib64 directory as a library directory, and linking <code>cudart</code>. You can pass additional flags to nvcc via <code>extra_cuda_cflags</code>, just like with <code>extra_cflags</code> for C++. Various heuristics for finding the CUDA install directory are used, which usually work fine. If not, setting the <code>CUDA_HOME</code> environment variable is the safest option.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>name</strong> – The name of the extension to build. This MUST be the same as the name of the pybind11 module!</li> <li>
<strong>sources</strong> – A list of relative or absolute paths to C++ source files.</li> <li>
<strong>extra_cflags</strong> – optional list of compiler flags to forward to the build.</li> <li>
<strong>extra_cuda_cflags</strong> – optional list of compiler flags to forward to nvcc when building CUDA sources.</li> <li>
<strong>extra_ldflags</strong> – optional list of linker flags to forward to the build.</li> <li>
<strong>extra_include_paths</strong> – optional list of include directories to forward to the build.</li> <li>
<strong>build_directory</strong> – optional path to use as build workspace.</li> <li>
<strong>verbose</strong> – If <code>True</code>, turns on verbose logging of load steps.</li> <li>
<strong>with_cuda</strong> – Determines whether CUDA headers and libraries are added to the build. If set to <code>None</code> (default), this value is automatically determined based on the existence of <code>.cu</code> or <code>.cuh</code> in <code>sources</code>. Set it to <code>True`</code> to force CUDA headers and libraries to be included.</li> <li>
<strong>is_python_module</strong> – If <code>True</code> (default), imports the produced shared library as a Python module. If <code>False</code>, behavior depends on <code>is_standalone</code>.</li> <li>
<strong>is_standalone</strong> – If <code>False</code> (default) loads the constructed extension into the process as a plain dynamic library. If <code>True</code>, build a standalone executable.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<p>Returns the loaded PyTorch extension as a Python module.</p> <dl class="simple"> <dt>
<code>If is_python_module is False and is_standalone is False:</code> </dt>
<dd>
<p>Returns nothing. (The shared library is loaded into the process as a side effect.)</p> </dd> <dt>
<code>If is_standalone is True.</code> </dt>
<dd>
<p>Return the path to the executable. (On Windows, TORCH_LIB_PATH is added to the PATH environment variable as a side effect.)</p> </dd> </dl> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>If <code>is_python_module</code> is <code>True</code></p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from torch.utils.cpp_extension import load
&gt;&gt;&gt; module = load(
        name='extension',
        sources=['extension.cpp', 'extension_kernel.cu'],
        extra_cflags=['-O2'],
        verbose=True)
</pre> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.load_inline">
<code>torch.utils.cpp_extension.load_inline(name, cpp_sources, cuda_sources=None, functions=None, extra_cflags=None, extra_cuda_cflags=None, extra_ldflags=None, extra_include_paths=None, build_directory=None, verbose=False, with_cuda=None, is_python_module=True, with_pytorch_error_handling=True, keep_intermediates=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#load_inline"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Loads a PyTorch C++ extension just-in-time (JIT) from string sources.</p> <p>This function behaves exactly like <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a>, but takes its sources as strings rather than filenames. These strings are stored to files in the build directory, after which the behavior of <a class="reference internal" href="#torch.utils.cpp_extension.load_inline" title="torch.utils.cpp_extension.load_inline"><code>load_inline()</code></a> is identical to <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a>.</p> <p>See <a class="reference external" href="https://github.com/pytorch/pytorch/blob/master/test/test_cpp_extensions.py">the tests</a> for good examples of using this function.</p> <p>Sources may omit two required parts of a typical non-inline C++ extension: the necessary header includes, as well as the (pybind11) binding code. More precisely, strings passed to <code>cpp_sources</code> are first concatenated into a single <code>.cpp</code> file. This file is then prepended with <code>#include
&lt;torch/extension.h&gt;</code>.</p> <p>Furthermore, if the <code>functions</code> argument is supplied, bindings will be automatically generated for each function specified. <code>functions</code> can either be a list of function names, or a dictionary mapping from function names to docstrings. If a list is given, the name of each function is used as its docstring.</p> <p>The sources in <code>cuda_sources</code> are concatenated into a separate <code>.cu</code> file and prepended with <code>torch/types.h</code>, <code>cuda.h</code> and <code>cuda_runtime.h</code> includes. The <code>.cpp</code> and <code>.cu</code> files are compiled separately, but ultimately linked into a single library. Note that no bindings are generated for functions in <code>cuda_sources</code> per se. To bind to a CUDA kernel, you must create a C++ function that calls it, and either declare or define this C++ function in one of the <code>cpp_sources</code> (and include its name in <code>functions</code>).</p> <p>See <a class="reference internal" href="#torch.utils.cpp_extension.load" title="torch.utils.cpp_extension.load"><code>load()</code></a> for a description of arguments omitted below.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>cpp_sources</strong> – A string, or list of strings, containing C++ source code.</li> <li>
<strong>cuda_sources</strong> – A string, or list of strings, containing CUDA source code.</li> <li>
<strong>functions</strong> – A list of function names for which to generate function bindings. If a dictionary is given, it should map function names to docstrings (which are otherwise just the function names).</li> <li>
<strong>with_cuda</strong> – Determines whether CUDA headers and libraries are added to the build. If set to <code>None</code> (default), this value is automatically determined based on whether <code>cuda_sources</code> is provided. Set it to <code>True</code> to force CUDA headers and libraries to be included.</li> <li>
<strong>with_pytorch_error_handling</strong> – Determines whether pytorch error and warning macros are handled by pytorch instead of pybind. To do this, each function <code>foo</code> is called via an intermediary <code>_safe_foo</code> function. This redirection might cause issues in obscure cases of cpp. This flag should be set to <code>False</code> when this redirect causes issues.</li> </ul> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; from torch.utils.cpp_extension import load_inline
&gt;&gt;&gt; source = \'\'\'
at::Tensor sin_add(at::Tensor x, at::Tensor y) {
  return x.sin() + y.sin();
}
\'\'\'
&gt;&gt;&gt; module = load_inline(name='inline_extension',
                         cpp_sources=[source],
                         functions=['sin_add'])
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>By default, the Ninja backend uses #CPUS + 2 workers to build the extension. This may use up too many resources on some systems. One can control the number of workers by setting the <code>MAX_JOBS</code> environment variable to a non-negative number.</p> </div> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.include_paths">
<code>torch.utils.cpp_extension.include_paths(cuda=False)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#include_paths"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Get the include paths required to build a C++ or CUDA extension.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>cuda</strong> – If <code>True</code>, includes CUDA-specific include paths.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A list of include path strings.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.check_compiler_abi_compatibility">
<code>torch.utils.cpp_extension.check_compiler_abi_compatibility(compiler)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#check_compiler_abi_compatibility"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Verifies that the given compiler is ABI-compatible with PyTorch.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>compiler</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.9)">str</a>) – The compiler executable name to check (e.g. <code>g++</code>). Must be executable in a shell process.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>False if the compiler is (likely) ABI-incompatible with PyTorch, else True.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.verify_ninja_availability">
<code>torch.utils.cpp_extension.verify_ninja_availability()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#verify_ninja_availability"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Raises <code>RuntimeError</code> if <a class="reference external" href="https://ninja-build.org/">ninja</a> build system is not available on the system, does nothing otherwise.</p> </dd>
</dl> <dl class="function"> <dt id="torch.utils.cpp_extension.is_ninja_available">
<code>torch.utils.cpp_extension.is_ninja_available()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/utils/cpp_extension.html#is_ninja_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns <code>True</code> if the <a class="reference external" href="https://ninja-build.org/">ninja</a> build system is available on the system, <code>False</code> otherwise.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/cpp_extension.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/cpp_extension.html</a>
  </p>
</div>
