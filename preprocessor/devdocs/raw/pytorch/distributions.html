<h1 id="probability-distributions-torch-distributions">Probability distributions - torch.distributions</h1> <p id="module-torch.distributions">The <code>distributions</code> package contains parameterizable probability distributions and sampling functions. This allows the construction of stochastic computation graphs and stochastic gradient estimators for optimization. This package generally follows the design of the <a class="reference external" href="https://arxiv.org/abs/1711.10604">TensorFlow Distributions</a> package.</p> <p>It is not possible to directly backpropagate through random samples. However, there are two main methods for creating surrogate functions that can be backpropagated through. These are the score function estimator/likelihood ratio estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly seen as the basis for policy gradient methods in reinforcement learning, and the pathwise derivative estimator is commonly seen in the reparameterization trick in variational autoencoders. Whilst the score function only requires the value of samples <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x)</annotation></semantics></math></span></span> </span>, the pathwise derivative requires the derivative <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f'(x)</annotation></semantics></math></span></span> </span>. The next sections discuss these two in a reinforcement learning example. For more details see <a class="reference external" href="https://arxiv.org/abs/1506.05254">Gradient Estimation Using Stochastic Computation Graphs</a> .</p>  <h2 id="score-function">Score function</h2> <p>When the probability density function is differentiable with respect to its parameters, we only need <code>sample()</code> and <code>log_prob()</code> to implement REINFORCE:</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Δ</mi><mi>θ</mi><mo>=</mo><mi>α</mi><mi>r</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>log</mi><mo>⁡</mo><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><msup><mi>π</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><mrow><mi mathvariant="normal">∂</mi><mi>θ</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\Delta\theta = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span></span> </span> are the parameters, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span></span> </span> is the learning rate, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span></span> </span> is the reward and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><msup><mi>π</mi><mi>θ</mi></msup><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(a|\pi^\theta(s))</annotation></semantics></math></span></span> </span> is the probability of taking action <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span></span> </span> in state <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span></span> </span> given policy <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>π</mi><mi>θ</mi></msup></mrow><annotation encoding="application/x-tex">\pi^\theta</annotation></semantics></math></span></span> </span>.</p> <p>In practice we would sample an action from the output of a network, apply this action in an environment, and then use <code>log_prob</code> to construct an equivalent loss function. Note that we use a negative because optimizers use gradient descent, whilst the rule above assumes gradient ascent. With a categorical policy, the code for implementing REINFORCE would be as follows:</p> <pre data-language="python">probs = policy_network(state)
# Note that this is equivalent to what used to be called multinomial
m = Categorical(probs)
action = m.sample()
next_state, reward = env.step(action)
loss = -m.log_prob(action) * reward
loss.backward()
</pre>   <h2 id="pathwise-derivative">Pathwise derivative</h2> <p>The other way to implement these stochastic/policy gradients would be to use the reparameterization trick from the <code>rsample()</code> method, where the parameterized random variable can be constructed via a parameterized deterministic function of a parameter-free random variable. The reparameterized sample therefore becomes differentiable. The code for implementing the pathwise derivative would be as follows:</p> <pre data-language="python">params = policy_network(state)
m = Normal(*params)
# Any distribution with .has_rsample == True could work based on the application
action = m.rsample()
next_state, reward = env.step(action)  # Assuming that reward is differentiable
loss = -reward
loss.backward()
</pre>   <h2 id="distribution"><span class="hidden-section">Distribution</span></h2> <dl class="class"> <dt id="torch.distributions.distribution.Distribution">
<code>class torch.distributions.distribution.Distribution(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.9)"><code>object</code></a></p> <p>Distribution is the abstract base class for probability distributions.</p> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.arg_constraints">
<code>property arg_constraints</code> </dt> <dd>
<p>Returns a dictionary from argument names to <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> objects that should be satisfied by each argument of this distribution. Args that are not tensors need not appear in this dict.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.batch_shape">
<code>property batch_shape</code> </dt> <dd>
<p>Returns the shape over which parameters are batched.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.cdf"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the cumulative density/mass function evaluated at <code>value</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>value</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – </p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.entropy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns entropy of distribution, batched over batch_shape.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>Tensor of shape batch_shape.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns tensor containing all values supported by a discrete distribution. The result will enumerate over dimension 0, so the shape of the result will be <code>(cardinality,) + batch_shape + event_shape</code> (where <code>event_shape = ()</code> for univariate distributions).</p> <p>Note that this enumerates over all batched tensors in lock-step <code>[[0, 0], [1, 1], …]</code>. With <code>expand=False</code>, enumeration happens along dim 0, but with the remaining batch dimensions being singleton dimensions, <code>[[0], [1], ..</code>.</p> <p>To iterate over the full Cartesian product use <code>itertools.product(m.enumerate_support())</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>expand</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – whether to expand the support over the batch dims to match the distribution’s <code>batch_shape</code>.</p> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Tensor iterating over dimension 0.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.event_shape">
<code>property event_shape</code> </dt> <dd>
<p>Returns the shape of a single sample (without batching).</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.expand"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a new distribution instance (or populates an existing instance provided by a derived class) with batch dimensions expanded to <code>batch_shape</code>. This method calls <a class="reference internal" href="tensors#torch.Tensor.expand" title="torch.Tensor.expand"><code>expand</code></a> on the distribution’s parameters. As such, this does not allocate new memory for the expanded distribution instance. Additionally, this does not repeat any args checking or parameter broadcasting in <code>__init__.py</code>, when an instance is first created.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>batch_shape</strong> (<em>torch.Size</em>) – the desired expanded size.</li> <li>
<strong>_instance</strong> – new instance provided by subclasses that need to override <code>.expand</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>New distribution instance with batch dimensions expanded to <code>batch_size</code>.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.icdf"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the inverse cumulative density/mass function evaluated at <code>value</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>value</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – </p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the log of the probability density/mass function evaluated at <code>value</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>value</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – </p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.mean">
<code>property mean</code> </dt> <dd>
<p>Returns the mean of the distribution.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.perplexity">
<code>perplexity()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.perplexity"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns perplexity of distribution, batched over batch_shape.</p> <dl class="field-list simple"> <dt class="field-odd">Returns</dt> <dd class="field-odd">
<p>Tensor of shape batch_shape.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.rsample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.sample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.sample_n">
<code>sample_n(n)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.sample_n"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generates n samples or n batches of samples if the distribution parameters are batched.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.set_default_validate_args">
<code>static set_default_validate_args(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/distribution.html#Distribution.set_default_validate_args"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets whether validation is enabled or disabled.</p> <p>The default behavior mimics Python’s <code>assert</code> statement: validation is on by default, but is disabled if Python is run in optimized mode (via <code>python -O</code>). Validation may be expensive, so you may want to disable it once a model is working.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>value</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – Whether to enable validation.</p> </dd> </dl> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.stddev">
<code>property stddev</code> </dt> <dd>
<p>Returns the standard deviation of the distribution.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.support">
<code>property support</code> </dt> <dd>
<p>Returns a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> object representing this distribution’s support.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.distribution.Distribution.variance">
<code>property variance</code> </dt> <dd>
<p>Returns the variance of the distribution.</p> </dd>
</dl> </dd>
</dl>   <h2 id="exponentialfamily"><span class="hidden-section">ExponentialFamily</span></h2> <dl class="class"> <dt id="torch.distributions.exp_family.ExponentialFamily">
<code>class torch.distributions.exp_family.ExponentialFamily(batch_shape=torch.Size([]), event_shape=torch.Size([]), validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exp_family.html#ExponentialFamily"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>ExponentialFamily is the abstract base class for probability distributions belonging to an exponential family, whose probability mass/density function has the form is defined below</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>p</mi><mi>F</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">;</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo stretchy="false">⟨</mo><mi>t</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mi>θ</mi><mo stretchy="false">⟩</mo><mo>−</mo><mi>F</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p_{F}(x; \theta) = \exp(\langle t(x), \theta\rangle - F(\theta) + k(x))</annotation></semantics></math></span></span></span> </div>
<p>where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span></span> </span> denotes the natural parameters, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">t(x)</annotation></semantics></math></span></span> </span> denotes the sufficient statistic, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F(\theta)</annotation></semantics></math></span></span> </span> is the log normalizer function for a given family and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">k(x)</annotation></semantics></math></span></span> </span> is the carrier measure.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This class is an intermediary between the <code>Distribution</code> class and distributions which belong to an exponential family mainly to check the correctness of the <code>.entropy()</code> and analytic KL divergence methods. We use this class to compute the entropy and KL divergence using the AD framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and Cross-entropies of Exponential Families).</p> </div> <dl class="method"> <dt id="torch.distributions.exp_family.ExponentialFamily.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exp_family.html#ExponentialFamily.entropy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Method to compute the entropy using Bregman divergence of the log normalizer.</p> </dd>
</dl> </dd>
</dl>   <h2 id="bernoulli"><span class="hidden-section">Bernoulli</span></h2> <dl class="class"> <dt id="torch.distributions.bernoulli.Bernoulli">
<code>class torch.distributions.bernoulli.Bernoulli(probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a Bernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.probs" title="torch.distributions.bernoulli.Bernoulli.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.bernoulli.Bernoulli.logits" title="torch.distributions.bernoulli.Bernoulli.logits"><code>logits</code></a> (but not both).</p> <p>Samples are binary (0 or 1). They take the value <code>1</code> with probability <code>p</code> and <code>0</code> with probability <code>1 - p</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Bernoulli(torch.tensor([0.3]))
&gt;&gt;&gt; m.sample()  # 30% chance 1; 70% chance 0
tensor([ 0.])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the probability of sampling <code>1</code>
</li> <li>
<strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the log-odds of sampling <code>1</code>
</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.bernoulli.Bernoulli.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.bernoulli.Bernoulli.has_enumerate_support">
<code>has_enumerate_support = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.bernoulli.Bernoulli.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.bernoulli.Bernoulli.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/bernoulli.html#Bernoulli.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.bernoulli.Bernoulli.support">
<code>support = Boolean()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.bernoulli.Bernoulli.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="beta"><span class="hidden-section">Beta</span></h2> <dl class="class"> <dt id="torch.distributions.beta.Beta">
<code>class torch.distributions.beta.Beta(concentration1, concentration0, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/beta.html#Beta"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Beta distribution parameterized by <a class="reference internal" href="#torch.distributions.beta.Beta.concentration1" title="torch.distributions.beta.Beta.concentration1"><code>concentration1</code></a> and <a class="reference internal" href="#torch.distributions.beta.Beta.concentration0" title="torch.distributions.beta.Beta.concentration0"><code>concentration0</code></a>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))
&gt;&gt;&gt; m.sample()  # Beta distributed with concentration concentration1 and concentration0
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – 1st concentration parameter of the distribution (often referred to as alpha)</li> <li>
<strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – 2nd concentration parameter of the distribution (often referred to as beta)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.beta.Beta.arg_constraints">
<code>arg_constraints = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.concentration0">
<code>property concentration0</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.concentration1">
<code>property concentration1</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/beta.html#Beta.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/beta.html#Beta.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.beta.Beta.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/beta.html#Beta.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.rsample">
<code>rsample(sample_shape=())</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/beta.html#Beta.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.beta.Beta.support">
<code>support = Interval(lower_bound=0.0, upper_bound=1.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.beta.Beta.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="binomial"><span class="hidden-section">Binomial</span></h2> <dl class="class"> <dt id="torch.distributions.binomial.Binomial">
<code>class torch.distributions.binomial.Binomial(total_count=1, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Binomial distribution parameterized by <code>total_count</code> and either <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code>logits</code></a> (but not both). <code>total_count</code> must be broadcastable with <a class="reference internal" href="#torch.distributions.binomial.Binomial.probs" title="torch.distributions.binomial.Binomial.probs"><code>probs</code></a>/<a class="reference internal" href="#torch.distributions.binomial.Binomial.logits" title="torch.distributions.binomial.Binomial.logits"><code>logits</code></a>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Binomial(100, torch.tensor([0 , .2, .8, 1]))
&gt;&gt;&gt; x = m.sample()
tensor([   0.,   22.,   71.,  100.])

&gt;&gt;&gt; m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))
&gt;&gt;&gt; x = m.sample()
tensor([[ 4.,  5.],
        [ 7.,  6.]])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – number of Bernoulli trials</li> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Event probabilities</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Event log-odds</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.binomial.Binomial.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0), 'total_count': IntegerGreaterThan(lower_bound=0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.binomial.Binomial.has_enumerate_support">
<code>has_enumerate_support = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.binomial.Binomial.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.binomial.Binomial.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/binomial.html#Binomial.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.binomial.Binomial.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="categorical"><span class="hidden-section">Categorical</span></h2> <dl class="class"> <dt id="torch.distributions.categorical.Categorical">
<code>class torch.distributions.categorical.Categorical(probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a categorical distribution parameterized by either <a class="reference internal" href="#torch.distributions.categorical.Categorical.probs" title="torch.distributions.categorical.Categorical.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.categorical.Categorical.logits" title="torch.distributions.categorical.Categorical.logits"><code>logits</code></a> (but not both).</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>It is equivalent to the distribution that <a class="reference internal" href="generated/torch.multinomial#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a> samples from.</p> </div> <p>Samples are integers from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><mn>0</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>K</mi><mo>−</mo><mn>1</mn><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{0, \ldots, K-1\}</annotation></semantics></math></span></span> </span> where <code>K</code> is <code>probs.size(-1)</code>.</p> <p>If <code>probs</code> is 1-dimensional with length-<code>K</code>, each element is the relative probability of sampling the class at that index.</p> <p>If <code>probs</code> is N-dimensional, the first N-1 dimensions are treated as a batch of relative probability vectors.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>probs</code> argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:<code>probs</code> will return this normalized value. The <code>logits</code> argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:<code>logits</code> will return this normalized value.</p> </div> <p>See also: <a class="reference internal" href="generated/torch.multinomial#torch.multinomial" title="torch.multinomial"><code>torch.multinomial()</code></a></p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3
tensor(3)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event probabilities</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event log probabilities (unnormalized)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.categorical.Categorical.arg_constraints">
<code>arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.categorical.Categorical.has_enumerate_support">
<code>has_enumerate_support = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.categorical.Categorical.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.categorical.Categorical.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/categorical.html#Categorical.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.categorical.Categorical.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="cauchy"><span class="hidden-section">Cauchy</span></h2> <dl class="class"> <dt id="torch.distributions.cauchy.Cauchy">
<code>class torch.distributions.cauchy.Cauchy(loc, scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of independent normally distributed random variables with means <code>0</code> follows a Cauchy distribution.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1
tensor([ 2.3214])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mode or median of the distribution.</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – half width at half maximum.</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.cauchy.Cauchy.arg_constraints">
<code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.cauchy.Cauchy.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/cauchy.html#Cauchy.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.cauchy.Cauchy.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.cauchy.Cauchy.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="chi2"><span class="hidden-section">Chi2</span></h2> <dl class="class"> <dt id="torch.distributions.chi2.Chi2">
<code>class torch.distributions.chi2.Chi2(df, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/chi2.html#Chi2"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.gamma.Gamma" title="torch.distributions.gamma.Gamma"><code>torch.distributions.gamma.Gamma</code></a></p> <p>Creates a Chi2 distribution parameterized by shape parameter <a class="reference internal" href="#torch.distributions.chi2.Chi2.df" title="torch.distributions.chi2.Chi2.df"><code>df</code></a>. This is exactly equivalent to <code>Gamma(alpha=0.5*df, beta=0.5)</code></p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Chi2(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Chi2 distributed with shape df=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – shape parameter of the distribution</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.chi2.Chi2.arg_constraints">
<code>arg_constraints = {'df': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.chi2.Chi2.df">
<code>property df</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.chi2.Chi2.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/chi2.html#Chi2.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl>   <h2 id="continuousbernoulli"><span class="hidden-section">ContinuousBernoulli</span></h2> <dl class="class"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli">
<code>class torch.distributions.continuous_bernoulli.ContinuousBernoulli(probs=None, logits=None, lims=(0.499, 0.501), validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a continuous Bernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs" title="torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits" title="torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits"><code>logits</code></a> (but not both).</p> <p>The distribution is supported in [0, 1] and parameterized by ‘probs’ (in (0,1)) or ‘logits’ (real-valued). Note that, unlike the Bernoulli, ‘probs’ does not correspond to a probability and ‘logits’ does not correspond to log-odds, but the same names are used due to the similarity with the Bernoulli. See [1] for more details.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = ContinuousBernoulli(torch.tensor([0.3]))
&gt;&gt;&gt; m.sample()
tensor([ 0.2538])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – (0,1) valued parameters</li> <li>
<strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – real valued parameters whose sigmoid matches ‘probs’</li> </ul> </dd> </dl> <p>[1] The continuous Bernoulli: fixing a pervasive error in variational autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019. <a class="reference external" href="https://arxiv.org/abs/1907.06845">https://arxiv.org/abs/1907.06845</a></p> <dl class="attribute"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/continuous_bernoulli.html#ContinuousBernoulli.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.support">
<code>support = Interval(lower_bound=0.0, upper_bound=1.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.continuous_bernoulli.ContinuousBernoulli.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="dirichlet"><span class="hidden-section">Dirichlet</span></h2> <dl class="class"> <dt id="torch.distributions.dirichlet.Dirichlet">
<code>class torch.distributions.dirichlet.Dirichlet(concentration, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/dirichlet.html#Dirichlet"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a Dirichlet distribution parameterized by concentration <code>concentration</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Dirichlet(torch.tensor([0.5, 0.5]))
&gt;&gt;&gt; m.sample()  # Dirichlet distributed with concentrarion concentration
tensor([ 0.1046,  0.8954])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>concentration</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – concentration parameter of the distribution (often referred to as alpha)</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.dirichlet.Dirichlet.arg_constraints">
<code>arg_constraints = {'concentration': IndependentConstraint(GreaterThan(lower_bound=0.0), 1)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/dirichlet.html#Dirichlet.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/dirichlet.html#Dirichlet.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.dirichlet.Dirichlet.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/dirichlet.html#Dirichlet.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.rsample">
<code>rsample(sample_shape=())</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/dirichlet.html#Dirichlet.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.dirichlet.Dirichlet.support">
<code>support = Simplex()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.dirichlet.Dirichlet.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="exponential"><span class="hidden-section">Exponential</span></h2> <dl class="class"> <dt id="torch.distributions.exponential.Exponential">
<code>class torch.distributions.exponential.Exponential(rate, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a Exponential distribution parameterized by <code>rate</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Exponential(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Exponential distributed with rate=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – rate = 1 / scale of the distribution</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.exponential.Exponential.arg_constraints">
<code>arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.exponential.Exponential.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/exponential.html#Exponential.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.exponential.Exponential.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.exponential.Exponential.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="fishersnedecor"><span class="hidden-section">FisherSnedecor</span></h2> <dl class="class"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor">
<code>class torch.distributions.fishersnedecor.FisherSnedecor(df1, df2, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Fisher-Snedecor distribution parameterized by <code>df1</code> and <code>df2</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2
tensor([ 0.2453])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>df1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – degrees of freedom parameter 1</li> <li>
<strong>df2</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – degrees of freedom parameter 2</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.arg_constraints">
<code>arg_constraints = {'df1': GreaterThan(lower_bound=0.0), 'df2': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/fishersnedecor.html#FisherSnedecor.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.fishersnedecor.FisherSnedecor.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="gamma"><span class="hidden-section">Gamma</span></h2> <dl class="class"> <dt id="torch.distributions.gamma.Gamma">
<code>class torch.distributions.gamma.Gamma(concentration, rate, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gamma.html#Gamma"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a Gamma distribution parameterized by shape <code>concentration</code> and <code>rate</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Gamma distributed with concentration=1 and rate=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – shape parameter of the distribution (often referred to as alpha)</li> <li>
<strong>rate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – rate = 1 / scale of the distribution (often referred to as beta)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.gamma.Gamma.arg_constraints">
<code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'rate': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gamma.html#Gamma.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gamma.html#Gamma.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.gamma.Gamma.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gamma.html#Gamma.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gamma.html#Gamma.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.gamma.Gamma.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gamma.Gamma.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="geometric"><span class="hidden-section">Geometric</span></h2> <dl class="class"> <dt id="torch.distributions.geometric.Geometric">
<code>class torch.distributions.geometric.Geometric(probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Geometric distribution parameterized by <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code>probs</code></a>, where <a class="reference internal" href="#torch.distributions.geometric.Geometric.probs" title="torch.distributions.geometric.Geometric.probs"><code>probs</code></a> is the probability of success of Bernoulli trials. It represents the probability that in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k + 1</annotation></semantics></math></span></span> </span> Bernoulli trials, the first <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span></span> </span> trials failed, before seeing a success.</p> <p>Samples are non-negative integers [0, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>inf</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\inf</annotation></semantics></math></span></span> </span>).</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Geometric(torch.tensor([0.3]))
&gt;&gt;&gt; m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0
tensor([ 2.])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the probability of sampling <code>1</code>. Must be in range (0, 1]</li> <li>
<strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the log-odds of sampling <code>1</code>.</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.geometric.Geometric.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.geometric.Geometric.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.mean">
<code>property mean</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.geometric.Geometric.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/geometric.html#Geometric.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.geometric.Geometric.support">
<code>support = IntegerGreaterThan(lower_bound=0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.geometric.Geometric.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="gumbel"><span class="hidden-section">Gumbel</span></h2> <dl class="class"> <dt id="torch.distributions.gumbel.Gumbel">
<code>class torch.distributions.gumbel.Gumbel(loc, scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gumbel.html#Gumbel"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Samples from a Gumbel Distribution.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # sample from Gumbel distribution with loc=1, scale=2
tensor([ 1.0124])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Location parameter of the distribution</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Scale parameter of the distribution</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.gumbel.Gumbel.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gumbel.html#Gumbel.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gumbel.html#Gumbel.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/gumbel.html#Gumbel.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.gumbel.Gumbel.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.gumbel.Gumbel.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="halfcauchy"><span class="hidden-section">HalfCauchy</span></h2> <dl class="class"> <dt id="torch.distributions.half_cauchy.HalfCauchy">
<code>class torch.distributions.half_cauchy.HalfCauchy(scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Creates a half-Cauchy distribution parameterized by <code>scale</code> where:</p> <pre data-language="python">X ~ Cauchy(0, scale)
Y = |X| ~ HalfCauchy(scale)
</pre> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = HalfCauchy(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # half-cauchy distributed with scale=1
tensor([ 2.3214])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scale of the full Cauchy distribution</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.half_cauchy.HalfCauchy.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.half_cauchy.HalfCauchy.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.icdf">
<code>icdf(prob)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_cauchy.html#HalfCauchy.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.scale">
<code>property scale</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.half_cauchy.HalfCauchy.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_cauchy.HalfCauchy.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="halfnormal"><span class="hidden-section">HalfNormal</span></h2> <dl class="class"> <dt id="torch.distributions.half_normal.HalfNormal">
<code>class torch.distributions.half_normal.HalfNormal(scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Creates a half-normal distribution parameterized by <code>scale</code> where:</p> <pre data-language="python">X ~ Normal(0, scale)
Y = |X| ~ HalfNormal(scale)
</pre> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = HalfNormal(torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # half-normal distributed with scale=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scale of the full Normal distribution</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.half_normal.HalfNormal.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.half_normal.HalfNormal.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.icdf">
<code>icdf(prob)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/half_normal.html#HalfNormal.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.scale">
<code>property scale</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.half_normal.HalfNormal.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.half_normal.HalfNormal.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="independent"><span class="hidden-section">Independent</span></h2> <dl class="class"> <dt id="torch.distributions.independent.Independent">
<code>class torch.distributions.independent.Independent(base_distribution, reinterpreted_batch_ndims, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Reinterprets some of the batch dims of a distribution as event dims.</p> <p>This is mainly useful for changing the shape of the result of <a class="reference internal" href="#torch.distributions.independent.Independent.log_prob" title="torch.distributions.independent.Independent.log_prob"><code>log_prob()</code></a>. For example to create a diagonal Normal distribution with the same shape as a Multivariate Normal distribution (so they are interchangeable), you can:</p> <pre data-language="python">&gt;&gt;&gt; loc = torch.zeros(3)
&gt;&gt;&gt; scale = torch.ones(3)
&gt;&gt;&gt; mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))
&gt;&gt;&gt; [mvn.batch_shape, mvn.event_shape]
[torch.Size(()), torch.Size((3,))]
&gt;&gt;&gt; normal = Normal(loc, scale)
&gt;&gt;&gt; [normal.batch_shape, normal.event_shape]
[torch.Size((3,)), torch.Size(())]
&gt;&gt;&gt; diagn = Independent(normal, 1)
&gt;&gt;&gt; [diagn.batch_shape, diagn.event_shape]
[torch.Size(()), torch.Size((3,))]
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>base_distribution</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution">torch.distributions.distribution.Distribution</a>) – a base distribution</li> <li>
<strong>reinterpreted_batch_ndims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – the number of batch dims to reinterpret as event dims</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.independent.Independent.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.has_enumerate_support">
<code>property has_enumerate_support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.has_rsample">
<code>property has_rsample</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/independent.html#Independent.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.independent.Independent.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="kumaraswamy"><span class="hidden-section">Kumaraswamy</span></h2> <dl class="class"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy">
<code>class torch.distributions.kumaraswamy.Kumaraswamy(concentration1, concentration0, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/kumaraswamy.html#Kumaraswamy"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Samples from a Kumaraswamy distribution.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Kumaraswamy(torch.Tensor([1.0]), torch.Tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Kumaraswamy distribution with concentration alpha=1 and beta=1
tensor([ 0.1729])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>concentration1</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – 1st concentration parameter of the distribution (often referred to as alpha)</li> <li>
<strong>concentration0</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – 2nd concentration parameter of the distribution (often referred to as beta)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration0': GreaterThan(lower_bound=0.0), 'concentration1': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/kumaraswamy.html#Kumaraswamy.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/kumaraswamy.html#Kumaraswamy.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.mean">
<code>property mean</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.support">
<code>support = Interval(lower_bound=0.0, upper_bound=1.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.kumaraswamy.Kumaraswamy.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="lkjcholesky"><span class="hidden-section">LKJCholesky</span></h2> <dl class="class"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky">
<code>class torch.distributions.lkj_cholesky.LKJCholesky(dim, concentration=1.0, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lkj_cholesky.html#LKJCholesky"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>LKJ distribution for lower Cholesky factor of correlation matrices. The distribution is controlled by <code>concentration</code> parameter <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>η</mi></mrow><annotation encoding="application/x-tex">\eta</annotation></semantics></math></span></span> </span> to make the probability of the correlation matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span></span> </span> generated from a Cholesky factor propotional to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>det</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>M</mi><msup><mo stretchy="false">)</mo><mrow><mi>η</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\det(M)^{\eta - 1}</annotation></semantics></math></span></span> </span>. Because of that, when <code>concentration == 1</code>, we have a uniform distribution over Cholesky factors of correlation matrices. Note that this distribution samples the Cholesky factor of correlation matrices and not the correlation matrices themselves and thereby differs slightly from the derivations in [1] for the <code>LKJCorr</code> distribution. For sampling, this uses the Onion method from [1] Section 3.</p>  <p>L ~ LKJCholesky(dim, concentration) X = L @ L’ ~ LKJCorr(dim, concentration)</p>  <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; l = LKJCholesky(3, 0.5)
&gt;&gt;&gt; l.sample()  # l @ l.T is a sample of a correlation 3x3 matrix
tensor([[ 1.0000,  0.0000,  0.0000],
        [ 0.3516,  0.9361,  0.0000],
        [-0.1899,  0.4748,  0.8593]])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dimension</strong> (<em>dim</em>) – dimension of the matrices</li> <li>
<strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – concentration/shape parameter of the distribution (often referred to as eta)</li> </ul> </dd> </dl> <p><strong>References</strong></p> <p>[1] <code>Generating random correlation matrices based on vines and extended onion method</code>, Daniel Lewandowski, Dorota Kurowicka, Harry Joe.</p> <dl class="attribute"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky.arg_constraints">
<code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lkj_cholesky.html#LKJCholesky.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lkj_cholesky.html#LKJCholesky.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lkj_cholesky.html#LKJCholesky.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lkj_cholesky.LKJCholesky.support">
<code>support = CorrCholesky()</code> </dt> 
</dl> </dd>
</dl>   <h2 id="laplace"><span class="hidden-section">Laplace</span></h2> <dl class="class"> <dt id="torch.distributions.laplace.Laplace">
<code>class torch.distributions.laplace.Laplace(loc, scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Laplace distribution parameterized by <code>loc</code> and <code>scale</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # Laplace distributed with loc=0, scale=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of the distribution</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scale of the distribution</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.laplace.Laplace.arg_constraints">
<code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.laplace.Laplace.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/laplace.html#Laplace.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.laplace.Laplace.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.laplace.Laplace.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="lognormal"><span class="hidden-section">LogNormal</span></h2> <dl class="class"> <dt id="torch.distributions.log_normal.LogNormal">
<code>class torch.distributions.log_normal.LogNormal(loc, scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/log_normal.html#LogNormal"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Creates a log-normal distribution parameterized by <a class="reference internal" href="#torch.distributions.log_normal.LogNormal.loc" title="torch.distributions.log_normal.LogNormal.loc"><code>loc</code></a> and <a class="reference internal" href="#torch.distributions.log_normal.LogNormal.scale" title="torch.distributions.log_normal.LogNormal.scale"><code>scale</code></a> where:</p> <pre data-language="python">X ~ Normal(loc, scale)
Y = exp(X) ~ LogNormal(loc, scale)
</pre> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # log-normal distributed with mean=0 and stddev=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of log of distribution</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – standard deviation of log of the distribution</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.log_normal.LogNormal.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/log_normal.html#LogNormal.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/log_normal.html#LogNormal.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.log_normal.LogNormal.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.loc">
<code>property loc</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.scale">
<code>property scale</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.log_normal.LogNormal.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.log_normal.LogNormal.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="lowrankmultivariatenormal"><span class="hidden-section">LowRankMultivariateNormal</span></h2> <dl class="class"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal">
<code>class torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal(loc, cov_factor, cov_diag, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a multivariate normal distribution with covariance matrix having a low-rank form parameterized by <code>cov_factor</code> and <code>cov_diag</code>:</p> <pre data-language="python">covariance_matrix = cov_factor @ cov_factor.T + cov_diag
</pre> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))
&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`
tensor([-0.2102, -0.5429])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of the distribution with shape <code>batch_shape + event_shape</code>
</li> <li>
<strong>cov_factor</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – factor part of low-rank form of covariance matrix with shape <code>batch_shape + event_shape + (rank,)</code>
</li> <li>
<strong>cov_diag</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – diagonal part of low-rank form of covariance matrix with shape <code>batch_shape + event_shape</code>
</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The computation for determinant and inverse of covariance matrix is avoided when <code>cov_factor.shape[1] &lt;&lt; cov_factor.shape[0]</code> thanks to <a class="reference external" href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">Woodbury matrix identity</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Matrix_determinant_lemma">matrix determinant lemma</a>. Thanks to these formulas, we just need to compute the determinant and inverse of the small size “capacitance” matrix:</p> <pre data-language="python">capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor
</pre> </div> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.arg_constraints">
<code>arg_constraints = {'cov_diag': IndependentConstraint(GreaterThan(lower_bound=0.0), 1), 'cov_factor': IndependentConstraint(Real(), 2), 'loc': IndependentConstraint(Real(), 1)}</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.covariance_matrix">
<code>covariance_matrix</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.covariance_matrix"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.mean">
<code>property mean</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.precision_matrix">
<code>precision_matrix</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.precision_matrix"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.scale_tril">
<code>scale_tril</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.scale_tril"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.support">
<code>support = IndependentConstraint(Real(), 1)</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.lowrank_multivariate_normal.LowRankMultivariateNormal.variance">
<code>variance</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/lowrank_multivariate_normal.html#LowRankMultivariateNormal.variance"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> </dd>
</dl>   <h2 id="mixturesamefamily"><span class="hidden-section">MixtureSameFamily</span></h2> <dl class="class"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily">
<code>class torch.distributions.mixture_same_family.MixtureSameFamily(mixture_distribution, component_distribution, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>The <code>MixtureSameFamily</code> distribution implements a (batch of) mixture distribution where all component are from different parameterizations of the same distribution type. It is parameterized by a <code>Categorical</code> “selecting distribution” (over <code>k</code> component) and a component distribution, i.e., a <code>Distribution</code> with a rightmost batch shape (equal to <code>[k]</code>) which indexes each (batch of) component.</p> <p>Examples:</p> <pre data-language="python"># Construct Gaussian Mixture Model in 1D consisting of 5 equally
# weighted normal distributions
&gt;&gt;&gt; mix = D.Categorical(torch.ones(5,))
&gt;&gt;&gt; comp = D.Normal(torch.randn(5,), torch.rand(5,))
&gt;&gt;&gt; gmm = MixtureSameFamily(mix, comp)

# Construct Gaussian Mixture Modle in 2D consisting of 5 equally
# weighted bivariate normal distributions
&gt;&gt;&gt; mix = D.Categorical(torch.ones(5,))
&gt;&gt;&gt; comp = D.Independent(D.Normal(
             torch.randn(5,2), torch.rand(5,2)), 1)
&gt;&gt;&gt; gmm = MixtureSameFamily(mix, comp)

# Construct a batch of 3 Gaussian Mixture Models in 2D each
# consisting of 5 random weighted bivariate normal distributions
&gt;&gt;&gt; mix = D.Categorical(torch.rand(3,5))
&gt;&gt;&gt; comp = D.Independent(D.Normal(
            torch.randn(3,5,2), torch.rand(3,5,2)), 1)
&gt;&gt;&gt; gmm = MixtureSameFamily(mix, comp)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mixture_distribution</strong> – <code>torch.distributions.Categorical</code>-like instance. Manages the probability of selecting component. The number of categories must match the rightmost batch dimension of the <code>component_distribution</code>. Must have either scalar <code>batch_shape</code> or <code>batch_shape</code> matching <code>component_distribution.batch_shape[:-1]</code>
</li> <li>
<strong>component_distribution</strong> – <code>torch.distributions.Distribution</code>-like instance. Right-most batch dimension indexes component.</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.cdf">
<code>cdf(x)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.component_distribution">
<code>property component_distribution</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.has_rsample">
<code>has_rsample = False</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.log_prob">
<code>log_prob(x)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.mixture_distribution">
<code>property mixture_distribution</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/mixture_same_family.html#MixtureSameFamily.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.mixture_same_family.MixtureSameFamily.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="multinomial"><span class="hidden-section">Multinomial</span></h2> <dl class="class"> <dt id="torch.distributions.multinomial.Multinomial">
<code>class torch.distributions.multinomial.Multinomial(total_count=1, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multinomial.html#Multinomial"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Multinomial distribution parameterized by <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.total_count" title="torch.distributions.multinomial.Multinomial.total_count"><code>total_count</code></a> and either <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.logits" title="torch.distributions.multinomial.Multinomial.logits"><code>logits</code></a> (but not both). The innermost dimension of <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.probs" title="torch.distributions.multinomial.Multinomial.probs"><code>probs</code></a> indexes over categories. All other dimensions index over batches.</p> <p>Note that <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.total_count" title="torch.distributions.multinomial.Multinomial.total_count"><code>total_count</code></a> need not be specified if only <a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code>log_prob()</code></a> is called (see example below)</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>probs</code> argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:<code>probs</code> will return this normalized value. The <code>logits</code> argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:<code>logits</code> will return this normalized value.</p> </div> <ul class="simple"> <li>
<a class="reference internal" href="#torch.distributions.multinomial.Multinomial.sample" title="torch.distributions.multinomial.Multinomial.sample"><code>sample()</code></a> requires a single shared <code>total_count</code> for all parameters and samples.</li> <li>
<a class="reference internal" href="#torch.distributions.multinomial.Multinomial.log_prob" title="torch.distributions.multinomial.Multinomial.log_prob"><code>log_prob()</code></a> allows different <code>total_count</code> for each parameter and sample.</li> </ul> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))
&gt;&gt;&gt; x = m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 21.,  24.,  30.,  25.])

&gt;&gt;&gt; Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)
tensor([-4.1338])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – number of trials</li> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event probabilities</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event log probabilities (unnormalized)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.multinomial.Multinomial.arg_constraints">
<code>arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multinomial.html#Multinomial.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multinomial.html#Multinomial.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.logits">
<code>property logits</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.probs">
<code>property probs</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multinomial.html#Multinomial.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.support">
<code>property support</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multinomial.Multinomial.total_count">
<code>total_count: int = None</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multinomial.Multinomial.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="multivariatenormal"><span class="hidden-section">MultivariateNormal</span></h2> <dl class="class"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal">
<code>class torch.distributions.multivariate_normal.MultivariateNormal(loc, covariance_matrix=None, precision_matrix=None, scale_tril=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a multivariate normal (also called Gaussian) distribution parameterized by a mean vector and a covariance matrix.</p> <p>The multivariate normal distribution can be parameterized either in terms of a positive definite covariance matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Σ</mi></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma}</annotation></semantics></math></span></span> </span> or a positive definite precision matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi mathvariant="bold">Σ</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma}^{-1}</annotation></semantics></math></span></span> </span> or a lower-triangular matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">L</mi></mrow><annotation encoding="application/x-tex">\mathbf{L}</annotation></semantics></math></span></span> </span> with positive-valued diagonal entries, such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">Σ</mi><mo>=</mo><mi mathvariant="bold">L</mi><msup><mi mathvariant="bold">L</mi><mi mathvariant="normal">⊤</mi></msup></mrow><annotation encoding="application/x-tex">\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top</annotation></semantics></math></span></span> </span>. This triangular matrix can be obtained via e.g. Cholesky decomposition of the covariance.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; m = MultivariateNormal(torch.zeros(2), torch.eye(2))
&gt;&gt;&gt; m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
tensor([-0.2102, -0.5429])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of the distribution</li> <li>
<strong>covariance_matrix</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – positive-definite covariance matrix</li> <li>
<strong>precision_matrix</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – positive-definite precision matrix</li> <li>
<strong>scale_tril</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – lower-triangular factor of covariance, with positive-valued diagonal</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Only one of <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code>covariance_matrix</code></a> or <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code>precision_matrix</code></a> or <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a> can be specified.</p> <p>Using <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a> will be more efficient: all computations internally are based on <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.scale_tril" title="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril"><code>scale_tril</code></a>. If <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix"><code>covariance_matrix</code></a> or <a class="reference internal" href="#torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix" title="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix"><code>precision_matrix</code></a> is passed instead, it is only used to compute the corresponding lower triangular matrices using a Cholesky decomposition.</p> </div> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.arg_constraints">
<code>arg_constraints = {'covariance_matrix': PositiveDefinite(), 'loc': IndependentConstraint(Real(), 1), 'precision_matrix': PositiveDefinite(), 'scale_tril': LowerCholesky()}</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.covariance_matrix">
<code>covariance_matrix</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.covariance_matrix"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.mean">
<code>property mean</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.precision_matrix">
<code>precision_matrix</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.precision_matrix"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.scale_tril">
<code>scale_tril</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/multivariate_normal.html#MultivariateNormal.scale_tril"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.support">
<code>support = IndependentConstraint(Real(), 1)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.multivariate_normal.MultivariateNormal.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="negativebinomial"><span class="hidden-section">NegativeBinomial</span></h2> <dl class="class"> <dt id="torch.distributions.negative_binomial.NegativeBinomial">
<code>class torch.distributions.negative_binomial.NegativeBinomial(total_count, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Negative Binomial distribution, i.e. distribution of the number of successful independent and identical Bernoulli trials before <code>total_count</code> failures are achieved. The probability of failure of each Bernoulli trial is <a class="reference internal" href="#torch.distributions.negative_binomial.NegativeBinomial.probs" title="torch.distributions.negative_binomial.NegativeBinomial.probs"><code>probs</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>total_count</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – non-negative number of negative Bernoulli trials to stop, although the distribution is still valid for real valued count</li> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Event probabilities of failure in the half open interval [0, 1)</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Event log-odds for probabilities of failure</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': HalfOpenInterval(lower_bound=0.0, upper_bound=1.0), 'total_count': GreaterThanEq(lower_bound=0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/negative_binomial.html#NegativeBinomial.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.support">
<code>support = IntegerGreaterThan(lower_bound=0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.negative_binomial.NegativeBinomial.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="normal"><span class="hidden-section">Normal</span></h2> <dl class="class"> <dt id="torch.distributions.normal.Normal">
<code>class torch.distributions.normal.Normal(loc, scale, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a normal (also called Gaussian) distribution parameterized by <code>loc</code> and <code>scale</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # normally distributed with loc=0 and scale=1
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of the distribution (often referred to as mu)</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – standard deviation of the distribution (often referred to as sigma)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.normal.Normal.arg_constraints">
<code>arg_constraints = {'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.normal.Normal.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/normal.html#Normal.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.normal.Normal.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.normal.Normal.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="onehotcategorical"><span class="hidden-section">OneHotCategorical</span></h2> <dl class="class"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical">
<code>class torch.distributions.one_hot_categorical.OneHotCategorical(probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a one-hot categorical distribution parameterized by <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code>logits</code></a>.</p> <p>Samples are one-hot coded vectors of size <code>probs.size(-1)</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>probs</code> argument must be non-negative, finite and have a non-zero sum, and it will be normalized to sum to 1 along the last dimension. attr:<code>probs</code> will return this normalized value. The <code>logits</code> argument will be interpreted as unnormalized log probabilities and can therefore be any real number. It will likewise be normalized so that the resulting probabilities sum to 1 along the last dimension. attr:<code>logits</code> will return this normalized value.</p> </div> <p>See also: <code>torch.distributions.Categorical()</code> for specifications of <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.probs" title="torch.distributions.one_hot_categorical.OneHotCategorical.probs"><code>probs</code></a> and <a class="reference internal" href="#torch.distributions.one_hot_categorical.OneHotCategorical.logits" title="torch.distributions.one_hot_categorical.OneHotCategorical.logits"><code>logits</code></a>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
&gt;&gt;&gt; m.sample()  # equal probability of 0, 1, 2, 3
tensor([ 0.,  0.,  0.,  1.])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event probabilities</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event log probabilities (unnormalized)</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.arg_constraints">
<code>arg_constraints = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.enumerate_support">
<code>enumerate_support(expand=True)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.enumerate_support"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.has_enumerate_support">
<code>has_enumerate_support = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.logits">
<code>property logits</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.probs">
<code>property probs</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/one_hot_categorical.html#OneHotCategorical.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.support">
<code>support = OneHot()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.one_hot_categorical.OneHotCategorical.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="pareto"><span class="hidden-section">Pareto</span></h2> <dl class="class"> <dt id="torch.distributions.pareto.Pareto">
<code>class torch.distributions.pareto.Pareto(scale, alpha, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/pareto.html#Pareto"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Samples from a Pareto Type 1 distribution.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1
tensor([ 1.5623])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Scale parameter of the distribution</li> <li>
<strong>alpha</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Shape parameter of the distribution</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.pareto.Pareto.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'alpha': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.pareto.Pareto.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/pareto.html#Pareto.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.pareto.Pareto.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/pareto.html#Pareto.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.pareto.Pareto.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.pareto.Pareto.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.pareto.Pareto.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="poisson"><span class="hidden-section">Poisson</span></h2> <dl class="class"> <dt id="torch.distributions.poisson.Poisson">
<code>class torch.distributions.poisson.Poisson(rate, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/poisson.html#Poisson"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.exp_family.ExponentialFamily" title="torch.distributions.exp_family.ExponentialFamily"><code>torch.distributions.exp_family.ExponentialFamily</code></a></p> <p>Creates a Poisson distribution parameterized by <code>rate</code>, the rate parameter.</p> <p>Samples are nonnegative integers, with a pmf given by</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mrow><mi>k</mi></msup><mfrac><msup><mi>e</mi><mrow><mo>−</mo><mrow><mi mathvariant="normal">r</mi><mi mathvariant="normal">a</mi><mi mathvariant="normal">t</mi><mi mathvariant="normal">e</mi></mrow></mrow></msup><mrow><mi>k</mi><mo stretchy="false">!</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!} </annotation></semantics></math></span></span></span> </div>
<p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Poisson(torch.tensor([4]))
&gt;&gt;&gt; m.sample()
tensor([ 3.])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>rate</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the rate parameter</p> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.poisson.Poisson.arg_constraints">
<code>arg_constraints = {'rate': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.poisson.Poisson.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/poisson.html#Poisson.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.poisson.Poisson.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/poisson.html#Poisson.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.poisson.Poisson.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.poisson.Poisson.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/poisson.html#Poisson.sample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.poisson.Poisson.support">
<code>support = IntegerGreaterThan(lower_bound=0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.poisson.Poisson.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="relaxedbernoulli"><span class="hidden-section">RelaxedBernoulli</span></h2> <dl class="class"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli">
<code>class torch.distributions.relaxed_bernoulli.RelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Creates a RelaxedBernoulli distribution, parametrized by <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature"><code>temperature</code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits"><code>logits</code></a> (but not both). This is a relaxed version of the <code>Bernoulli</code> distribution, so the values are in (0, 1), and has reparametrizable samples.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = RelaxedBernoulli(torch.tensor([2.2]),
                         torch.tensor([0.1, 0.2, 0.3, 0.99]))
&gt;&gt;&gt; m.sample()
tensor([ 0.2951,  0.3442,  0.8918,  0.9021])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>temperature</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – relaxation temperature</li> <li>
<strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the probability of sampling <code>1</code>
</li> <li>
<strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the log-odds of sampling <code>1</code>
</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#RelaxedBernoulli.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.logits">
<code>property logits</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.probs">
<code>property probs</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.support">
<code>support = Interval(lower_bound=0.0, upper_bound=1.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.RelaxedBernoulli.temperature">
<code>property temperature</code> </dt> 
</dl> </dd>
</dl>   <h2 id="logitrelaxedbernoulli"><span class="hidden-section">LogitRelaxedBernoulli</span></h2> <dl class="class"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli">
<code>class torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli(temperature, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a LogitRelaxedBernoulli distribution parameterized by <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits" title="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits"><code>logits</code></a> (but not both), which is the logit of a RelaxedBernoulli distribution.</p> <p>Samples are logits of values in (0, 1). See [1] for more details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>temperature</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – relaxation temperature</li> <li>
<strong>probs</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the probability of sampling <code>1</code>
</li> <li>
<strong>logits</strong> (<em>Number</em><em>, </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the log-odds of sampling <code>1</code>
</li> </ul> </dd> </dl> <p>[1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables (Maddison et al, 2017)</p> <p>[2] Categorical Reparametrization with Gumbel-Softmax (Jang et al, 2017)</p> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.arg_constraints">
<code>arg_constraints = {'logits': Real(), 'probs': Interval(lower_bound=0.0, upper_bound=1.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.logits">
<code>logits</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.logits"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.param_shape">
<code>property param_shape</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.probs">
<code>probs</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.probs"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_bernoulli.html#LogitRelaxedBernoulli.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_bernoulli.LogitRelaxedBernoulli.support">
<code>support = Real()</code> </dt> 
</dl> </dd>
</dl>   <h2 id="relaxedonehotcategorical"><span class="hidden-section">RelaxedOneHotCategorical</span></h2> <dl class="class"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical">
<code>class torch.distributions.relaxed_categorical.RelaxedOneHotCategorical(temperature, probs=None, logits=None, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_categorical.html#RelaxedOneHotCategorical"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Creates a RelaxedOneHotCategorical distribution parametrized by <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature"><code>temperature</code></a>, and either <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs"><code>probs</code></a> or <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits"><code>logits</code></a>. This is a relaxed version of the <code>OneHotCategorical</code> distribution, so its samples are on simplex, and are reparametrizable.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = RelaxedOneHotCategorical(torch.tensor([2.2]),
                                 torch.tensor([0.1, 0.2, 0.3, 0.4]))
&gt;&gt;&gt; m.sample()
tensor([ 0.1294,  0.2324,  0.3859,  0.2523])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>temperature</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – relaxation temperature</li> <li>
<strong>probs</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – event probabilities</li> <li>
<strong>logits</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – unnormalized log probability for each event</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'logits': IndependentConstraint(Real(), 1), 'probs': Simplex()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/relaxed_categorical.html#RelaxedOneHotCategorical.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.logits">
<code>property logits</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.probs">
<code>property probs</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.support">
<code>support = Simplex()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical.temperature">
<code>property temperature</code> </dt> 
</dl> </dd>
</dl>   <h2 id="studentt"><span class="hidden-section">StudentT</span></h2> <dl class="class"> <dt id="torch.distributions.studentT.StudentT">
<code>class torch.distributions.studentT.StudentT(df, loc=0.0, scale=1.0, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/studentT.html#StudentT"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Creates a Student’s t-distribution parameterized by degree of freedom <code>df</code>, mean <code>loc</code> and scale <code>scale</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = StudentT(torch.tensor([2.0]))
&gt;&gt;&gt; m.sample()  # Student's t-distributed with degrees of freedom=2
tensor([ 0.1046])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>df</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – degrees of freedom</li> <li>
<strong>loc</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – mean of the distribution</li> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – scale of the distribution</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.studentT.StudentT.arg_constraints">
<code>arg_constraints = {'df': GreaterThan(lower_bound=0.0), 'loc': Real(), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/studentT.html#StudentT.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/studentT.html#StudentT.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.studentT.StudentT.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/studentT.html#StudentT.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/studentT.html#StudentT.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.studentT.StudentT.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.studentT.StudentT.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="transformeddistribution"><span class="hidden-section">TransformedDistribution</span></h2> <dl class="class"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution">
<code>class torch.distributions.transformed_distribution.TransformedDistribution(base_distribution, transforms, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Extension of the Distribution class, which applies a sequence of Transforms to a base distribution. Let f be the composition of transforms applied:</p> <pre data-language="python">X ~ BaseDistribution
Y = f(X) ~ TransformedDistribution(BaseDistribution, f)
log p(Y) = log p(X) + log |det (dX/dY)|
</pre> <p>Note that the <code>.event_shape</code> of a <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>TransformedDistribution</code></a> is the maximum shape of its base distribution and its transforms, since transforms can introduce correlations among events.</p> <p>An example for the usage of <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>TransformedDistribution</code></a> would be:</p> <pre data-language="python"># Building a Logistic Distribution
# X ~ Uniform(0, 1)
# f = a + b * logit(X)
# Y ~ f(X) ~ Logistic(a, b)
base_distribution = Uniform(0, 1)
transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]
logistic = TransformedDistribution(base_distribution, transforms)
</pre> <p>For more examples, please look at the implementations of <a class="reference internal" href="#torch.distributions.gumbel.Gumbel" title="torch.distributions.gumbel.Gumbel"><code>Gumbel</code></a>, <a class="reference internal" href="#torch.distributions.half_cauchy.HalfCauchy" title="torch.distributions.half_cauchy.HalfCauchy"><code>HalfCauchy</code></a>, <a class="reference internal" href="#torch.distributions.half_normal.HalfNormal" title="torch.distributions.half_normal.HalfNormal"><code>HalfNormal</code></a>, <a class="reference internal" href="#torch.distributions.log_normal.LogNormal" title="torch.distributions.log_normal.LogNormal"><code>LogNormal</code></a>, <a class="reference internal" href="#torch.distributions.pareto.Pareto" title="torch.distributions.pareto.Pareto"><code>Pareto</code></a>, <a class="reference internal" href="#torch.distributions.weibull.Weibull" title="torch.distributions.weibull.Weibull"><code>Weibull</code></a>, <a class="reference internal" href="#torch.distributions.relaxed_bernoulli.RelaxedBernoulli" title="torch.distributions.relaxed_bernoulli.RelaxedBernoulli"><code>RelaxedBernoulli</code></a> and <a class="reference internal" href="#torch.distributions.relaxed_categorical.RelaxedOneHotCategorical" title="torch.distributions.relaxed_categorical.RelaxedOneHotCategorical"><code>RelaxedOneHotCategorical</code></a></p> <dl class="attribute"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.cdf"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the cumulative distribution function by inverting the transform(s) and computing the score of the base distribution.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.has_rsample">
<code>property has_rsample</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.icdf"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the inverse cumulative distribution function using transform(s) and computing the score of the base distribution.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Scores the sample by inverting the transform(s) and computing the score using the score of the base distribution and the log abs det jacobian.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.rsample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generates a sample_shape shaped reparameterized sample or sample_shape shaped batch of reparameterized samples if the distribution parameters are batched. Samples first from base distribution and applies <code>transform()</code> for every transform in the list.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transformed_distribution.html#TransformedDistribution.sample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generates a sample_shape shaped sample or sample_shape shaped batch of samples if the distribution parameters are batched. Samples first from base distribution and applies <code>transform()</code> for every transform in the list.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transformed_distribution.TransformedDistribution.support">
<code>property support</code> </dt> 
</dl> </dd>
</dl>   <h2 id="uniform"><span class="hidden-section">Uniform</span></h2> <dl class="class"> <dt id="torch.distributions.uniform.Uniform">
<code>class torch.distributions.uniform.Uniform(low, high, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>Generates uniformly distributed random samples from the half-open interval <code>[low, high)</code>.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))
&gt;&gt;&gt; m.sample()  # uniformly distributed in the range [0.0, 5.0)
tensor([ 2.3418])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>low</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – lower range (inclusive).</li> <li>
<strong>high</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – upper range (exclusive).</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.uniform.Uniform.arg_constraints">
<code>arg_constraints = {'high': Dependent(), 'low': Dependent()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.cdf">
<code>cdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.cdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.uniform.Uniform.has_rsample">
<code>has_rsample = True</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.icdf">
<code>icdf(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.icdf"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.mean">
<code>property mean</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.rsample">
<code>rsample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/uniform.html#Uniform.rsample"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.stddev">
<code>property stddev</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.support">
<code>property support</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.uniform.Uniform.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="vonmises"><span class="hidden-section">VonMises</span></h2> <dl class="class"> <dt id="torch.distributions.von_mises.VonMises">
<code>class torch.distributions.von_mises.VonMises(loc, concentration, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/von_mises.html#VonMises"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution"><code>torch.distributions.distribution.Distribution</code></a></p> <p>A circular von Mises distribution.</p> <p>This implementation uses polar coordinates. The <code>loc</code> and <code>value</code> args can be any real number (to facilitate unconstrained optimization), but are interpreted as angles modulo 2 pi.</p> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample() # von Mises distributed with loc=1 and concentration=1
tensor([1.9777])
</pre> </dd> </dl> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – an angle in radians.</li> <li>
<strong>concentration</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – concentration parameter</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.von_mises.VonMises.arg_constraints">
<code>arg_constraints = {'concentration': GreaterThan(lower_bound=0.0), 'loc': Real()}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.von_mises.VonMises.expand">
<code>expand(batch_shape)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/von_mises.html#VonMises.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.von_mises.VonMises.has_rsample">
<code>has_rsample = False</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.von_mises.VonMises.log_prob">
<code>log_prob(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/von_mises.html#VonMises.log_prob"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.von_mises.VonMises.mean">
<code>property mean</code> </dt> <dd>
<p>The provided mean is the circular one.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.von_mises.VonMises.sample">
<code>sample(sample_shape=torch.Size([]))</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/von_mises.html#VonMises.sample"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The sampling algorithm for the von Mises distribution is based on the following paper: Best, D. J., and Nicholas I. Fisher. “Efficient simulation of the von Mises distribution.” Applied Statistics (1979): 152-157.</p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.von_mises.VonMises.support">
<code>support = Real()</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.von_mises.VonMises.variance">
<code>variance</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/von_mises.html#VonMises.variance"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The provided variance is the circular one.</p> </dd>
</dl> </dd>
</dl>   <h2 id="weibull"><span class="hidden-section">Weibull</span></h2> <dl class="class"> <dt id="torch.distributions.weibull.Weibull">
<code>class torch.distributions.weibull.Weibull(scale, concentration, validate_args=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/weibull.html#Weibull"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Bases: <a class="reference internal" href="#torch.distributions.transformed_distribution.TransformedDistribution" title="torch.distributions.transformed_distribution.TransformedDistribution"><code>torch.distributions.transformed_distribution.TransformedDistribution</code></a></p> <p>Samples from a two-parameter Weibull distribution.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))
&gt;&gt;&gt; m.sample()  # sample from a Weibull distribution with scale=1, concentration=1
tensor([ 0.4784])
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>scale</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Scale parameter of distribution (lambda).</li> <li>
<strong>concentration</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Concentration parameter of distribution (k/shape).</li> </ul> </dd> </dl> <dl class="attribute"> <dt id="torch.distributions.weibull.Weibull.arg_constraints">
<code>arg_constraints: Dict[str, torch.distributions.constraints.Constraint] = {'concentration': GreaterThan(lower_bound=0.0), 'scale': GreaterThan(lower_bound=0.0)}</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.weibull.Weibull.entropy">
<code>entropy()</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/weibull.html#Weibull.entropy"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.weibull.Weibull.expand">
<code>expand(batch_shape, _instance=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/weibull.html#Weibull.expand"><span class="viewcode-link">[source]</span></a>
</dt> 
</dl> <dl class="method"> <dt id="torch.distributions.weibull.Weibull.mean">
<code>property mean</code> </dt> 
</dl> <dl class="attribute"> <dt id="torch.distributions.weibull.Weibull.support">
<code>support = GreaterThan(lower_bound=0.0)</code> </dt> 
</dl> <dl class="method"> <dt id="torch.distributions.weibull.Weibull.variance">
<code>property variance</code> </dt> 
</dl> </dd>
</dl>   <h2 id="kl-divergence"><code>KL Divergence</code></h2> <dl class="function" id="module-torch.distributions.kl"> <dt id="torch.distributions.kl.kl_divergence">
<code>torch.distributions.kl.kl_divergence(p, q)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/kl.html#kl_divergence"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute Kullback-Leibler divergence <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∥</mi><mi>q</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">KL(p \| q)</annotation></semantics></math></span></span> </span> between two distributions.</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mi>L</mi><mo stretchy="false">(</mo><mi>p</mi><mi mathvariant="normal">∥</mi><mi>q</mi><mo stretchy="false">)</mo><mo>=</mo><mo>∫</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>log</mi><mo>⁡</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>q</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx</annotation></semantics></math></span></span></span> </div>
<dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>p</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution">Distribution</a>) – A <code>Distribution</code> object.</li> <li>
<strong>q</strong> (<a class="reference internal" href="#torch.distributions.distribution.Distribution" title="torch.distributions.distribution.Distribution">Distribution</a>) – A <code>Distribution</code> object.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A batch of KL divergences of shape <code>batch_shape</code>.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> <dt class="field-even">Raises</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.9)"><strong>NotImplementedError</strong></a> – If the distribution types have not been registered via <a class="reference internal" href="#torch.distributions.kl.register_kl" title="torch.distributions.kl.register_kl"><code>register_kl()</code></a>.</p> </dd> </dl> </dd>
</dl> <dl class="function"> <dt id="torch.distributions.kl.register_kl">
<code>torch.distributions.kl.register_kl(type_p, type_q)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/kl.html#register_kl"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Decorator to register a pairwise function with <a class="reference internal" href="#torch.distributions.kl.kl_divergence" title="torch.distributions.kl.kl_divergence"><code>kl_divergence()</code></a>. Usage:</p> <pre data-language="python">@register_kl(Normal, Normal)
def kl_normal_normal(p, q):
    # insert implementation here
</pre> <p>Lookup returns the most specific (type,type) match ordered by subclass. If the match is ambiguous, a <code>RuntimeWarning</code> is raised. For example to resolve the ambiguous situation:</p> <pre data-language="python">@register_kl(BaseP, DerivedQ)
def kl_version1(p, q): ...
@register_kl(DerivedP, BaseQ)
def kl_version2(p, q): ...
</pre> <p>you should register a third most-specific implementation, e.g.:</p> <pre data-language="python">register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>type_p</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.9)">type</a>) – A subclass of <code>Distribution</code>.</li> <li>
<strong>type_q</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.9)">type</a>) – A subclass of <code>Distribution</code>.</li> </ul> </dd> </dl> </dd>
</dl>   <h2 id="transforms"><code>Transforms</code></h2> <dl class="class" id="module-torch.distributions.transforms"> <dt id="torch.distributions.transforms.Transform">
<code>class torch.distributions.transforms.Transform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#Transform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Abstract class for invertable transformations with computable log det jacobians. They are primarily used in <code>torch.distributions.TransformedDistribution</code>.</p> <p>Caching is useful for transforms whose inverses are either expensive or numerically unstable. Note that care must be taken with memoized values since the autograd graph may be reversed. For example while the following works with or without caching:</p> <pre data-language="python">y = t(x)
t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.
</pre> <p>However the following will error when caching due to dependency reversal:</p> <pre data-language="python">y = t(x)
z = t.inv(y)
grad(z.sum(), [y])  # error because z is x
</pre> <p>Derived classes should implement one or both of <code>_call()</code> or <code>_inverse()</code>. Derived classes that set <code>bijective=True</code> should also implement <a class="reference internal" href="#torch.distributions.transforms.Transform.log_abs_det_jacobian" title="torch.distributions.transforms.Transform.log_abs_det_jacobian"><code>log_abs_det_jacobian()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>cache_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.</p> </dd> <dt class="field-even">Variables</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>~Transform.domain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – The constraint representing valid inputs to this transform.</li> <li>
<strong>~Transform.codomain</strong> (<a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – The constraint representing valid outputs to this transform which are inputs to the inverse transform.</li> <li>
<strong>~Transform.bijective</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – Whether this transform is bijective. A transform <code>t</code> is bijective iff <code>t.inv(t(x)) == x</code> and <code>t(t.inv(y)) == y</code> for every <code>x</code> in the domain and <code>y</code> in the codomain. Transforms that are not bijective should at least maintain the weaker pseudoinverse properties <code>t(t.inv(t(x)) == t(x)</code> and <code>t.inv(t(t.inv(y))) == t.inv(y)</code>.</li> <li>
<strong>~Transform.sign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – For bijective univariate transforms, this should be +1 or -1 depending on whether transform is monotone increasing or decreasing.</li> </ul> </dd> </dl> <dl class="method"> <dt id="torch.distributions.transforms.Transform.inv">
<code>property inv</code> </dt> <dd>
<p>Returns the inverse <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> of this transform. This should satisfy <code>t.inv.inv is t</code>.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transforms.Transform.sign">
<code>property sign</code> </dt> <dd>
<p>Returns the sign of the determinant of the Jacobian, if applicable. In general this only makes sense for bijective transforms.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transforms.Transform.log_abs_det_jacobian">
<code>log_abs_det_jacobian(x, y)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#Transform.log_abs_det_jacobian"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the log det jacobian <code>log |dy/dx|</code> given input and output.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transforms.Transform.forward_shape">
<code>forward_shape(shape)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#Transform.forward_shape"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Infers the shape of the forward computation, given the input shape. Defaults to preserving shape.</p> </dd>
</dl> <dl class="method"> <dt id="torch.distributions.transforms.Transform.inverse_shape">
<code>inverse_shape(shape)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#Transform.inverse_shape"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Infers the shapes of the inverse computation, given the output shape. Defaults to preserving shape.</p> </dd>
</dl> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.ComposeTransform">
<code>class torch.distributions.transforms.ComposeTransform(parts, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#ComposeTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Composes multiple transforms in a chain. The transforms being composed are responsible for caching.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>parts</strong> (list of <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a>) – A list of transforms to compose.</li> <li>
<strong>cache_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Size of cache. If zero, no caching is done. If one, the latest single value is cached. Only 0 and 1 are supported.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.IndependentTransform">
<code>class torch.distributions.transforms.IndependentTransform(base_transform, reinterpreted_batch_ndims, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#IndependentTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wrapper around another transform to treat <code>reinterpreted_batch_ndims</code>-many extra of the right most dimensions as dependent. This has no effect on the forward or backward transforms, but does sum out <code>reinterpreted_batch_ndims</code>-many of the rightmost dimensions in <code>log_abs_det_jacobian()</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>base_transform</strong> (<a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a>) – A base transform.</li> <li>
<strong>reinterpreted_batch_ndims</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – The number of extra rightmost dimensions to treat as dependent.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.ReshapeTransform">
<code>class torch.distributions.transforms.ReshapeTransform(in_shape, out_shape, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#ReshapeTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Unit Jacobian transform to reshape the rightmost part of a tensor.</p> <p>Note that <code>in_shape</code> and <code>out_shape</code> must have the same number of elements, just as for <a class="reference internal" href="tensors#torch.Tensor.reshape" title="torch.Tensor.reshape"><code>torch.Tensor.reshape()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>in_shape</strong> (<em>torch.Size</em>) – The input event shape.</li> <li>
<strong>out_shape</strong> (<em>torch.Size</em>) – The output event shape.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.ExpTransform">
<code>class torch.distributions.transforms.ExpTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#ExpTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \exp(x)</annotation></semantics></math></span></span> </span>.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.PowerTransform">
<code>class torch.distributions.transforms.PowerTransform(exponent, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#PowerTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msup><mi>x</mi><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">y = x^{\text{exponent}}</annotation></semantics></math></span></span> </span>.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.SigmoidTransform">
<code>class torch.distributions.transforms.SigmoidTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#SigmoidTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">y = \frac{1}{1 + \exp(-x)}</annotation></semantics></math></span></span> </span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mtext>logit</mtext><mo stretchy="false">(</mo><mi>y</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">x = \text{logit}(y)</annotation></semantics></math></span></span> </span>.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.TanhTransform">
<code>class torch.distributions.transforms.TanhTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#TanhTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \tanh(x)</annotation></semantics></math></span></span> </span>.</p> <p>It is equivalent to <code>`
ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])
`</code> However this might not be numerically stable, thus it is recommended to use <code>TanhTransform</code> instead.</p> <p>Note that one should use <code>cache_size=1</code> when it comes to <code>NaN/Inf</code> values.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.AbsTransform">
<code>class torch.distributions.transforms.AbsTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#AbsTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">y = |x|</annotation></semantics></math></span></span> </span>.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.AffineTransform">
<code>class torch.distributions.transforms.AffineTransform(loc, scale, event_dim=0, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#AffineTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform via the pointwise affine mapping <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mtext>loc</mtext><mo>+</mo><mtext>scale</mtext><mo>×</mo><mi>x</mi></mrow><annotation encoding="application/x-tex">y = \text{loc} + \text{scale} \times x</annotation></semantics></math></span></span> </span>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>loc</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a>) – Location parameter.</li> <li>
<strong>scale</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.9)">float</a>) – Scale parameter.</li> <li>
<strong>event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Optional size of <code>event_shape</code>. This should be zero for univariate random variables, 1 for distributions over vectors, 2 for distributions over matrices, etc.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.CorrCholeskyTransform">
<code>class torch.distributions.transforms.CorrCholeskyTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#CorrCholeskyTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transforms an uncontrained real vector <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span></span> </span> with length <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>D</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mn>2</mn></mrow><annotation encoding="application/x-tex">D*(D-1)/2</annotation></semantics></math></span></span> </span> into the Cholesky factor of a D-dimension correlation matrix. This Cholesky factor is a lower triangular matrix with positive diagonals and unit Euclidean norm for each row. The transform is processed as follows:</p>  <ol class="arabic simple"> <li>First we convert x into a lower triangular matrix in row order.</li> <li>For each row <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_i</annotation></semantics></math></span></span> </span> of the lower triangular part, we apply a <em>signed</em> version of class <a class="reference internal" href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code>StickBreakingTransform</code></a> to transform <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>X</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">X_i</annotation></semantics></math></span></span> </span> into a unit Euclidean length vector using the following steps: - Scales into the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>−</mo><mn>1</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(-1, 1)</annotation></semantics></math></span></span> </span> domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mi>i</mi></msub><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><msub><mi>X</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">r_i = \tanh(X_i)</annotation></semantics></math></span></span> </span>. - Transforms into an unsigned domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mi>i</mi></msub><mo>=</mo><msubsup><mi>r</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">z_i = r_i^2</annotation></semantics></math></span></span> </span>. - Applies <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>S</mi><mi>t</mi><mi>i</mi><mi>c</mi><mi>k</mi><mi>B</mi><mi>r</mi><mi>e</mi><mi>a</mi><mi>k</mi><mi>i</mi><mi>n</mi><mi>g</mi><mi>T</mi><mi>r</mi><mi>a</mi><mi>n</mi><mi>s</mi><mi>f</mi><mi>o</mi><mi>r</mi><mi>m</mi><mo stretchy="false">(</mo><msub><mi>z</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_i = StickBreakingTransform(z_i)</annotation></semantics></math></span></span> </span>. - Transforms back into signed domain: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>n</mi><mo stretchy="false">(</mo><msub><mi>r</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>∗</mo><msqrt><msub><mi>s</mi><mi>i</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">y_i = sign(r_i) * \sqrt{s_i}</annotation></semantics></math></span></span> </span>.</li> </ol>  </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.SoftmaxTransform">
<code>class torch.distributions.transforms.SoftmaxTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#SoftmaxTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform from unconstrained space to the simplex via <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y = \exp(x)</annotation></semantics></math></span></span> </span> then normalizing.</p> <p>This is not bijective and cannot be used for HMC. However this acts mostly coordinate-wise (except for the final normalization), and thus is appropriate for coordinate-wise optimization algorithms.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.StickBreakingTransform">
<code>class torch.distributions.transforms.StickBreakingTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#StickBreakingTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform from unconstrained space to the simplex of one additional dimension via a stick-breaking process.</p> <p>This transform arises as an iterated sigmoid transform in a stick-breaking construction of the <code>Dirichlet</code> distribution: the first logit is transformed via sigmoid to the first probability and the probability of everything else, and then the process recurses.</p> <p>This is bijective and appropriate for use in HMC; however it mixes coordinates together and is less appropriate for optimization.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.LowerCholeskyTransform">
<code>class torch.distributions.transforms.LowerCholeskyTransform(cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#LowerCholeskyTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform from unconstrained matrices to lower-triangular matrices with nonnegative diagonal entries.</p> <p>This is useful for parameterizing positive definite matrices in terms of their Cholesky factorization.</p> </dd>
</dl> <dl class="class"> <dt id="torch.distributions.transforms.StackTransform">
<code>class torch.distributions.transforms.StackTransform(tseq, dim=0, cache_size=0)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/transforms.html#StackTransform"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Transform functor that applies a sequence of transforms <code>tseq</code> component-wise to each submatrix at <code>dim</code> in a way compatible with <a class="reference internal" href="generated/torch.stack#torch.stack" title="torch.stack"><code>torch.stack()</code></a>.</p> <dl class="simple"> <dt>Example::</dt>
<dd>
<p>x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1) t = StackTransform([ExpTransform(), identity_transform], dim=1) y = t(x)</p> </dd> </dl> </dd>
</dl>   <h2 id="constraints"><code>Constraints</code></h2> <p id="module-torch.distributions.constraints">The following constraints are implemented:</p> <ul class="simple"> <li><code>constraints.boolean</code></li> <li><code>constraints.cat</code></li> <li><code>constraints.corr_cholesky</code></li> <li><code>constraints.dependent</code></li> <li><code>constraints.greater_than(lower_bound)</code></li> <li><code>constraints.greater_than_eq(lower_bound)</code></li> <li><code>constraints.independent(constraint, reinterpreted_batch_ndims)</code></li> <li><code>constraints.integer_interval(lower_bound, upper_bound)</code></li> <li><code>constraints.interval(lower_bound, upper_bound)</code></li> <li><code>constraints.less_than(upper_bound)</code></li> <li><code>constraints.lower_cholesky</code></li> <li><code>constraints.lower_triangular</code></li> <li><code>constraints.multinomial</code></li> <li><code>constraints.nonnegative_integer</code></li> <li><code>constraints.one_hot</code></li> <li><code>constraints.positive_definite</code></li> <li><code>constraints.positive_integer</code></li> <li><code>constraints.positive</code></li> <li><code>constraints.real_vector</code></li> <li><code>constraints.real</code></li> <li><code>constraints.simplex</code></li> <li><code>constraints.stack</code></li> <li><code>constraints.unit_interval</code></li> </ul> <dl class="class"> <dt id="torch.distributions.constraints.Constraint">
<code>class torch.distributions.constraints.Constraint</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/constraints.html#Constraint"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Abstract base class for constraints.</p> <p>A constraint object represents a region over which a variable is valid, e.g. within which a variable can be optimized.</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Constraint.is_discrete</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.9)">bool</a>) – Whether constrained space is discrete. Defaults to False.</li> <li>
<strong>~Constraint.event_dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.9)">int</a>) – Number of rightmost dimensions that together define an event. The <a class="reference internal" href="#torch.distributions.constraints.Constraint.check" title="torch.distributions.constraints.Constraint.check"><code>check()</code></a> method will remove this many dimensions when computing validity.</li> </ul> </dd> </dl> <dl class="method"> <dt id="torch.distributions.constraints.Constraint.check">
<code>check(value)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/constraints.html#Constraint.check"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a byte tensor of <code>sample_shape + batch_shape</code> indicating whether each event in value satisfies this constraint.</p> </dd>
</dl> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.dependent_property">
<code>torch.distributions.constraints.dependent_property</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._DependentProperty</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.independent">
<code>torch.distributions.constraints.independent</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._IndependentConstraint</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.integer_interval">
<code>torch.distributions.constraints.integer_interval</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._IntegerInterval</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.greater_than">
<code>torch.distributions.constraints.greater_than</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._GreaterThan</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.greater_than_eq">
<code>torch.distributions.constraints.greater_than_eq</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._GreaterThanEq</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.less_than">
<code>torch.distributions.constraints.less_than</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._LessThan</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.multinomial">
<code>torch.distributions.constraints.multinomial</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._Multinomial</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.interval">
<code>torch.distributions.constraints.interval</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._Interval</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.half_open_interval">
<code>torch.distributions.constraints.half_open_interval</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._HalfOpenInterval</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.cat">
<code>torch.distributions.constraints.cat</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._Cat</code></p> </dd>
</dl> <dl class="attribute"> <dt id="torch.distributions.constraints.stack">
<code>torch.distributions.constraints.stack</code> </dt> <dd>
<p>alias of <code>torch.distributions.constraints._Stack</code></p> </dd>
</dl>   <h2 id="constraint-registry"><code>Constraint Registry</code></h2> <p id="module-torch.distributions.constraint_registry">PyTorch provides two global <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code>ConstraintRegistry</code></a> objects that link <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> objects to <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> objects. These objects both input constraints and return transforms, but they have different guarantees on bijectivity.</p> <ol class="arabic simple"> <li>
<code>biject_to(constraint)</code> looks up a bijective <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> from <code>constraints.real</code> to the given <code>constraint</code>. The returned transform is guaranteed to have <code>.bijective = True</code> and should implement <code>.log_abs_det_jacobian()</code>.</li> <li>
<code>transform_to(constraint)</code> looks up a not-necessarily bijective <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> from <code>constraints.real</code> to the given <code>constraint</code>. The returned transform is not guaranteed to implement <code>.log_abs_det_jacobian()</code>.</li> </ol> <p>The <code>transform_to()</code> registry is useful for performing unconstrained optimization on constrained parameters of probability distributions, which are indicated by each distribution’s <code>.arg_constraints</code> dict. These transforms often overparameterize a space in order to avoid rotation; they are thus more suitable for coordinate-wise optimization algorithms like Adam:</p> <pre data-language="python">loc = torch.zeros(100, requires_grad=True)
unconstrained = torch.zeros(100, requires_grad=True)
scale = transform_to(Normal.arg_constraints['scale'])(unconstrained)
loss = -Normal(loc, scale).log_prob(data).sum()
</pre> <p>The <code>biject_to()</code> registry is useful for Hamiltonian Monte Carlo, where samples from a probability distribution with constrained <code>.support</code> are propagated in an unconstrained space, and algorithms are typically rotation invariant.:</p> <pre data-language="python">dist = Exponential(rate)
unconstrained = torch.zeros(100, requires_grad=True)
sample = biject_to(dist.support)(unconstrained)
potential_energy = -dist.log_prob(sample).sum()
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>An example where <code>transform_to</code> and <code>biject_to</code> differ is <code>constraints.simplex</code>: <code>transform_to(constraints.simplex)</code> returns a <a class="reference internal" href="#torch.distributions.transforms.SoftmaxTransform" title="torch.distributions.transforms.SoftmaxTransform"><code>SoftmaxTransform</code></a> that simply exponentiates and normalizes its inputs; this is a cheap and mostly coordinate-wise operation appropriate for algorithms like SVI. In contrast, <code>biject_to(constraints.simplex)</code> returns a <a class="reference internal" href="#torch.distributions.transforms.StickBreakingTransform" title="torch.distributions.transforms.StickBreakingTransform"><code>StickBreakingTransform</code></a> that bijects its input down to a one-fewer-dimensional space; this a more expensive less numerically stable transform but is needed for algorithms like HMC.</p> </div> <p>The <code>biject_to</code> and <code>transform_to</code> objects can be extended by user-defined constraints and transforms using their <code>.register()</code> method either as a function on singleton constraints:</p> <pre data-language="python">transform_to.register(my_constraint, my_transform)
</pre> <p>or as a decorator on parameterized constraints:</p> <pre data-language="python">@transform_to.register(MyConstraintClass)
def my_factory(constraint):
    assert isinstance(constraint, MyConstraintClass)
    return MyTransform(constraint.param1, constraint.param2)
</pre> <p>You can create your own registry by creating a new <a class="reference internal" href="#torch.distributions.constraint_registry.ConstraintRegistry" title="torch.distributions.constraint_registry.ConstraintRegistry"><code>ConstraintRegistry</code></a> object.</p> <dl class="class"> <dt id="torch.distributions.constraint_registry.ConstraintRegistry">
<code>class torch.distributions.constraint_registry.ConstraintRegistry</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/constraint_registry.html#ConstraintRegistry"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registry to link constraints to transforms.</p> <dl class="method"> <dt id="torch.distributions.constraint_registry.ConstraintRegistry.register">
<code>register(constraint, factory=None)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/distributions/constraint_registry.html#ConstraintRegistry.register"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a> subclass in this registry. Usage:</p> <pre data-language="python">@my_registry.register(MyConstraintClass)
def construct_transform(constraint):
    assert isinstance(constraint, MyConstraint)
    return MyTransform(constraint.arg_constraints)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>constraint</strong> (subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>) – A subclass of <a class="reference internal" href="#torch.distributions.constraints.Constraint" title="torch.distributions.constraints.Constraint"><code>Constraint</code></a>, or a singleton object of the desired class.</li> <li>
<strong>factory</strong> (<em>callable</em>) – A callable that inputs a constraint object and returns a <a class="reference internal" href="#torch.distributions.transforms.Transform" title="torch.distributions.transforms.Transform"><code>Transform</code></a> object.</li> </ul> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/distributions.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/distributions.html</a>
  </p>
</div>
