<h1 id="torch-nn-quantized-dynamic">torch.nn.quantized.dynamic</h1>  <h2 id="module-torch.nn.quantized.dynamic">Linear</h2> <dl class="class"> <dt id="torch.nn.quantized.dynamic.Linear">
<code>class torch.nn.quantized.dynamic.Linear(in_features, out_features, bias_=True, dtype=torch.qint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/linear.html#Linear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A dynamic quantized linear module with floating point tensor as inputs and outputs. We adopt the same interface as <code>torch.nn.Linear</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.Linear">https://pytorch.org/docs/stable/nn.html#torch.nn.Linear</a> for documentation.</p> <p>Similar to <a class="reference internal" href="generated/torch.nn.linear#torch.nn.Linear" title="torch.nn.Linear"><code>torch.nn.Linear</code></a>, attributes will be randomly initialized at module creation time and will be overwritten later</p> <dl class="field-list simple"> <dt class="field-odd">Variables</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>~Linear.weight</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable quantized weights of the module which are of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_features</mtext><mo separator="true">,</mo><mtext>in_features</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_features}, \text{in\_features})</annotation></semantics></math></span></span> </span>.</li> <li>
<strong>~Linear.bias</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the non-learnable floating point bias of the module of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mtext>out_features</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\text{out\_features})</annotation></semantics></math></span></span> </span>. If <code>bias</code> is <code>True</code>, the values are initialized to zero.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; m = nn.quantized.dynamic.Linear(20, 30)
&gt;&gt;&gt; input = torch.randn(128, 20)
&gt;&gt;&gt; output = m(input)
&gt;&gt;&gt; print(output.size())
torch.Size([128, 30])
</pre> <dl class="method"> <dt id="torch.nn.quantized.dynamic.Linear.from_float">
<code>classmethod from_float(mod)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/linear.html#Linear.from_float"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a dynamic quantized module from a float module or qparams_dict</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>mod</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">Module</a>) – a float module, either produced by torch.quantization utilities or provided by the user</p> </dd> </dl> </dd>
</dl> </dd>
</dl>   <h2 id="lstm">LSTM</h2> <dl class="class"> <dt id="torch.nn.quantized.dynamic.LSTM">
<code>class torch.nn.quantized.dynamic.LSTM(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#LSTM"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A dynamic quantized LSTM module with floating point tensor as inputs and outputs. We adopt the same interface as <code>torch.nn.LSTM</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM">https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM</a> for documentation.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))
</pre> </dd>
</dl>   <h2 id="lstmcell">LSTMCell</h2> <dl class="class"> <dt id="torch.nn.quantized.dynamic.LSTMCell">
<code>class torch.nn.quantized.dynamic.LSTMCell(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#LSTMCell"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A long short-term memory (LSTM) cell.</p> <p>A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as <code>torch.nn.LSTMCell</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell">https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell</a> for documentation.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; cx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx, cx = rnn(input[i], (hx, cx))
        output.append(hx)
</pre> </dd>
</dl>   <h2 id="grucell">GRUCell</h2> <dl class="class"> <dt id="torch.nn.quantized.dynamic.GRUCell">
<code>class torch.nn.quantized.dynamic.GRUCell(input_size, hidden_size, bias=True, dtype=torch.qint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#GRUCell"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A gated recurrent unit (GRU) cell</p> <p>A dynamic quantized GRUCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as <code>torch.nn.GRUCell</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell">https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell</a> for documentation.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; rnn = nn.GRUCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)
</pre> </dd>
</dl>   <h2 id="rnncell">RNNCell</h2> <dl class="class"> <dt id="torch.nn.quantized.dynamic.RNNCell">
<code>class torch.nn.quantized.dynamic.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh', dtype=torch.qint8)</code> <a class="reference internal" href="https://pytorch.org/docs/1.8.0/_modules/torch/nn/quantized/dynamic/modules/rnn.html#RNNCell"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>An Elman RNN cell with tanh or ReLU non-linearity. A dynamic quantized RNNCell module with floating point tensor as inputs and outputs. Weights are quantized to 8 bits. We adopt the same interface as <code>torch.nn.RNNCell</code>, please see <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell">https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell</a> for documentation.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; rnn = nn.RNNCell(10, 20)
&gt;&gt;&gt; input = torch.randn(6, 3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx = rnn(input[i], hx)
        output.append(hx)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2019 Torch Contributors<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://pytorch.org/docs/1.8.0/torch.nn.quantized.dynamic.html" class="_attribution-link">https://pytorch.org/docs/1.8.0/torch.nn.quantized.dynamic.html</a>
  </p>
</div>
