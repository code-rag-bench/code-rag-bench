<h1 class="devsite-page-title">Module: tf</h1>   <p><devsite-mathjax config="TeX-AMS-MML_SVG"></devsite-mathjax> </p>   <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/__init__.py">  View source on GitHub </a> </td> </table> <h2 id="tensorflow" data-text="TensorFlow">TensorFlow</h2> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">pip install tensorflow
</pre> <h2 id="modules" data-text="Modules">Modules</h2> <p><a href="audio"><code translate="no" dir="ltr">audio</code></a> module: Public API for tf.audio namespace.</p> <p><a href="autodiff"><code translate="no" dir="ltr">autodiff</code></a> module: Public API for tf.autodiff namespace.</p> <p><a href="autograph"><code translate="no" dir="ltr">autograph</code></a> module: Conversion of plain Python into TensorFlow graph code.</p> <p><a href="bitwise"><code translate="no" dir="ltr">bitwise</code></a> module: Operations for manipulating the binary representations of integers.</p> <p><a href="compat"><code translate="no" dir="ltr">compat</code></a> module: Compatibility functions.</p> <p><a href="config"><code translate="no" dir="ltr">config</code></a> module: Public API for tf.config namespace.</p> <p><a href="data"><code translate="no" dir="ltr">data</code></a> module: <a href="data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> API for input pipelines.</p> <p><a href="debugging"><code translate="no" dir="ltr">debugging</code></a> module: Public API for tf.debugging namespace.</p> <p><a href="distribute"><code translate="no" dir="ltr">distribute</code></a> module: Library for running a computation across multiple devices.</p> <p><a href="dtypes"><code translate="no" dir="ltr">dtypes</code></a> module: Public API for tf.dtypes namespace.</p> <p><a href="errors"><code translate="no" dir="ltr">errors</code></a> module: Exception types for TensorFlow errors.</p> <p><a href="estimator"><code translate="no" dir="ltr">estimator</code></a> module: Estimator: High level tools for working with models.</p> <p><a href="experimental"><code translate="no" dir="ltr">experimental</code></a> module: Public API for tf.experimental namespace.</p> <p><a href="feature_column"><code translate="no" dir="ltr">feature_column</code></a> module: Public API for tf.feature_column namespace.</p> <p><a href="graph_util"><code translate="no" dir="ltr">graph_util</code></a> module: Helpers to manipulate a tensor graph in python.</p> <p><a href="image"><code translate="no" dir="ltr">image</code></a> module: Image ops.</p> <p><a href="keras/initializers"><code translate="no" dir="ltr">initializers</code></a> module: Keras initializer serialization / deserialization.</p> <p><a href="io"><code translate="no" dir="ltr">io</code></a> module: Public API for tf.io namespace.</p> <p><a href="keras"><code translate="no" dir="ltr">keras</code></a> module: Implementation of the Keras API meant to be a high-level API for TensorFlow.</p> <p><a href="linalg"><code translate="no" dir="ltr">linalg</code></a> module: Operations for linear algebra.</p> <p><a href="lite"><code translate="no" dir="ltr">lite</code></a> module: Public API for tf.lite namespace.</p> <p><a href="lookup"><code translate="no" dir="ltr">lookup</code></a> module: Public API for tf.lookup namespace.</p> <p><a href="keras/losses"><code translate="no" dir="ltr">losses</code></a> module: Built-in loss functions.</p> <p><a href="math"><code translate="no" dir="ltr">math</code></a> module: Math Operations.</p> <p><a href="keras/metrics"><code translate="no" dir="ltr">metrics</code></a> module: Built-in metrics.</p> <p><a href="mixed_precision"><code translate="no" dir="ltr">mixed_precision</code></a> module: Public API for tf.mixed_precision namespace.</p> <p><a href="mlir"><code translate="no" dir="ltr">mlir</code></a> module: Public API for tf.mlir namespace.</p> <p><a href="nest"><code translate="no" dir="ltr">nest</code></a> module: Public API for tf.nest namespace.</p> <p><a href="nn"><code translate="no" dir="ltr">nn</code></a> module: Wrappers for primitive Neural Net (NN) Operations.</p> <p><a href="keras/optimizers"><code translate="no" dir="ltr">optimizers</code></a> module: Built-in optimizer classes.</p> <p><a href="profiler"><code translate="no" dir="ltr">profiler</code></a> module: Public API for tf.profiler namespace.</p> <p><a href="quantization"><code translate="no" dir="ltr">quantization</code></a> module: Public API for tf.quantization namespace.</p> <p><a href="queue"><code translate="no" dir="ltr">queue</code></a> module: Public API for tf.queue namespace.</p> <p><a href="ragged"><code translate="no" dir="ltr">ragged</code></a> module: Ragged Tensors.</p> <p><a href="random"><code translate="no" dir="ltr">random</code></a> module: Public API for tf.random namespace.</p> <p><a href="raw_ops"><code translate="no" dir="ltr">raw_ops</code></a> module: Public API for tf.raw_ops namespace.</p> <p><a href="saved_model"><code translate="no" dir="ltr">saved_model</code></a> module: Public API for tf.saved_model namespace.</p> <p><a href="sets"><code translate="no" dir="ltr">sets</code></a> module: Tensorflow set operations.</p> <p><a href="signal"><code translate="no" dir="ltr">signal</code></a> module: Signal processing operations.</p> <p><a href="sparse"><code translate="no" dir="ltr">sparse</code></a> module: Sparse Tensor Representation.</p> <p><a href="strings"><code translate="no" dir="ltr">strings</code></a> module: Operations for working with string Tensors.</p> <p><a href="summary"><code translate="no" dir="ltr">summary</code></a> module: Operations for writing summary data, for use in analysis and visualization.</p> <p><a href="sysconfig"><code translate="no" dir="ltr">sysconfig</code></a> module: System configuration library.</p> <p><a href="test"><code translate="no" dir="ltr">test</code></a> module: Testing.</p> <p><a href="tpu"><code translate="no" dir="ltr">tpu</code></a> module: Ops related to Tensor Processing Units.</p> <p><a href="train"><code translate="no" dir="ltr">train</code></a> module: Support for training models.</p> <p><a href="types"><code translate="no" dir="ltr">types</code></a> module: Public TensorFlow type definitions.</p> <p><a href="version"><code translate="no" dir="ltr">version</code></a> module: Public API for tf.version namespace.</p> <p><a href="xla"><code translate="no" dir="ltr">xla</code></a> module: Public API for tf.xla namespace.</p> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="aggregationmethod"><code translate="no" dir="ltr">class AggregationMethod</code></a>: A class listing aggregation methods used to combine gradients.</p> <p><a href="criticalsection"><code translate="no" dir="ltr">class CriticalSection</code></a>: Critical section.</p> <p><a href="dtypes/dtype"><code translate="no" dir="ltr">class DType</code></a>: Represents the type of the elements in a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="devicespec"><code translate="no" dir="ltr">class DeviceSpec</code></a>: Represents a (possibly partial) specification for a TensorFlow device.</p> <p><a href="gradienttape"><code translate="no" dir="ltr">class GradientTape</code></a>: Record operations for automatic differentiation.</p> <p><a href="graph"><code translate="no" dir="ltr">class Graph</code></a>: A TensorFlow computation, represented as a dataflow graph.</p> <p><a href="indexedslices"><code translate="no" dir="ltr">class IndexedSlices</code></a>: A sparse representation of a set of tensor slices at given indices.</p> <p><a href="indexedslicesspec"><code translate="no" dir="ltr">class IndexedSlicesSpec</code></a>: Type specification for a <a href="indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p><a href="module"><code translate="no" dir="ltr">class Module</code></a>: Base neural network module class.</p> <p><a href="operation"><code translate="no" dir="ltr">class Operation</code></a>: Represents a graph node that performs computation on tensors.</p> <p><a href="optionalspec"><code translate="no" dir="ltr">class OptionalSpec</code></a>: Type specification for <a href="experimental/optional"><code translate="no" dir="ltr">tf.experimental.Optional</code></a>.</p> <p><a href="raggedtensor"><code translate="no" dir="ltr">class RaggedTensor</code></a>: Represents a ragged tensor.</p> <p><a href="raggedtensorspec"><code translate="no" dir="ltr">class RaggedTensorSpec</code></a>: Type specification for a <a href="raggedtensor"><code translate="no" dir="ltr">tf.RaggedTensor</code></a>.</p> <p><a href="registergradient"><code translate="no" dir="ltr">class RegisterGradient</code></a>: A decorator for registering the gradient function for an op type.</p> <p><a href="sparse/sparsetensor"><code translate="no" dir="ltr">class SparseTensor</code></a>: Represents a sparse tensor.</p> <p><a href="sparsetensorspec"><code translate="no" dir="ltr">class SparseTensorSpec</code></a>: Type specification for a <a href="sparse/sparsetensor"><code translate="no" dir="ltr">tf.sparse.SparseTensor</code></a>.</p> <p><a href="tensor"><code translate="no" dir="ltr">class Tensor</code></a>: A tensor is a multidimensional array of elements represented by a</p> <p><a href="tensorarray"><code translate="no" dir="ltr">class TensorArray</code></a>: Class wrapping dynamic-sized, per-time-step, write-once Tensor arrays.</p> <p><a href="tensorarrayspec"><code translate="no" dir="ltr">class TensorArraySpec</code></a>: Type specification for a <a href="tensorarray"><code translate="no" dir="ltr">tf.TensorArray</code></a>.</p> <p><a href="tensorshape"><code translate="no" dir="ltr">class TensorShape</code></a>: Represents the shape of a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="tensorspec"><code translate="no" dir="ltr">class TensorSpec</code></a>: Describes a tf.Tensor.</p> <p><a href="typespec"><code translate="no" dir="ltr">class TypeSpec</code></a>: Specifies a TensorFlow value type.</p> <p><a href="unconnectedgradients"><code translate="no" dir="ltr">class UnconnectedGradients</code></a>: Controls how gradient computation behaves when y does not depend on x.</p> <p><a href="variable"><code translate="no" dir="ltr">class Variable</code></a>: See the <a href="https://tensorflow.org/guide/variable">variable guide</a>.</p> <p><a href="variableaggregation"><code translate="no" dir="ltr">class VariableAggregation</code></a>: Indicates how a distributed variable will be aggregated.</p> <p><a href="variablesynchronization"><code translate="no" dir="ltr">class VariableSynchronization</code></a>: Indicates when a distributed variable will be synced.</p> <p><a href="constant_initializer"><code translate="no" dir="ltr">class constant_initializer</code></a>: Initializer that generates tensors with constant values.</p> <p><a href="name_scope"><code translate="no" dir="ltr">class name_scope</code></a>: A context manager for use when defining a Python op.</p> <p><a href="ones_initializer"><code translate="no" dir="ltr">class ones_initializer</code></a>: Initializer that generates tensors initialized to 1.</p> <p><a href="random_normal_initializer"><code translate="no" dir="ltr">class random_normal_initializer</code></a>: Initializer that generates tensors with a normal distribution.</p> <p><a href="random_uniform_initializer"><code translate="no" dir="ltr">class random_uniform_initializer</code></a>: Initializer that generates tensors with a uniform distribution.</p> <p><a href="zeros_initializer"><code translate="no" dir="ltr">class zeros_initializer</code></a>: Initializer that generates tensors initialized to 0.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="debugging/assert"><code translate="no" dir="ltr">Assert(...)</code></a>: Asserts that the given condition is true.</p> <p><a href="math/abs"><code translate="no" dir="ltr">abs(...)</code></a>: Computes the absolute value of a tensor.</p> <p><a href="math/acos"><code translate="no" dir="ltr">acos(...)</code></a>: Computes acos of x element-wise.</p> <p><a href="math/acosh"><code translate="no" dir="ltr">acosh(...)</code></a>: Computes inverse hyperbolic cosine of x element-wise.</p> <p><a href="math/add"><code translate="no" dir="ltr">add(...)</code></a>: Returns x + y element-wise.</p> <p><a href="math/add_n"><code translate="no" dir="ltr">add_n(...)</code></a>: Adds all input tensors element-wise.</p> <p><a href="math/argmax"><code translate="no" dir="ltr">argmax(...)</code></a>: Returns the index with the largest value across axes of a tensor.</p> <p><a href="math/argmin"><code translate="no" dir="ltr">argmin(...)</code></a>: Returns the index with the smallest value across axes of a tensor.</p> <p><a href="argsort"><code translate="no" dir="ltr">argsort(...)</code></a>: Returns the indices of a tensor that give its sorted order along an axis.</p> <p><a href="dtypes/as_dtype"><code translate="no" dir="ltr">as_dtype(...)</code></a>: Converts the given <code translate="no" dir="ltr">type_value</code> to a <code translate="no" dir="ltr">DType</code>.</p> <p><a href="strings/as_string"><code translate="no" dir="ltr">as_string(...)</code></a>: Converts each entry in the given tensor to strings.</p> <p><a href="math/asin"><code translate="no" dir="ltr">asin(...)</code></a>: Computes the trignometric inverse sine of x element-wise.</p> <p><a href="math/asinh"><code translate="no" dir="ltr">asinh(...)</code></a>: Computes inverse hyperbolic sine of x element-wise.</p> <p><a href="debugging/assert_equal"><code translate="no" dir="ltr">assert_equal(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x == y</code> holds element-wise.</p> <p><a href="debugging/assert_greater"><code translate="no" dir="ltr">assert_greater(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &gt; y</code> holds element-wise.</p> <p><a href="debugging/assert_less"><code translate="no" dir="ltr">assert_less(...)</code></a>: Assert the condition <code translate="no" dir="ltr">x &lt; y</code> holds element-wise.</p> <p><a href="debugging/assert_rank"><code translate="no" dir="ltr">assert_rank(...)</code></a>: Assert that <code translate="no" dir="ltr">x</code> has rank equal to <code translate="no" dir="ltr">rank</code>.</p> <p><a href="math/atan"><code translate="no" dir="ltr">atan(...)</code></a>: Computes the trignometric inverse tangent of x element-wise.</p> <p><a href="math/atan2"><code translate="no" dir="ltr">atan2(...)</code></a>: Computes arctangent of <code translate="no" dir="ltr">y/x</code> element-wise, respecting signs of the arguments.</p> <p><a href="math/atanh"><code translate="no" dir="ltr">atanh(...)</code></a>: Computes inverse hyperbolic tangent of x element-wise.</p> <p><a href="batch_to_space"><code translate="no" dir="ltr">batch_to_space(...)</code></a>: BatchToSpace for N-D tensors of type T.</p> <p><a href="bitcast"><code translate="no" dir="ltr">bitcast(...)</code></a>: Bitcasts a tensor from one type to another without copying data.</p> <p><a href="boolean_mask"><code translate="no" dir="ltr">boolean_mask(...)</code></a>: Apply boolean mask to tensor.</p> <p><a href="broadcast_dynamic_shape"><code translate="no" dir="ltr">broadcast_dynamic_shape(...)</code></a>: Computes the shape of a broadcast given symbolic shapes.</p> <p><a href="broadcast_static_shape"><code translate="no" dir="ltr">broadcast_static_shape(...)</code></a>: Computes the shape of a broadcast given known shapes.</p> <p><a href="broadcast_to"><code translate="no" dir="ltr">broadcast_to(...)</code></a>: Broadcast an array for a compatible shape.</p> <p><a href="case"><code translate="no" dir="ltr">case(...)</code></a>: Create a case operation.</p> <p><a href="cast"><code translate="no" dir="ltr">cast(...)</code></a>: Casts a tensor to a new type.</p> <p><a href="clip_by_global_norm"><code translate="no" dir="ltr">clip_by_global_norm(...)</code></a>: Clips values of multiple tensors by the ratio of the sum of their norms.</p> <p><a href="clip_by_norm"><code translate="no" dir="ltr">clip_by_norm(...)</code></a>: Clips tensor values to a maximum L2-norm.</p> <p><a href="clip_by_value"><code translate="no" dir="ltr">clip_by_value(...)</code></a>: Clips tensor values to a specified min and max.</p> <p><a href="dtypes/complex"><code translate="no" dir="ltr">complex(...)</code></a>: Converts two real numbers to a complex number.</p> <p><a href="concat"><code translate="no" dir="ltr">concat(...)</code></a>: Concatenates tensors along one dimension.</p> <p><a href="cond"><code translate="no" dir="ltr">cond(...)</code></a>: Return <code translate="no" dir="ltr">true_fn()</code> if the predicate <code translate="no" dir="ltr">pred</code> is true else <code translate="no" dir="ltr">false_fn()</code>.</p> <p><a href="constant"><code translate="no" dir="ltr">constant(...)</code></a>: Creates a constant tensor from a tensor-like object.</p> <p><a href="control_dependencies"><code translate="no" dir="ltr">control_dependencies(...)</code></a>: Wrapper for <a href="graph#control_dependencies"><code translate="no" dir="ltr">Graph.control_dependencies()</code></a> using the default graph.</p> <p><a href="convert_to_tensor"><code translate="no" dir="ltr">convert_to_tensor(...)</code></a>: Converts the given <code translate="no" dir="ltr">value</code> to a <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="math/cos"><code translate="no" dir="ltr">cos(...)</code></a>: Computes cos of x element-wise.</p> <p><a href="math/cosh"><code translate="no" dir="ltr">cosh(...)</code></a>: Computes hyperbolic cosine of x element-wise.</p> <p><a href="math/cumsum"><code translate="no" dir="ltr">cumsum(...)</code></a>: Compute the cumulative sum of the tensor <code translate="no" dir="ltr">x</code> along <code translate="no" dir="ltr">axis</code>.</p> <p><a href="custom_gradient"><code translate="no" dir="ltr">custom_gradient(...)</code></a>: Decorator to define a function with a custom gradient.</p> <p><a href="device"><code translate="no" dir="ltr">device(...)</code></a>: Specifies the device for ops created/executed in this context.</p> <p><a href="math/divide"><code translate="no" dir="ltr">divide(...)</code></a>: Computes Python style division of <code translate="no" dir="ltr">x</code> by <code translate="no" dir="ltr">y</code>.</p> <p><a href="dynamic_partition"><code translate="no" dir="ltr">dynamic_partition(...)</code></a>: Partitions <code translate="no" dir="ltr">data</code> into <code translate="no" dir="ltr">num_partitions</code> tensors using indices from <code translate="no" dir="ltr">partitions</code>.</p> <p><a href="dynamic_stitch"><code translate="no" dir="ltr">dynamic_stitch(...)</code></a>: Interleave the values from the <code translate="no" dir="ltr">data</code> tensors into a single tensor.</p> <p><a href="edit_distance"><code translate="no" dir="ltr">edit_distance(...)</code></a>: Computes the Levenshtein distance between sequences.</p> <p><a href="linalg/eig"><code translate="no" dir="ltr">eig(...)</code></a>: Computes the eigen decomposition of a batch of matrices.</p> <p><a href="linalg/eigvals"><code translate="no" dir="ltr">eigvals(...)</code></a>: Computes the eigenvalues of one or more matrices.</p> <p><a href="einsum"><code translate="no" dir="ltr">einsum(...)</code></a>: Tensor contraction over specified indices and outer product.</p> <p><a href="ensure_shape"><code translate="no" dir="ltr">ensure_shape(...)</code></a>: Updates the shape of a tensor and checks at runtime that the shape holds.</p> <p><a href="math/equal"><code translate="no" dir="ltr">equal(...)</code></a>: Returns the truth value of (x == y) element-wise.</p> <p><a href="executing_eagerly"><code translate="no" dir="ltr">executing_eagerly(...)</code></a>: Checks whether the current thread has eager execution enabled.</p> <p><a href="math/exp"><code translate="no" dir="ltr">exp(...)</code></a>: Computes exponential of x element-wise. \(y = e^x\).</p> <p><a href="expand_dims"><code translate="no" dir="ltr">expand_dims(...)</code></a>: Returns a tensor with a length 1 axis inserted at index <code translate="no" dir="ltr">axis</code>.</p> <p><a href="extract_volume_patches"><code translate="no" dir="ltr">extract_volume_patches(...)</code></a>: Extract <code translate="no" dir="ltr">patches</code> from <code translate="no" dir="ltr">input</code> and put them in the <code translate="no" dir="ltr">"depth"</code> output dimension. 3D extension of <code translate="no" dir="ltr">extract_image_patches</code>.</p> <p><a href="eye"><code translate="no" dir="ltr">eye(...)</code></a>: Construct an identity matrix, or a batch of matrices.</p> <p><a href="fill"><code translate="no" dir="ltr">fill(...)</code></a>: Creates a tensor filled with a scalar value.</p> <p><a href="fingerprint"><code translate="no" dir="ltr">fingerprint(...)</code></a>: Generates fingerprint values.</p> <p><a href="math/floor"><code translate="no" dir="ltr">floor(...)</code></a>: Returns element-wise largest integer not greater than x.</p> <p><a href="foldl"><code translate="no" dir="ltr">foldl(...)</code></a>: foldl on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href="foldr"><code translate="no" dir="ltr">foldr(...)</code></a>: foldr on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href="function"><code translate="no" dir="ltr">function(...)</code></a>: Compiles a function into a callable TensorFlow graph.</p> <p><a href="gather"><code translate="no" dir="ltr">gather(...)</code></a>: Gather slices from params axis <code translate="no" dir="ltr">axis</code> according to indices.</p> <p><a href="gather_nd"><code translate="no" dir="ltr">gather_nd(...)</code></a>: Gather slices from <code translate="no" dir="ltr">params</code> into a Tensor with shape specified by <code translate="no" dir="ltr">indices</code>.</p> <p><a href="get_logger"><code translate="no" dir="ltr">get_logger(...)</code></a>: Return TF logger instance.</p> <p><a href="get_static_value"><code translate="no" dir="ltr">get_static_value(...)</code></a>: Returns the constant value of the given tensor, if efficiently calculable.</p> <p><a href="grad_pass_through"><code translate="no" dir="ltr">grad_pass_through(...)</code></a>: Creates a grad-pass-through op with the forward behavior provided in f.</p> <p><a href="gradients"><code translate="no" dir="ltr">gradients(...)</code></a>: Constructs symbolic derivatives of sum of <code translate="no" dir="ltr">ys</code> w.r.t. x in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="math/greater"><code translate="no" dir="ltr">greater(...)</code></a>: Returns the truth value of (x &gt; y) element-wise.</p> <p><a href="math/greater_equal"><code translate="no" dir="ltr">greater_equal(...)</code></a>: Returns the truth value of (x &gt;= y) element-wise.</p> <p><a href="group"><code translate="no" dir="ltr">group(...)</code></a>: Create an op that groups multiple operations.</p> <p><a href="guarantee_const"><code translate="no" dir="ltr">guarantee_const(...)</code></a>: Gives a guarantee to the TF runtime that the input tensor is a constant.</p> <p><a href="hessians"><code translate="no" dir="ltr">hessians(...)</code></a>: Constructs the Hessian of sum of <code translate="no" dir="ltr">ys</code> with respect to <code translate="no" dir="ltr">x</code> in <code translate="no" dir="ltr">xs</code>.</p> <p><a href="histogram_fixed_width"><code translate="no" dir="ltr">histogram_fixed_width(...)</code></a>: Return histogram of values.</p> <p><a href="histogram_fixed_width_bins"><code translate="no" dir="ltr">histogram_fixed_width_bins(...)</code></a>: Bins the given values for use in a histogram.</p> <p><a href="identity"><code translate="no" dir="ltr">identity(...)</code></a>: Return a Tensor with the same shape and contents as input.</p> <p><a href="identity_n"><code translate="no" dir="ltr">identity_n(...)</code></a>: Returns a list of tensors with the same shapes and contents as the input</p> <p><a href="graph_util/import_graph_def"><code translate="no" dir="ltr">import_graph_def(...)</code></a>: Imports the graph from <code translate="no" dir="ltr">graph_def</code> into the current default <code translate="no" dir="ltr">Graph</code>. (deprecated arguments)</p> <p><a href="init_scope"><code translate="no" dir="ltr">init_scope(...)</code></a>: A context manager that lifts ops out of control-flow scopes and function-building graphs.</p> <p><a href="inside_function"><code translate="no" dir="ltr">inside_function(...)</code></a>: Indicates whether the caller code is executing inside a <a href="function"><code translate="no" dir="ltr">tf.function</code></a>.</p> <p><a href="is_tensor"><code translate="no" dir="ltr">is_tensor(...)</code></a>: Checks whether <code translate="no" dir="ltr">x</code> is a TF-native type that can be passed to many TF ops.</p> <p><a href="math/less"><code translate="no" dir="ltr">less(...)</code></a>: Returns the truth value of (x &lt; y) element-wise.</p> <p><a href="math/less_equal"><code translate="no" dir="ltr">less_equal(...)</code></a>: Returns the truth value of (x &lt;= y) element-wise.</p> <p><a href="linspace"><code translate="no" dir="ltr">linspace(...)</code></a>: Generates evenly-spaced values in an interval along a given axis.</p> <p><a href="load_library"><code translate="no" dir="ltr">load_library(...)</code></a>: Loads a TensorFlow plugin.</p> <p><a href="load_op_library"><code translate="no" dir="ltr">load_op_library(...)</code></a>: Loads a TensorFlow plugin, containing custom ops and kernels.</p> <p><a href="math/logical_and"><code translate="no" dir="ltr">logical_and(...)</code></a>: Logical AND function.</p> <p><a href="math/logical_not"><code translate="no" dir="ltr">logical_not(...)</code></a>: Returns the truth value of <code translate="no" dir="ltr">NOT x</code> element-wise.</p> <p><a href="math/logical_or"><code translate="no" dir="ltr">logical_or(...)</code></a>: Returns the truth value of x OR y element-wise.</p> <p><a href="make_ndarray"><code translate="no" dir="ltr">make_ndarray(...)</code></a>: Create a numpy ndarray from a tensor.</p> <p><a href="make_tensor_proto"><code translate="no" dir="ltr">make_tensor_proto(...)</code></a>: Create a TensorProto.</p> <p><a href="map_fn"><code translate="no" dir="ltr">map_fn(...)</code></a>: Transforms <code translate="no" dir="ltr">elems</code> by applying <code translate="no" dir="ltr">fn</code> to each element unstacked on axis 0. (deprecated arguments)</p> <p><a href="linalg/matmul"><code translate="no" dir="ltr">matmul(...)</code></a>: Multiplies matrix <code translate="no" dir="ltr">a</code> by matrix <code translate="no" dir="ltr">b</code>, producing <code translate="no" dir="ltr">a</code> * <code translate="no" dir="ltr">b</code>.</p> <p><a href="linalg/sqrtm"><code translate="no" dir="ltr">matrix_square_root(...)</code></a>: Computes the matrix square root of one or more square matrices:</p> <p><a href="math/maximum"><code translate="no" dir="ltr">maximum(...)</code></a>: Returns the max of x and y (i.e. x &gt; y ? x : y) element-wise.</p> <p><a href="meshgrid"><code translate="no" dir="ltr">meshgrid(...)</code></a>: Broadcasts parameters for evaluation on an N-D grid.</p> <p><a href="math/minimum"><code translate="no" dir="ltr">minimum(...)</code></a>: Returns the min of x and y (i.e. x &lt; y ? x : y) element-wise.</p> <p><a href="math/multiply"><code translate="no" dir="ltr">multiply(...)</code></a>: Returns an element-wise x * y.</p> <p><a href="math/negative"><code translate="no" dir="ltr">negative(...)</code></a>: Computes numerical negative value element-wise.</p> <p><a href="no_gradient"><code translate="no" dir="ltr">no_gradient(...)</code></a>: Specifies that ops of type <code translate="no" dir="ltr">op_type</code> is not differentiable.</p> <p><a href="no_op"><code translate="no" dir="ltr">no_op(...)</code></a>: Does nothing. Only useful as a placeholder for control edges.</p> <p><a href="nondifferentiable_batch_function"><code translate="no" dir="ltr">nondifferentiable_batch_function(...)</code></a>: Batches the computation done by the decorated function.</p> <p><a href="norm"><code translate="no" dir="ltr">norm(...)</code></a>: Computes the norm of vectors, matrices, and tensors.</p> <p><a href="math/not_equal"><code translate="no" dir="ltr">not_equal(...)</code></a>: Returns the truth value of (x != y) element-wise.</p> <p><a href="numpy_function"><code translate="no" dir="ltr">numpy_function(...)</code></a>: Wraps a python function and uses it as a TensorFlow op.</p> <p><a href="one_hot"><code translate="no" dir="ltr">one_hot(...)</code></a>: Returns a one-hot tensor.</p> <p><a href="ones"><code translate="no" dir="ltr">ones(...)</code></a>: Creates a tensor with all elements set to one (1).</p> <p><a href="ones_like"><code translate="no" dir="ltr">ones_like(...)</code></a>: Creates a tensor of all ones that has the same shape as the input.</p> <p><a href="pad"><code translate="no" dir="ltr">pad(...)</code></a>: Pads a tensor.</p> <p><a href="parallel_stack"><code translate="no" dir="ltr">parallel_stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor in parallel.</p> <p><a href="math/pow"><code translate="no" dir="ltr">pow(...)</code></a>: Computes the power of one value to another.</p> <p><a href="print"><code translate="no" dir="ltr">print(...)</code></a>: Print the specified inputs.</p> <p><a href="py_function"><code translate="no" dir="ltr">py_function(...)</code></a>: Wraps a python function into a TensorFlow op that executes it eagerly.</p> <p><a href="quantize_and_dequantize_v4"><code translate="no" dir="ltr">quantize_and_dequantize_v4(...)</code></a>: Returns the gradient of <code translate="no" dir="ltr">QuantizeAndDequantizeV4</code>.</p> <p><a href="range"><code translate="no" dir="ltr">range(...)</code></a>: Creates a sequence of numbers.</p> <p><a href="rank"><code translate="no" dir="ltr">rank(...)</code></a>: Returns the rank of a tensor.</p> <p><a href="realdiv"><code translate="no" dir="ltr">realdiv(...)</code></a>: Returns x / y element-wise for real types.</p> <p><a href="recompute_grad"><code translate="no" dir="ltr">recompute_grad(...)</code></a>: An eager-compatible version of recompute_grad.</p> <p><a href="math/reduce_all"><code translate="no" dir="ltr">reduce_all(...)</code></a>: Computes the "logical and" of elements across dimensions of a tensor.</p> <p><a href="math/reduce_any"><code translate="no" dir="ltr">reduce_any(...)</code></a>: Computes the "logical or" of elements across dimensions of a tensor.</p> <p><a href="math/reduce_logsumexp"><code translate="no" dir="ltr">reduce_logsumexp(...)</code></a>: Computes log(sum(exp(elements across dimensions of a tensor))).</p> <p><a href="math/reduce_max"><code translate="no" dir="ltr">reduce_max(...)</code></a>: Computes the maximum of elements across dimensions of a tensor.</p> <p><a href="math/reduce_mean"><code translate="no" dir="ltr">reduce_mean(...)</code></a>: Computes the mean of elements across dimensions of a tensor.</p> <p><a href="math/reduce_min"><code translate="no" dir="ltr">reduce_min(...)</code></a>: Computes the minimum of elements across dimensions of a tensor.</p> <p><a href="math/reduce_prod"><code translate="no" dir="ltr">reduce_prod(...)</code></a>: Computes the product of elements across dimensions of a tensor.</p> <p><a href="math/reduce_sum"><code translate="no" dir="ltr">reduce_sum(...)</code></a>: Computes the sum of elements across dimensions of a tensor.</p> <p><a href="register_tensor_conversion_function"><code translate="no" dir="ltr">register_tensor_conversion_function(...)</code></a>: Registers a function for converting objects of <code translate="no" dir="ltr">base_type</code> to <code translate="no" dir="ltr">Tensor</code>.</p> <p><a href="repeat"><code translate="no" dir="ltr">repeat(...)</code></a>: Repeat elements of <code translate="no" dir="ltr">input</code>.</p> <p><a href="required_space_to_batch_paddings"><code translate="no" dir="ltr">required_space_to_batch_paddings(...)</code></a>: Calculate padding required to make block_shape divide input_shape.</p> <p><a href="reshape"><code translate="no" dir="ltr">reshape(...)</code></a>: Reshapes a tensor.</p> <p><a href="reverse"><code translate="no" dir="ltr">reverse(...)</code></a>: Reverses specific dimensions of a tensor.</p> <p><a href="reverse_sequence"><code translate="no" dir="ltr">reverse_sequence(...)</code></a>: Reverses variable length slices.</p> <p><a href="roll"><code translate="no" dir="ltr">roll(...)</code></a>: Rolls the elements of a tensor along an axis.</p> <p><a href="math/round"><code translate="no" dir="ltr">round(...)</code></a>: Rounds the values of a tensor to the nearest integer, element-wise.</p> <p><a href="dtypes/saturate_cast"><code translate="no" dir="ltr">saturate_cast(...)</code></a>: Performs a safe saturating cast of <code translate="no" dir="ltr">value</code> to <code translate="no" dir="ltr">dtype</code>.</p> <p><a href="math/scalar_mul"><code translate="no" dir="ltr">scalar_mul(...)</code></a>: Multiplies a scalar times a <code translate="no" dir="ltr">Tensor</code> or <code translate="no" dir="ltr">IndexedSlices</code> object.</p> <p><a href="scan"><code translate="no" dir="ltr">scan(...)</code></a>: scan on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. (deprecated argument values)</p> <p><a href="scatter_nd"><code translate="no" dir="ltr">scatter_nd(...)</code></a>: Scatter <code translate="no" dir="ltr">updates</code> into a new tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="searchsorted"><code translate="no" dir="ltr">searchsorted(...)</code></a>: Searches input tensor for values on the innermost dimension.</p> <p><a href="sequence_mask"><code translate="no" dir="ltr">sequence_mask(...)</code></a>: Returns a mask tensor representing the first N positions of each cell.</p> <p><a href="shape"><code translate="no" dir="ltr">shape(...)</code></a>: Returns a tensor containing the shape of the input tensor.</p> <p><a href="shape_n"><code translate="no" dir="ltr">shape_n(...)</code></a>: Returns shape of tensors.</p> <p><a href="math/sigmoid"><code translate="no" dir="ltr">sigmoid(...)</code></a>: Computes sigmoid of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="math/sign"><code translate="no" dir="ltr">sign(...)</code></a>: Returns an element-wise indication of the sign of a number.</p> <p><a href="math/sin"><code translate="no" dir="ltr">sin(...)</code></a>: Computes sine of x element-wise.</p> <p><a href="math/sinh"><code translate="no" dir="ltr">sinh(...)</code></a>: Computes hyperbolic sine of x element-wise.</p> <p><a href="size"><code translate="no" dir="ltr">size(...)</code></a>: Returns the size of a tensor.</p> <p><a href="slice"><code translate="no" dir="ltr">slice(...)</code></a>: Extracts a slice from a tensor.</p> <p><a href="sort"><code translate="no" dir="ltr">sort(...)</code></a>: Sorts a tensor.</p> <p><a href="space_to_batch"><code translate="no" dir="ltr">space_to_batch(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="space_to_batch_nd"><code translate="no" dir="ltr">space_to_batch_nd(...)</code></a>: SpaceToBatch for N-D tensors of type T.</p> <p><a href="split"><code translate="no" dir="ltr">split(...)</code></a>: Splits a tensor <code translate="no" dir="ltr">value</code> into a list of sub tensors.</p> <p><a href="math/sqrt"><code translate="no" dir="ltr">sqrt(...)</code></a>: Computes element-wise square root of the input tensor.</p> <p><a href="math/square"><code translate="no" dir="ltr">square(...)</code></a>: Computes square of x element-wise.</p> <p><a href="squeeze"><code translate="no" dir="ltr">squeeze(...)</code></a>: Removes dimensions of size 1 from the shape of a tensor.</p> <p><a href="stack"><code translate="no" dir="ltr">stack(...)</code></a>: Stacks a list of rank-<code translate="no" dir="ltr">R</code> tensors into one rank-<code translate="no" dir="ltr">(R+1)</code> tensor.</p> <p><a href="stop_gradient"><code translate="no" dir="ltr">stop_gradient(...)</code></a>: Stops gradient computation.</p> <p><a href="strided_slice"><code translate="no" dir="ltr">strided_slice(...)</code></a>: Extracts a strided slice of a tensor (generalized Python array indexing).</p> <p><a href="math/subtract"><code translate="no" dir="ltr">subtract(...)</code></a>: Returns x - y element-wise.</p> <p><a href="switch_case"><code translate="no" dir="ltr">switch_case(...)</code></a>: Create a switch/case operation, i.e. an integer-indexed conditional.</p> <p><a href="math/tan"><code translate="no" dir="ltr">tan(...)</code></a>: Computes tan of x element-wise.</p> <p><a href="math/tanh"><code translate="no" dir="ltr">tanh(...)</code></a>: Computes hyperbolic tangent of <code translate="no" dir="ltr">x</code> element-wise.</p> <p><a href="tensor_scatter_nd_add"><code translate="no" dir="ltr">tensor_scatter_nd_add(...)</code></a>: Adds sparse <code translate="no" dir="ltr">updates</code> to an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_max"><code translate="no" dir="ltr">tensor_scatter_nd_max(...)</code></a></p> <p><a href="tensor_scatter_nd_min"><code translate="no" dir="ltr">tensor_scatter_nd_min(...)</code></a></p> <p><a href="tensor_scatter_nd_sub"><code translate="no" dir="ltr">tensor_scatter_nd_sub(...)</code></a>: Subtracts sparse <code translate="no" dir="ltr">updates</code> from an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensor_scatter_nd_update"><code translate="no" dir="ltr">tensor_scatter_nd_update(...)</code></a>: "Scatter <code translate="no" dir="ltr">updates</code> into an existing tensor according to <code translate="no" dir="ltr">indices</code>.</p> <p><a href="tensordot"><code translate="no" dir="ltr">tensordot(...)</code></a>: Tensor contraction of a and b along specified axes and outer product.</p> <p><a href="tile"><code translate="no" dir="ltr">tile(...)</code></a>: Constructs a tensor by tiling a given tensor.</p> <p><a href="timestamp"><code translate="no" dir="ltr">timestamp(...)</code></a>: Provides the time since epoch in seconds.</p> <p><a href="transpose"><code translate="no" dir="ltr">transpose(...)</code></a>: Transposes <code translate="no" dir="ltr">a</code>, where <code translate="no" dir="ltr">a</code> is a Tensor.</p> <p><a href="math/truediv"><code translate="no" dir="ltr">truediv(...)</code></a>: Divides x / y elementwise (using Python 3 division operator semantics).</p> <p><a href="truncatediv"><code translate="no" dir="ltr">truncatediv(...)</code></a>: Returns x / y element-wise for integer types.</p> <p><a href="truncatemod"><code translate="no" dir="ltr">truncatemod(...)</code></a>: Returns element-wise remainder of division. This emulates C semantics in that</p> <p><a href="tuple"><code translate="no" dir="ltr">tuple(...)</code></a>: Group tensors together.</p> <p><a href="type_spec_from_value"><code translate="no" dir="ltr">type_spec_from_value(...)</code></a>: Returns a <a href="typespec"><code translate="no" dir="ltr">tf.TypeSpec</code></a> that represents the given <code translate="no" dir="ltr">value</code>.</p> <p><a href="unique"><code translate="no" dir="ltr">unique(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="unique_with_counts"><code translate="no" dir="ltr">unique_with_counts(...)</code></a>: Finds unique elements in a 1-D tensor.</p> <p><a href="unravel_index"><code translate="no" dir="ltr">unravel_index(...)</code></a>: Converts an array of flat indices into a tuple of coordinate arrays.</p> <p><a href="unstack"><code translate="no" dir="ltr">unstack(...)</code></a>: Unpacks the given dimension of a rank-<code translate="no" dir="ltr">R</code> tensor into rank-<code translate="no" dir="ltr">(R-1)</code> tensors.</p> <p><a href="variable_creator_scope"><code translate="no" dir="ltr">variable_creator_scope(...)</code></a>: Scope which defines a variable creation function to be used by variable().</p> <p><a href="vectorized_map"><code translate="no" dir="ltr">vectorized_map(...)</code></a>: Parallel map on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0.</p> <p><a href="where"><code translate="no" dir="ltr">where(...)</code></a>: Return the elements where <code translate="no" dir="ltr">condition</code> is <code translate="no" dir="ltr">True</code> (multiplexing <code translate="no" dir="ltr">x</code> and <code translate="no" dir="ltr">y</code>).</p> <p><a href="while_loop"><code translate="no" dir="ltr">while_loop(...)</code></a>: Repeat <code translate="no" dir="ltr">body</code> while the condition <code translate="no" dir="ltr">cond</code> is true. (deprecated argument values)</p> <p><a href="zeros"><code translate="no" dir="ltr">zeros(...)</code></a>: Creates a tensor with all elements set to zero.</p> <p><a href="zeros_like"><code translate="no" dir="ltr">zeros_like(...)</code></a>: Creates a tensor with all elements set to zero.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Other Members</th></tr> 
<tr> <td> <strong>version</strong> </td> <td> <code translate="no" dir="ltr">'2.4.0'</code> </td> </tr>
<tr> <td> bfloat16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> bool </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> complex128 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> complex64 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> double </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> float16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> float32 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> float64 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> half </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> int16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> int32 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> int64 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> int8 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> newaxis </td> <td> <code translate="no" dir="ltr">None</code> </td> </tr>
<tr> <td> qint16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> qint32 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> qint8 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> quint16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> quint8 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> resource </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> string </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> uint16 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> uint32 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> uint64 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> uint8 </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr>
<tr> <td> variant </td> <td> <a href="dtypes/dtype"><code translate="no" dir="ltr">tf.dtypes.DType</code></a> </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf</a>
  </p>
</div>
