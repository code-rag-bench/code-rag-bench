<h1 class="devsite-page-title">tf.saved_model.experimental.VariablePolicy</h1>       <p>Enum defining options for variable handling when saving.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/saved_model/experimental/VariablePolicy"><code translate="no" dir="ltr">tf.compat.v1.saved_model.experimental.VariablePolicy</code></a></p> </section>  <p>NONE No policy applied: Distributed variables are saved as one variable, with no device attached.</p> <p>SAVE_VARIABLE_DEVICES When saving variables, also save their device assignment. This is useful if one wants to hardcode devices in saved models, but it also makes them non-portable if soft device placement is disabled (more details in <a href="../../config/set_soft_device_placement"><code translate="no" dir="ltr">tf.config.set_soft_device_placement</code></a>). This is currently not fully supported by <a href="../load"><code translate="no" dir="ltr">saved_model.load</code></a>, and is mainly intended to be used when one will be reading the saved model at a lower API level. In the example below, the graph saved by the call to <a href="../save"><code translate="no" dir="ltr">saved_model.save</code></a> will have the variable devices correctly specified:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">exported = tf.train.Checkpoint()
with tf.device('/GPU:0'):
  exported.x_gpu = tf.Variable(1.0)
with tf.device('/CPU:0'):
  exported.x_cpu = tf.Variable(1.0)
tf.saved_model.save(exported, export_dir,
    options = tf.saved_model.SaveOptions(
        experimental_variable_policy=
          tf.saved_model.experimental.VariablePolicy.SAVE_VARIABLE_DEVICES))
</pre> <p>Distributed variables are still saved as one variable under this policy.</p> <p>EXPAND_DISTRIBUTED_VARIABLES Distributed variables will be saved with information about their components, allowing for their restoration on load. Also, the saved graph will contain references to those variables. This is useful when one wants to use the model for training in environments where the original distribution strategy is not available.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Class Variables</th></tr> 
<tr> <td> EXPAND_DISTRIBUTED_VARIABLES </td> <td> <a href="variablepolicy"><code translate="no" dir="ltr">tf.saved_model.experimental.VariablePolicy</code></a> </td> </tr>
<tr> <td> NONE </td> <td> <a href="variablepolicy"><code translate="no" dir="ltr">tf.saved_model.experimental.VariablePolicy</code></a> </td> </tr>
<tr> <td> SAVE_VARIABLE_DEVICES </td> <td> <a href="variablepolicy"><code translate="no" dir="ltr">tf.saved_model.experimental.VariablePolicy</code></a> </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/saved_model/experimental/VariablePolicy" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/saved_model/experimental/VariablePolicy</a>
  </p>
</div>
