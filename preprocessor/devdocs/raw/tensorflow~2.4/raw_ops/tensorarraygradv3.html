<h1 class="devsite-page-title">tf.raw_ops.TensorArrayGradV3</h1>       <p>Creates a TensorArray for storing the gradients of values in the given handle.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/raw_ops/TensorArrayGradV3"><code translate="no" dir="ltr">tf.compat.v1.raw_ops.TensorArrayGradV3</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.raw_ops.TensorArrayGradV3(
    handle, flow_in, source, name=None
)
</pre>  <p>If the given TensorArray gradient already exists, returns a reference to it.</p> <p>Locks the size of the original TensorArray by disabling its dynamic size flag.</p> <p><strong>A note about the input flow_in:</strong></p> <p>The handle flow_in forces the execution of the gradient lookup to occur only after certain other operations have occurred. For example, when the forward TensorArray is dynamically sized, writes to this TensorArray may resize the object. The gradient TensorArray is statically sized based on the size of the forward TensorArray when this operation executes. Furthermore, the size of the forward TensorArray is frozen by this call. As a result, the flow is used to ensure that the call to generate the gradient TensorArray only happens after all writes are executed.</p> <p>In the case of dynamically sized TensorArrays, gradient computation should only be performed on read operations that have themselves been chained via flow to occur only after all writes have executed. That way the final size of the forward TensorArray is known when this operation is called.</p> <p><strong>A note about the source attribute:</strong></p> <p>TensorArray gradient calls use an accumulator TensorArray object. If multiple gradients are calculated and run in the same session, the multiple gradient nodes may accidentally flow through the same accumulator TensorArray. This double counts and generally breaks the TensorArray gradient flow.</p> <p>The solution is to identify which gradient call this particular TensorArray gradient is being called in. This is performed by identifying a unique string (e.g. "gradients", "gradients_1", ...) from the input gradient Tensor's name. This string is used as a suffix when creating the TensorArray gradient object here (the attribute <code translate="no" dir="ltr">source</code>).</p> <p>The attribute <code translate="no" dir="ltr">source</code> is added as a suffix to the forward TensorArray's name when performing the creation / lookup, so that each separate gradient calculation gets its own TensorArray accumulator.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">handle</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">resource</code>. The handle to the forward TensorArray. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">flow_in</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. A float scalar that enforces proper chaining of operations. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">source</code> </td> <td> A <code translate="no" dir="ltr">string</code>. The gradient source string, used to decide which gradient TensorArray to return. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tuple of <code translate="no" dir="ltr">Tensor</code> objects (grad_handle, flow_out). </td> </tr> <tr> <td> <code translate="no" dir="ltr">grad_handle</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">resource</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">flow_out</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float32</code>. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops/TensorArrayGradV3" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/raw_ops/TensorArrayGradV3</a>
  </p>
</div>
