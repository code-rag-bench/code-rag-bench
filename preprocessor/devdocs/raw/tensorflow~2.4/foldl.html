<h1 class="devsite-page-title">tf.foldl</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/functional_ops.py#L166-L240">  View source on GitHub </a> </td> </table> <p>foldl on the list of tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. (deprecated argument values)</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.foldl(
    fn, elems, initializer=None, parallel_iterations=10, back_prop=True,
    swap_memory=False, name=None
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> SOME ARGUMENT VALUES ARE DEPRECATED: <code translate="no" dir="ltr">(back_prop=False)</code>. They will be removed in a future version. Instructions for updating: back_prop=False is deprecated. Consider using tf.stop_gradient instead. Instead of: results = tf.foldl(fn, elems, back_prop=False) Use: results = tf.nest.map_structure(tf.stop_gradient, tf.foldl(fn, elems))</span></aside> <p>This foldl operator repeatedly applies the callable <code translate="no" dir="ltr">fn</code> to a sequence of elements from first to last. The elements are made of the tensors unpacked from <code translate="no" dir="ltr">elems</code> on dimension 0. The callable fn takes two tensors as arguments. The first argument is the accumulated value computed from the preceding invocation of fn, and the second is the value at the current position of <code translate="no" dir="ltr">elems</code>. If <code translate="no" dir="ltr">initializer</code> is None, <code translate="no" dir="ltr">elems</code> must contain at least one element, and its first element is used as the initializer.</p> <p>Suppose that <code translate="no" dir="ltr">elems</code> is unpacked into <code translate="no" dir="ltr">values</code>, a list of tensors. The shape of the result tensor is fn(initializer, values[0]).shape`.</p> <p>This method also allows multi-arity <code translate="no" dir="ltr">elems</code> and output of <code translate="no" dir="ltr">fn</code>. If <code translate="no" dir="ltr">elems</code> is a (possibly nested) list or tuple of tensors, then each of these tensors must have a matching first (unpack) dimension. The signature of <code translate="no" dir="ltr">fn</code> may match the structure of <code translate="no" dir="ltr">elems</code>. That is, if <code translate="no" dir="ltr">elems</code> is <code translate="no" dir="ltr">(t1, [t2, t3, [t4, t5]])</code>, then an appropriate signature for <code translate="no" dir="ltr">fn</code> is: <code translate="no" dir="ltr">fn = lambda (t1, [t2, t3, [t4, t5]]):</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> The callable to be performed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">elems</code> </td> <td> A tensor or (possibly nested) sequence of tensors, each of which will be unpacked along their first dimension. The nested sequence of the resulting slices will be the first argument to <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initializer</code> </td> <td> (optional) A tensor or (possibly nested) sequence of tensors, as the initial value for the accumulator. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_iterations</code> </td> <td> (optional) The number of iterations allowed to run in parallel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">back_prop</code> </td> <td> (optional) Deprecated. False disables support for back propagation. Prefer using <a href="stop_gradient"><code translate="no" dir="ltr">tf.stop_gradient</code></a> instead. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">swap_memory</code> </td> <td> (optional) True enables GPU-CPU memory swapping. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (optional) Name prefix for the returned tensors. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or (possibly nested) sequence of tensors, resulting from applying <code translate="no" dir="ltr">fn</code> consecutively to the list of tensors unpacked from <code translate="no" dir="ltr">elems</code>, from first to last. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">TypeError</code> </td> <td> if <code translate="no" dir="ltr">fn</code> is not callable. </td> </tr> </table> <h4 id="example" data-text="Example:">Example:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">elems = tf.constant([1, 2, 3, 4, 5, 6])
sum = foldl(lambda a, x: a + x, elems)
# sum == 21
</pre>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/foldl" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/foldl</a>
  </p>
</div>
