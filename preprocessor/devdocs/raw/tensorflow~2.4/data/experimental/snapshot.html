<h1 class="devsite-page-title">tf.data.experimental.snapshot</h1>       <p>API to persist the output of the input dataset.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/snapshot"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.snapshot</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.snapshot(
    path, compression='AUTO', reader_func=None, shard_func=None
)
</pre>  <p>The snapshot API allows users to transparently persist the output of their preprocessing pipeline to disk, and materialize the pre-processed data on a different training run.</p> <p>This API enables repeated preprocessing steps to be consolidated, and allows re-use of already processed data, trading off disk storage and network bandwidth for freeing up more valuable CPU resources and accelerator compute time.</p> <p><a href="https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md">https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md</a> has detailed design documentation of this feature.</p> <p>Users can specify various options to control the behavior of snapshot, including how snapshots are read from and written to by passing in user-defined functions to the <code translate="no" dir="ltr">reader_func</code> and <code translate="no" dir="ltr">shard_func</code> parameters.</p> <p><code translate="no" dir="ltr">shard_func</code> is a user specified function that maps input elements to snapshot shards.</p> <p>Users may want to specify this function to control how snapshot files should be written to disk. Below is an example of how a potential shard_func could be written.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">dataset = ...
dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.snapshot("/path/to/snapshot/dir",
    shard_func=lambda x, y: x % NUM_SHARDS, ...))
dataset = dataset.map(lambda x, y: y)
</pre> <p><code translate="no" dir="ltr">reader_func</code> is a user specified function that accepts a single argument: (1) a Dataset of Datasets, each representing a "split" of elements of the original dataset. The cardinality of the input dataset matches the number of the shards specified in the <code translate="no" dir="ltr">shard_func</code> (see above). The function should return a Dataset of elements of the original dataset.</p> <p>Users may want specify this function to control how snapshot files should be read from disk, including the amount of shuffling and parallelism.</p> <p>Here is an example of a standard reader function a user can define. This function enables both dataset shuffling and parallel reading of datasets:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">def user_reader_func(datasets):
  # shuffle the datasets splits
  datasets = datasets.shuffle(NUM_CORES)
  # read datasets in parallel and interleave their elements
  return datasets.interleave(lambda x: x, num_parallel_calls=AUTOTUNE)

dataset = dataset.apply(tf.data.experimental.snapshot("/path/to/snapshot/dir",
    reader_func=user_reader_func))
</pre> <p>By default, snapshot parallelizes reads by the number of cores available on the system, but will not attempt to shuffle the data.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">path</code> </td> <td> Required. A directory to use for storing / loading the snapshot to / from. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">compression</code> </td> <td> Optional. The type of compression to apply to the snapshot written to disk. Supported options are <code translate="no" dir="ltr">GZIP</code>, <code translate="no" dir="ltr">SNAPPY</code>, <code translate="no" dir="ltr">AUTO</code> or None. Defaults to AUTO, which attempts to pick an appropriate compression algorithm for the dataset. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">reader_func</code> </td> <td> Optional. A function to control how to read data from snapshot shards. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shard_func</code> </td> <td> Optional. A function to control how to shard data when writing a snapshot. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Dataset</code> transformation function, which can be passed to <a href="../dataset#apply"><code translate="no" dir="ltr">tf.data.Dataset.apply</code></a>. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/snapshot" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/snapshot</a>
  </p>
</div>
