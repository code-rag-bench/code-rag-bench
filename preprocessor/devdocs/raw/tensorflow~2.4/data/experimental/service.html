<h1 class="devsite-page-title">Module: tf.data.experimental.service</h1>       <p>API for using the tf.data service.</p> <h4 id="this_module_contains" data-text="This module contains:">This module contains:</h4> <ol> <li>tf.data server implementations for running the tf.data service.</li> <li>A <code translate="no" dir="ltr">distribute</code> dataset transformation that moves a dataset's preprocessing to happen in the tf.data service.</li> </ol> <p>The tf.data service offers a way to improve training speed when the host attached to a training device can't keep up with the data consumption of the model. For example, suppose a host can generate 100 examples/second, but the model can process 200 examples/second. Training speed could be doubled by using the tf.data service to generate 200 examples/second.</p> <h2 id="before_using_the_tfdata_service" data-text="Before using the tf.data service">Before using the tf.data service</h2> <p>There are a few things to do before using the tf.data service to speed up training.</p> <h3 id="understand_processing_mode" data-text="Understand processing_mode">Understand processing_mode</h3> <p>The tf.data service uses a cluster of workers to prepare data for training your model. The <code translate="no" dir="ltr">processing_mode</code> argument to <a href="service/distribute"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> describes how to leverage multiple workers to process the input dataset. Currently, there are two processing modes to choose from: "distributed_epoch" and "parallel_epochs".</p> <p>"distributed_epoch" means that the dataset will be split across all tf.data service workers. The dispatcher produces "splits" for the dataset and sends them to workers for further processing. For example, if a dataset begins with a list of filenames, the dispatcher will iterate through the filenames and send the filenames to tf.data workers, which will perform the rest of the dataset transformations on those files. "distributed_epoch" is useful when your model needs to see each element of the dataset exactly once, or if it needs to see the data in a generally-sequential order. "distributed_epoch" only works for datasets with splittable sources, such as <a href="../dataset#from_tensor_slices"><code translate="no" dir="ltr">Dataset.from_tensor_slices</code></a>, <a href="../dataset#list_files"><code translate="no" dir="ltr">Dataset.list_files</code></a>, or <a href="../dataset#range"><code translate="no" dir="ltr">Dataset.range</code></a>.</p> <p>"parallel_epochs" means that the entire input dataset will be processed independently by each of the tf.data service workers. For this reason, it is important to shuffle data (e.g. filenames) non-deterministically, so that each worker will process the elements of the dataset in a different order. "parallel_epochs" can be used to distribute datasets that aren't splittable.</p> <h3 id="measure_potential_impact" data-text="Measure potential impact">Measure potential impact</h3> <p>Before using the tf.data service, it is useful to first measure the potential performance improvement. To do this, add</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">dataset = dataset.take(1).cache().repeat()
</pre> <p>at the end of your dataset, and see how it affects your model's step time. <code translate="no" dir="ltr">take(1).cache().repeat()</code> will cache the first element of your dataset and produce it repeatedly. This should make the dataset very fast, so that the model becomes the bottleneck and you can identify the ideal model speed. With enough workers, the tf.data service should be able to achieve similar speed.</p> <h2 id="running_the_tfdata_service" data-text="Running the tf.data service">Running the tf.data service</h2> <p>tf.data servers should be brought up alongside your training jobs, and brought down when the jobs are finished. The tf.data service uses one <code translate="no" dir="ltr">DispatchServer</code> and any number of <code translate="no" dir="ltr">WorkerServers</code>. See <a href="https://github.com/tensorflow/ecosystem/tree/master/data_service">https://github.com/tensorflow/ecosystem/tree/master/data_service</a> for an example of using Google Kubernetes Engine (GKE) to manage the tf.data service. The server implementation in <a href="https://github.com/tensorflow/ecosystem/blob/master/data_service/tf_std_data_server.py">tf_std_data_server.py</a> is not GKE-specific, and can be used to run the tf.data service in other contexts.</p> <h3 id="fault_tolerance" data-text="Fault tolerance">Fault tolerance</h3> <p>By default, the tf.data dispatch server stores its state in-memory, making it a single point of failure during training. To avoid this, pass <code translate="no" dir="ltr">fault_tolerant_mode=True</code> when creating your <code translate="no" dir="ltr">DispatchServer</code>. Dispatcher fault tolerance requires <code translate="no" dir="ltr">work_dir</code> to be configured and accessible from the dispatcher both before and after restart (e.g. a GCS path). With fault tolerant mode enabled, the dispatcher will journal its state to the work directory so that no state is lost when the dispatcher is restarted.</p> <p>WorkerServers may be freely restarted, added, or removed during training. At startup, workers will register with the dispatcher and begin processing all outstanding jobs from the beginning.</p> <h2 id="using_the_tfdata_service_from_your_training_job" data-text="Using the tf.data service from your training job">Using the tf.data service from your training job</h2> <p>Once you have a tf.data service cluster running, take note of the dispatcher IP address and port. To connect to the service, you will use a string in the format "grpc://<dispatcher_address>:<dispatcher_port>".</dispatcher_port></dispatcher_address></p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp"># Create the dataset however you were before using the tf.data service.
dataset = your_dataset_factory()

service = "grpc://{}:{}".format(dispatcher_address, dispatcher_port)
# This will register the dataset with the tf.data service cluster so that
# tf.data workers can run the dataset to produce elements. The dataset returned
# from applying `distribute` will fetch elements produced by tf.data workers.
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=service))
</pre> <p>Below is a toy example that you can run yourself.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split("://")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode="parallel_epochs", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
</pre> <p>See the documentation of <a href="service/distribute"><code translate="no" dir="ltr">tf.data.experimental.service.distribute</code></a> for more details about using the <code translate="no" dir="ltr">distribute</code> transformation.</p> <h2 id="classes" data-text="Classes">Classes</h2> <p><a href="service/dispatchserver"><code translate="no" dir="ltr">class DispatchServer</code></a>: An in-process tf.data service dispatch server.</p> <p><a href="service/dispatcherconfig"><code translate="no" dir="ltr">class DispatcherConfig</code></a>: Configuration class for tf.data service dispatchers.</p> <p><a href="service/workerconfig"><code translate="no" dir="ltr">class WorkerConfig</code></a>: Configuration class for tf.data service dispatchers.</p> <p><a href="service/workerserver"><code translate="no" dir="ltr">class WorkerServer</code></a>: An in-process tf.data service worker server.</p> <h2 id="functions" data-text="Functions">Functions</h2> <p><a href="service/distribute"><code translate="no" dir="ltr">distribute(...)</code></a>: A transformation that moves dataset processing to the tf.data service.</p> <p><a href="service/from_dataset_id"><code translate="no" dir="ltr">from_dataset_id(...)</code></a>: Creates a dataset which reads data from the tf.data service.</p> <p><a href="service/register_dataset"><code translate="no" dir="ltr">register_dataset(...)</code></a>: Registers a dataset with the tf.data service.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/service" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/service</a>
  </p>
</div>
