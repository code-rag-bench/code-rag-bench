<h1 class="devsite-page-title">tf.data.experimental.OptimizationOptions</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/experimental/ops/optimization_options.py#L74-L329">  View source on GitHub </a> </td> </table> <p>Represents options for dataset optimizations.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/experimental/OptimizationOptions"><code translate="no" dir="ltr">tf.compat.v1.data.experimental.OptimizationOptions</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.data.experimental.OptimizationOptions()
</pre>  <p>You can set the optimization options of a dataset through the <code translate="no" dir="ltr">experimental_optimization</code> property of <a href="../options"><code translate="no" dir="ltr">tf.data.Options</code></a>; the property is an instance of <a href="optimizationoptions"><code translate="no" dir="ltr">tf.data.experimental.OptimizationOptions</code></a>.</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">options = tf.data.Options()
options.experimental_optimization.noop_elimination = True
options.experimental_optimization.map_vectorization.enabled = True
options.experimental_optimization.apply_default_optimizations = False
dataset = dataset.with_options(options)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">apply_default_optimizations</code> </td> <td> Whether to apply default graph optimizations. If False, only graph optimizations that have been explicitly enabled will be applied. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">autotune</code> </td> <td> Whether to automatically tune performance knobs. If None, defaults to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">autotune_buffers</code> </td> <td> When autotuning is enabled (through <code translate="no" dir="ltr">autotune</code>), determines whether to also autotune buffer sizes for datasets with parallelism. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">autotune_cpu_budget</code> </td> <td> When autotuning is enabled (through <code translate="no" dir="ltr">autotune</code>), determines the CPU budget to use. Values greater than the number of schedulable CPU cores are allowed but may result in CPU contention. If None, defaults to the number of schedulable CPU cores. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">autotune_ram_budget</code> </td> <td> When autotuning is enabled (through <code translate="no" dir="ltr">autotune</code>), determines the RAM budget to use. Values greater than the available RAM in bytes may result in OOM. If None, defaults to half of the available RAM in bytes. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">filter_fusion</code> </td> <td> Whether to fuse filter transformations. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">filter_with_random_uniform_fusion</code> </td> <td> Whether to fuse filter dataset that predicts random_uniform &lt; rate into a sampling dataset. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">hoist_random_uniform</code> </td> <td> Whether to hoist <code translate="no" dir="ltr">tf.random_uniform()</code> ops out of map transformations. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">map_and_batch_fusion</code> </td> <td> Whether to fuse map and batch transformations. If None, defaults to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">map_and_filter_fusion</code> </td> <td> Whether to fuse map and filter transformations. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">map_fusion</code> </td> <td> Whether to fuse map transformations. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">map_parallelization</code> </td> <td> Whether to parallelize stateless map transformations. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">map_vectorization</code> </td> <td> The map vectorization options associated with the dataset. See <a href="mapvectorizationoptions"><code translate="no" dir="ltr">tf.data.experimental.MapVectorizationOptions</code></a> for more details. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">noop_elimination</code> </td> <td> Whether to eliminate no-op transformations. If None, defaults to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parallel_batch</code> </td> <td> Whether to parallelize copying of batch elements. This optimization is highly experimental and can cause performance degradation (e.g. when the parallelization overhead exceeds the benefits of performing the data copies in parallel). You should only enable this optimization if a) your input pipeline is bottlenecked on batching and b) you have validated that this optimization improves performance. If None, defaults to False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">reorder_data_discarding_ops</code> </td> <td> Whether to reorder ops that will discard data to the front of unary cardinality preserving transformations, e.g. dataset.map(...).take(3) will be optimized to dataset.take(3).map(...). For now this optimization will move <code translate="no" dir="ltr">skip</code>, <code translate="no" dir="ltr">shard</code> and <code translate="no" dir="ltr">take</code> to the front of <code translate="no" dir="ltr">map</code> and <code translate="no" dir="ltr">prefetch</code>. This optimization is only for performance; it will not affect the output of the dataset. If None, defaults to True. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">shuffle_and_repeat_fusion</code> </td> <td> Whether to fuse shuffle and repeat transformations. If None, defaults to True. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="__eq__" data-text="__eq__"><code translate="no" dir="ltr">__eq__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/util/options.py#L41-L47">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__eq__(
    other
)
</pre> <p>Return self==value.</p> <h3 id="__ne__" data-text="__ne__"><code translate="no" dir="ltr">__ne__</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/data/util/options.py#L49-L53">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
__ne__(
    other
)
</pre> <p>Return self!=value.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/OptimizationOptions" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/data/experimental/OptimizationOptions</a>
  </p>
</div>
