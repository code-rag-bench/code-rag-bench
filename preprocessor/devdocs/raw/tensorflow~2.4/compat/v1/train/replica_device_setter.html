<h1 class="devsite-page-title">tf.compat.v1.train.replica_device_setter</h1>       <p>Return a <code translate="no" dir="ltr">device function</code> to use when building a Graph for replicas.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.train.replica_device_setter(
    ps_tasks=0, ps_device='/job:ps',
    worker_device='/job:worker', merge_devices=True, cluster=None,
    ps_ops=None, ps_strategy=None
)
</pre>  <p>Device Functions are used in <code translate="no" dir="ltr">with tf.device(device_function):</code> statement to automatically assign devices to <code translate="no" dir="ltr">Operation</code> objects as they are constructed, Device constraints are added from the inner-most context first, working outwards. The merging behavior adds constraints to fields that are yet unset by a more inner context. Currently the fields are (job, task, cpu/gpu).</p> <p>If <code translate="no" dir="ltr">cluster</code> is <code translate="no" dir="ltr">None</code>, and <code translate="no" dir="ltr">ps_tasks</code> is 0, the returned function is a no-op. Otherwise, the value of <code translate="no" dir="ltr">ps_tasks</code> is derived from <code translate="no" dir="ltr">cluster</code>.</p> <p>By default, only Variable ops are placed on ps tasks, and the placement strategy is round-robin over all ps tasks. A custom <code translate="no" dir="ltr">ps_strategy</code> may be used to do more intelligent placement, such as <code translate="no" dir="ltr">tf.contrib.training.GreedyLoadBalancingStrategy</code>.</p> <p>For example,</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
# jobs on hosts worker0, worker1 and worker2.
cluster_spec = {
    "ps": ["ps0:2222", "ps1:2222"],
    "worker": ["worker0:2222", "worker1:2222", "worker2:2222"]}
with
tf.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
  # Build your graph
  v1 = tf.Variable(...)  # assigned to /job:ps/task:0
  v2 = tf.Variable(...)  # assigned to /job:ps/task:1
  v3 = tf.Variable(...)  # assigned to /job:ps/task:0
# Run compute
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ps_tasks</code> </td> <td> Number of tasks in the <code translate="no" dir="ltr">ps</code> job. Ignored if <code translate="no" dir="ltr">cluster</code> is provided. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ps_device</code> </td> <td> String. Device of the <code translate="no" dir="ltr">ps</code> job. If empty no <code translate="no" dir="ltr">ps</code> job is used. Defaults to <code translate="no" dir="ltr">ps</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">worker_device</code> </td> <td> String. Device of the <code translate="no" dir="ltr">worker</code> job. If empty no <code translate="no" dir="ltr">worker</code> job is used. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">merge_devices</code> </td> <td> <code translate="no" dir="ltr">Boolean</code>. If <code translate="no" dir="ltr">True</code>, merges or only sets a device if the device constraint is completely unset. merges device specification rather than overriding them. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">cluster</code> </td> <td> <code translate="no" dir="ltr">ClusterDef</code> proto or <code translate="no" dir="ltr">ClusterSpec</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ps_ops</code> </td> <td> List of strings representing <code translate="no" dir="ltr">Operation</code> types that need to be placed on <code translate="no" dir="ltr">ps</code> devices. If <code translate="no" dir="ltr">None</code>, defaults to <code translate="no" dir="ltr">STANDARD_PS_OPS</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ps_strategy</code> </td> <td> A callable invoked for every ps <code translate="no" dir="ltr">Operation</code> (i.e. matched by <code translate="no" dir="ltr">ps_ops</code>), that takes the <code translate="no" dir="ltr">Operation</code> and returns the ps task index to use. If <code translate="no" dir="ltr">None</code>, defaults to a round-robin strategy across all <code translate="no" dir="ltr">ps</code> devices. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A function to pass to <a href="../../../device"><code translate="no" dir="ltr">tf.device()</code></a>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> <tr class="alt"> <td colspan="2"> TypeError if <code translate="no" dir="ltr">cluster</code> is not a dictionary or <code translate="no" dir="ltr">ClusterDef</code> protocol buffer, or if <code translate="no" dir="ltr">ps_strategy</code> is provided but not a callable. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/train/replica_device_setter" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/train/replica_device_setter</a>
  </p>
</div>
