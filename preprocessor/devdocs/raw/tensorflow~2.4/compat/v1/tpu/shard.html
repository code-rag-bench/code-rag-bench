<h1 class="devsite-page-title">tf.compat.v1.tpu.shard</h1>       <p>Shards <code translate="no" dir="ltr">computation</code> for parallel execution.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.tpu.shard(
    computation, inputs=None, num_shards=1, input_shard_axes=None,
    outputs_from_all_shards=True, output_shard_axes=None, infeed_queue=None,
    device_assignment=None, name=None, xla_options=None
)
</pre>  <p><code translate="no" dir="ltr">inputs</code> must be a list of Tensors or None (equivalent to an empty list), each of which has a corresponding split axis (from <code translate="no" dir="ltr">input_shard_axes</code>). Each input is split into <code translate="no" dir="ltr">num_shards</code> pieces along the corresponding axis, and computation is applied to each shard in parallel.</p> <p>Tensors are broadcast to all shards if they are lexically captured by <code translate="no" dir="ltr">computation</code>. e.g.,</p> <p>x = tf.constant(7) def computation(): return x + 3 ... = shard(computation, ...)</p> <p>as inputs.</p> <p>If <code translate="no" dir="ltr">outputs_from_all_shards</code> is true, the outputs from all shards of <code translate="no" dir="ltr">computation</code> are concatenated back together along their <code translate="no" dir="ltr">output_shard_axes</code>. Otherwise, each output is taken from an arbitrary shard.</p> <p>Inputs and outputs of the computation must be at least rank-1 Tensors.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">computation</code> </td> <td> A Python function that builds a computation to apply to each shard of the input. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">inputs</code> </td> <td> A list of input tensors or None (equivalent to an empty list). Each input tensor has a corresponding shard axes, given by <code translate="no" dir="ltr">input_shard_axes</code>, which must have size divisible by <code translate="no" dir="ltr">num_shards</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_shards</code> </td> <td> The number of shards. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">input_shard_axes</code> </td> <td> A list of dimensions along which to shard <code translate="no" dir="ltr">inputs</code>, or <code translate="no" dir="ltr">None</code>. <code translate="no" dir="ltr">None</code> means "shard all inputs along dimension 0". If not <code translate="no" dir="ltr">None</code>, there must be one dimension per input. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">outputs_from_all_shards</code> </td> <td> Boolean or list of boolean. For each output, if <code translate="no" dir="ltr">True</code>, outputs from all shards are concatenated along the corresponding <code translate="no" dir="ltr">output_shard_axes</code> entry. Otherwise, each output is taken from an arbitrary shard. If the argument is a boolean, the argument's value is used for each output. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">output_shard_axes</code> </td> <td> A list of dimensions along which to concatenate the outputs of <code translate="no" dir="ltr">computation</code>, or <code translate="no" dir="ltr">None</code>. <code translate="no" dir="ltr">None</code> means "concatenate all outputs along dimension 0". If not <code translate="no" dir="ltr">None</code>, there must be one dimension per output. Ignored if <code translate="no" dir="ltr">outputs_from_all_shards</code> is False. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">infeed_queue</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, the <code translate="no" dir="ltr">InfeedQueue</code> to use to augment the inputs of <code translate="no" dir="ltr">computation</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">device_assignment</code> </td> <td> If not <code translate="no" dir="ltr">None</code>, a <code translate="no" dir="ltr">DeviceAssignment</code> describing the mapping between logical cores in the computation with physical cores in the TPU topology. Uses a default device assignment if <code translate="no" dir="ltr">None</code>. The <code translate="no" dir="ltr">DeviceAssignment</code> may be omitted if each shard of the computation uses only one core, and there is either only one shard, or the number of shards is equal to the number of cores in the TPU system. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> (Deprecated) Does nothing. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">xla_options</code> </td> <td> An instance of <code translate="no" dir="ltr">tpu.XLAOptions</code> which indicates the options passed to XLA compiler. Use <code translate="no" dir="ltr">None</code> for default options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of output tensors. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If num_shards &lt;= 0 </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If len(input_shard_axes) != len(inputs) </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If len(output_shard_axes) != len(outputs from <code translate="no" dir="ltr">computation</code>) </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/tpu/shard" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/tpu/shard</a>
  </p>
</div>
