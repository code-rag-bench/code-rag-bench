<h1 class="devsite-page-title">tf.compat.v1.distribute.ReplicaContext</h1>       <p>A class with a collection of APIs that can be called in a replica context.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.distribute.ReplicaContext(
    strategy, replica_id_in_sync_group
)
</pre>  <p>You can use <a href="../../../distribute/get_replica_context"><code translate="no" dir="ltr">tf.distribute.get_replica_context</code></a> to get an instance of <code translate="no" dir="ltr">ReplicaContext</code>, which can only be called inside the function passed to <a href="../../../distribute/strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
def func():
  replica_context = tf.distribute.get_replica_context()
  return replica_context.replica_id_in_sync_group
strategy.run(func)
PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;
}
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> A <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> An integer, a <code translate="no" dir="ltr">Tensor</code> or None. Prefer an integer whenever possible to avoid issues with nested <a href="../../../function"><code translate="no" dir="ltr">tf.function</code></a>. It accepts a <code translate="no" dir="ltr">Tensor</code> only to be compatible with <code translate="no" dir="ltr">tpu.replicate</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">devices</code> </td> <td> Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated) <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please avoid relying on devices property.</span></aside><blockquote class="note">
<strong>Note:</strong><span> For <a href="../../../distribute/mirroredstrategy"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a> and <a href="../../../distribute/experimental/multiworkermirroredstrategy"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>, this returns a nested list of device strings, e.g, [["GPU:0"]]. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_replicas_in_sync</code> </td> <td> Returns number of replicas that are kept in sync. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> Returns the id of the replica. <p>This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to <a href="../../../distribute/replicacontext#num_replicas_in_sync"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.num_replicas_in_sync</code></a> - 1.</p> <blockquote class="note">
<strong>Note:</strong><span> This is not guaranteed to be the same ID as the XLA replica ID use for low-level operations such as collective_permute. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> The current <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="all_reduce" data-text="all_reduce"><code translate="no" dir="ltr">all_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3009-L3094">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
all_reduce(
    reduce_op, value, options=None
)
</pre> <p>All-reduces <code translate="no" dir="ltr">value</code> across all replicas.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value = tf.identity(1.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)
strategy.experimental_local_results(strategy.run(step_fn))
(&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;)
</pre> <p>It supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify <code translate="no" dir="ltr">options</code> to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value1 = tf.identity(1.)
  value2 = tf.identity(2.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, [value1, value2])
strategy.experimental_local_results(strategy.run(step_fn))
([PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;
}],)
</pre> <p>Note that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone.</p> <p>This API currently can only be called in the replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>: the reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>: the batched reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="../../../distribute/strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="../../../distribute/reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a nested structure of <a href="../../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> which <a href="../../../nest/flatten"><code translate="no" dir="ltr">tf.nest.flatten</code></a> accepts. The structure and the shapes of the <a href="../../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> need to be same on all replicas. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with the reduced values. The structure is the same as <code translate="no" dir="ltr">value</code>. </td> </tr> 
</table> <h3 id="merge_call" data-text="merge_call"><code translate="no" dir="ltr">merge_call</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2908-L2941">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
merge_call(
    merge_fn, args=(), kwargs=None
)
</pre> <p>Merge args across replicas and run <code translate="no" dir="ltr">merge_fn</code> in a cross-replica context.</p> <p>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to <code translate="no" dir="ltr">strategy.run(step_fn, ...)</code>.</p> <p>See <a href="../../../distribute/strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> for an explanation.</p> <p>If not inside a distributed scope, this is equivalent to:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.get_strategy()
with cross-replica-context(strategy):
  return merge_fn(strategy, *args, **kwargs)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">merge_fn</code> </td> <td> Function that joins arguments from threads that are given as PerReplica. It accepts <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> List or tuple with positional per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The return value of <code translate="no" dir="ltr">merge_fn</code>, except for <code translate="no" dir="ltr">PerReplica</code> values which are unpacked. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/distribute/ReplicaContext" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/distribute/ReplicaContext</a>
  </p>
</div>
