<h1 class="devsite-page-title">tf.compat.v1.distribute.StrategyExtended</h1>       <p>Additional APIs for algorithms that need to be distribution-aware.</p> <p>Inherits From: <a href="../../../distribute/strategyextended"><code translate="no" dir="ltr">StrategyExtended</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.compat.v1.distribute.StrategyExtended(
    container_strategy
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> For most usage of <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>, there should be no need to call these methods, since TensorFlow libraries (such as optimizers) already call these methods when needed on your behalf.</span>
</blockquote> <p>Some common use cases of functions on this page:</p> <ul> <li><em>Locality</em></li> </ul> <p><a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> can have the same <em>locality</em> as a <em>distributed variable</em>, which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to <a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> to update the value of a variable. You may use <a href="../../../distribute/strategyextended#colocate_vars_with"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.colocate_vars_with</code></a> to give a variable the same locality as another variable. You may convert a "PerReplica" value to a variable's locality by using <a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a> or <a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>.</p> <ul> <li><em>How to update a distributed variable</em></li> </ul> <p>A distributed variable is variables created on multiple devices. As discussed in the <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">glossary</a>, mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:</p> <ol> <li>In your function passed to <a href="../../../distribute/strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>, compute a list of (update, variable) pairs. For example, the update might be a gradient of the loss with respect to the variable.</li> <li>Switch to cross-replica mode by calling <code translate="no" dir="ltr">tf.distribute.get_replica_context().merge_call()</code> with the updates and variables as arguments.</li> <li>Call <a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to(VariableAggregation.SUM, t, v)</code></a> (for one variable) or <a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a> (for a list of variables) to sum the updates.</li> <li>Call <a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update(v)</code></a> for each variable to update its value.</li> </ol> <p>Steps 2 through 4 are done automatically by class <a href="../../../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> if you call its <a href="../../../keras/optimizers/optimizer#apply_gradients"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer.apply_gradients</code></a> method in a replica context.</p> <p>In fact, a higher-level solution to update a distributed variable is by calling <code translate="no" dir="ltr">assign</code> on the variable as you would do to a regular <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a>. You can call the method in both <em>replica context</em> and <em>cross-replica context</em>. For a <em>mirrored variable</em>, calling <code translate="no" dir="ltr">assign</code> in <em>replica context</em> requires you to specify the <code translate="no" dir="ltr">aggregation</code> type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call <code translate="no" dir="ltr">assign</code> on <em>mirrored variable</em> in <em>cross-replica context</em>, you can only assign a single value or assign values from another mirrored variable or a mirrored <a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. For a <em>SyncOnRead variable</em>, in <em>replica context</em>, you can simply call <code translate="no" dir="ltr">assign</code> on it and no aggregation happens under the hood. In <em>cross-replica context</em>, you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the <code translate="no" dir="ltr">aggregation</code> type of the variable is <a href="../../../variableaggregation#SUM"><code translate="no" dir="ltr">tf.VariableAggregation.SUM</code></a>, it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the <code translate="no" dir="ltr">aggregation</code> type is <a href="../../../variableaggregation#MEAN"><code translate="no" dir="ltr">tf.VariableAggregation.MEAN</code></a>, the value is assigned to each replica directly.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">experimental_between_graph</code> </td> <td> Whether the strategy uses between-graph replication or not. <p>This is expected to return a constant value that will not be changed throughout its life cycle. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_require_static_shapes</code> </td> <td> Returns <code translate="no" dir="ltr">True</code> if static shape is required; <code translate="no" dir="ltr">False</code> otherwise. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">experimental_should_init</code> </td> <td> Whether initialization is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parameter_devices</code> </td> <td> Returns the tuple of all devices used to place variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">should_checkpoint</code> </td> <td> Whether checkpointing is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">should_save_summary</code> </td> <td> Whether saving summaries is needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">worker_devices</code> </td> <td> Returns the tuple of all devices used to for compute replica execution. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="batch_reduce_to" data-text="batch_reduce_to"><code translate="no" dir="ltr">batch_reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2304-L2374">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
batch_reduce_to(
    reduce_op, value_destination_pairs, options=None
)
</pre> <p>Combine multiple <code translate="no" dir="ltr">reduce_to</code> calls into one for faster execution.</p> <p>Similar to <code translate="no" dir="ltr">reduce_to</code>, but accepts a list of (value, destinations) pairs. It's more efficient than reduce each value separately.</p> <p>This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="../../../distribute/strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>: the non-batch version of this API.</li> <li>
<a href="../../../distribute/replicacontext#all_reduce"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.all_reduce</code></a>: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.</li> <li>
<a href="../../../distribute/strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul> <p>See <code translate="no" dir="ltr">reduce_to</code> for more information.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
@tf.function
def step_fn(var):

  def merge_fn(strategy, value, var):
    # All-reduce the value. Note that `value` here is a
    # `tf.distribute.DistributedValues`.
    reduced = strategy.extended.batch_reduce_to(
        tf.distribute.ReduceOp.SUM, [(value, var)])[0]
    strategy.extended.update(var, lambda var, value: var.assign(value),
        args=(reduced,))

  value = tf.identity(1.)
  tf.distribute.get_replica_context().merge_call(merge_fn,
    args=(value, var))

def run(strategy):
  with strategy.scope():
    v = tf.Variable(0.)
    strategy.run(step_fn, args=(v,))
    return v

run(tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"]))
MirroredVariable:{
  0: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0&gt;
}
run(tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=["GPU:0", "GPU:1"], parameter_device="CPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;
run(tf.distribute.OneDeviceStrategy("GPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="../../../distribute/reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_destination_pairs</code> </td> <td> a sequence of (value, destinations) pairs. See <code translate="no" dir="ltr">tf.distribute.Strategy.reduce_to</code> for descriptions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of reduced values, one per pair in <code translate="no" dir="ltr">value_destination_pairs</code>. </td> </tr> 
</table> <h3 id="broadcast_to" data-text="broadcast_to"><code translate="no" dir="ltr">broadcast_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2611-L2626">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
broadcast_to(
    tensor, destinations
)
</pre> <p>Mirror a tensor on one device to all worker devices.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> A Tensor value to broadcast. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> A mirrored variable or device string specifying the destination devices to copy <code translate="no" dir="ltr">tensor</code> to. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A value mirrored to <code translate="no" dir="ltr">destinations</code> devices. </td> </tr> 
</table> <h3 id="call_for_each_replica" data-text="call_for_each_replica"><code translate="no" dir="ltr">call_for_each_replica</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2682-L2730">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
call_for_each_replica(
    fn, args=(), kwargs=None
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> once per replica.</p> <p><code translate="no" dir="ltr">fn</code> may call <code translate="no" dir="ltr">tf.get_replica_context()</code> to access methods such as <code translate="no" dir="ltr">replica_id_in_sync_group</code> and <code translate="no" dir="ltr">merge_call()</code>.</p> <p><code translate="no" dir="ltr">merge_call()</code> is used to communicate between the replicas and re-enter the cross-replica context. All replicas pause their execution having encountered a <code translate="no" dir="ltr">merge_call()</code> call. After that the <code translate="no" dir="ltr">merge_fn</code>-function is executed. Its results are then unwrapped and given back to each replica call. After that execution resumes until <code translate="no" dir="ltr">fn</code> is complete or encounters another <code translate="no" dir="ltr">merge_call()</code>. Example:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python"># Called once in "cross-replica" context.
def merge_fn(distribution, three_plus_replica_id):
  # sum the values across replicas
  return sum(distribution.experimental_local_results(three_plus_replica_id))

# Called once per replica in `distribution`, in a "replica" context.
def fn(three):
  replica_ctx = tf.get_replica_context()
  v = three + replica_ctx.replica_id_in_sync_group
  # Computes the sum of the `v` values across all replicas.
  s = replica_ctx.merge_call(merge_fn, args=(v,))
  return s + v

with distribution.scope():
  # in "cross-replica" context
  ...
  merged_results = distribution.run(fn, args=[3])
  # merged_results has the values from every replica execution of `fn`.
  # This statement prints a list:
  print(distribution.experimental_local_results(merged_results))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> function to run (will be run once per replica). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list with positional arguments for <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments for <code translate="no" dir="ltr">fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Merged return value of <code translate="no" dir="ltr">fn</code> across all replicas. </td> </tr> 
</table> <h3 id="colocate_vars_with" data-text="colocate_vars_with"><code translate="no" dir="ltr">colocate_vars_with</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2145-L2190">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
colocate_vars_with(
    colocate_with_variable
)
</pre> <p>Scope that controls which devices variables will be created on.</p> <p>No operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope).</p> <p>This may only be used inside <code translate="no" dir="ltr">self.scope()</code>.</p> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">with strategy.scope():
  var1 = tf.Variable(...)
  with strategy.extended.colocate_vars_with(var1):
    # var2 and var3 will be created on the same device(s) as var1
    var2 = tf.Variable(...)
    var3 = tf.Variable(...)

  def fn(v1, v2, v3):
    # operates on v1 from var1, v2 from var2, and v3 from var3

  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there
  # too.
  strategy.extended.update(var1, fn, args=(var2, var3))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">colocate_with_variable</code> </td> <td> A variable created in this strategy's <code translate="no" dir="ltr">scope()</code>. Variables created while in the returned context manager will be on the same set of devices as <code translate="no" dir="ltr">colocate_with_variable</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A context manager. </td> </tr> 
</table> <h3 id="experimental_make_numpy_dataset" data-text="experimental_make_numpy_dataset"><code translate="no" dir="ltr">experimental_make_numpy_dataset</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2588-L2606">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_make_numpy_dataset(
    numpy_input, session=None
)
</pre> <p>Makes a dataset for input provided via a numpy array.</p> <p>This avoids adding <code translate="no" dir="ltr">numpy_input</code> as a large constant in the graph, and copies the data to the machine or machines that will be processing the input.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">numpy_input</code> </td> <td> A nest of NumPy input arrays that will be distributed evenly across all replicas. Note that lists of Numpy arrays are stacked, as that is normal <a href="../../../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> behavior. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">session</code> </td> <td> (TensorFlow v1.x graph execution only) A session used for initialization. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../../../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> representing <code translate="no" dir="ltr">numpy_input</code>. </td> </tr> 
</table> <h3 id="experimental_run_steps_on_iterator" data-text="experimental_run_steps_on_iterator"><code translate="no" dir="ltr">experimental_run_steps_on_iterator</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2631-L2676">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_run_steps_on_iterator(
    fn, iterator, iterations=1, initial_loop_values=None
)
</pre> <p>DEPRECATED: please use <code translate="no" dir="ltr">run</code> instead.</p> <p>Run <code translate="no" dir="ltr">fn</code> with input from <code translate="no" dir="ltr">iterator</code> for <code translate="no" dir="ltr">iterations</code> times.</p> <p>This method can be used to run a step function for training a number of times using input from a dataset.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> function to run using this distribution strategy. The function must have the following signature: <code translate="no" dir="ltr">def fn(context, inputs)</code>. <code translate="no" dir="ltr">context</code> is an instance of <code translate="no" dir="ltr">MultiStepContext</code> that will be passed when <code translate="no" dir="ltr">fn</code> is run. <code translate="no" dir="ltr">context</code> can be used to specify the outputs to be returned from <code translate="no" dir="ltr">fn</code> by calling <code translate="no" dir="ltr">context.set_last_step_output</code>. It can also be used to capture non tensor outputs by <code translate="no" dir="ltr">context.set_non_tensor_output</code>. See <code translate="no" dir="ltr">MultiStepContext</code> documentation for more information. <code translate="no" dir="ltr">inputs</code> will have same type/structure as <code translate="no" dir="ltr">iterator.get_next()</code>. Typically, <code translate="no" dir="ltr">fn</code> will use <code translate="no" dir="ltr">call_for_each_replica</code> method of the strategy to distribute the computation over multiple replicas. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">iterator</code> </td> <td> Iterator of a dataset that represents the input for <code translate="no" dir="ltr">fn</code>. The caller is responsible for initializing the iterator as needed. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">iterations</code> </td> <td> (Optional) Number of iterations that <code translate="no" dir="ltr">fn</code> should be run. Defaults to 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">initial_loop_values</code> </td> <td> (Optional) Initial values to be passed into the loop that runs <code translate="no" dir="ltr">fn</code>. Defaults to <code translate="no" dir="ltr">None</code>. initial_loop_values argument when we have a mechanism to infer the outputs of <code translate="no" dir="ltr">fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Returns the <code translate="no" dir="ltr">MultiStepContext</code> object which has the following properties, among other things: <ul> <li>run_op: An op that runs <code translate="no" dir="ltr">fn</code> <code translate="no" dir="ltr">iterations</code> times.</li> <li>last_step_outputs: A dictionary containing tensors set using <code translate="no" dir="ltr">context.set_last_step_output</code>. Evaluating this returns the value of the tensors after the last iteration.</li> <li>non_tensor_outputs: A dictionary containing anything that was set by <code translate="no" dir="ltr">fn</code> by calling <code translate="no" dir="ltr">context.set_non_tensor_output</code>. </li>
</ul>
</td> </tr> 
</table> <h3 id="non_slot_devices" data-text="non_slot_devices"><code translate="no" dir="ltr">non_slot_devices</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2780-L2799">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
non_slot_devices(
    var_list
)
</pre> <p>Device(s) for non-slot variables.</p> <p>DEPRECATED: TF 1.x ONLY.</p> <p>This method returns non-slot devices where non-slot variables are placed. Users can create non-slot variables on these devices by using a block:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">with tf.distribute.StrategyExtended.colocate_vars_with(tf.distribute.StrategyExtended.non_slot_devices(...)):
  ...
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var_list</code> </td> <td> The list of variables being optimized, needed with the default <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A sequence of devices for non-slot variables. </td> </tr> 
</table> <h3 id="read_var" data-text="read_var"><code translate="no" dir="ltr">read_var</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2735-L2748">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
read_var(
    v
)
</pre> <p>Reads the value of a variable.</p> <p>Returns the aggregate value of a replica-local variable, or the (read-only) value of any other variable.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">v</code> </td> <td> A variable allocated within the scope of this <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor representing the value of <code translate="no" dir="ltr">v</code>, aggregated across replicas if necessary. </td> </tr> 
</table> <h3 id="reduce_to" data-text="reduce_to"><code translate="no" dir="ltr">reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2216-L2299">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reduce_to(
    reduce_op, value, destinations, options=None
)
</pre> <p>Combine (via e.g. sum or mean) values across replicas.</p> <p><code translate="no" dir="ltr">reduce_to</code> aggregates <a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> and distributed variables. It supports both dense values and <a href="../../../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p>This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="../../../distribute/strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>: the batch version of this API.</li> <li>
<a href="../../../distribute/replicacontext#all_reduce"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.all_reduce</code></a>: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.</li> <li>
<a href="../../../distribute/strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul> <p><code translate="no" dir="ltr">destinations</code> specifies where to reduce the value to, e.g. "GPU:0". You can also pass in a <code translate="no" dir="ltr">Tensor</code>, and the destinations will be the device of that tensor. For all-reduce, pass the same to <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">destinations</code>.</p> <p>It can be used in <a href="../../../distribute/replicacontext#merge_call"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.merge_call</code></a> to write code that works for all <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
@tf.function
def step_fn(var):

  def merge_fn(strategy, value, var):
    # All-reduce the value. Note that `value` here is a
    # `tf.distribute.DistributedValues`.
    reduced = strategy.extended.reduce_to(tf.distribute.ReduceOp.SUM,
        value, destinations=var)
    strategy.extended.update(var, lambda var, value: var.assign(value),
        args=(reduced,))

  value = tf.identity(1.)
  tf.distribute.get_replica_context().merge_call(merge_fn,
    args=(value, var))

def run(strategy):
  with strategy.scope():
    v = tf.Variable(0.)
    strategy.run(step_fn, args=(v,))
    return v

run(tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"]))
MirroredVariable:{
  0: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0&gt;
}
run(tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=["GPU:0", "GPU:1"], parameter_device="CPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;
run(tf.distribute.OneDeviceStrategy("GPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="../../../distribute/reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a <a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, or a <a href="../../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> like object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> a <a href="../../../distribute/distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, a <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, a <a href="../../../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">destinations</code>. Note that if it's a <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, the value is reduced to the devices of that variable, and this method doesn't update the variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="../../../distribute/strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="../../../distribute/experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or value reduced to <code translate="no" dir="ltr">destinations</code>. </td> </tr> 
</table> <h3 id="update" data-text="update"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2426-L2494">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update(
    var, fn, args=(), kwargs=None, group=True
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> to update <code translate="no" dir="ltr">var</code> using inputs mirrored to the same devices.</p> <p><a href="../../../distribute/strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> takes a distributed variable <code translate="no" dir="ltr">var</code> to be updated, an update function <code translate="no" dir="ltr">fn</code>, and <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code> for <code translate="no" dir="ltr">fn</code>. It applies <code translate="no" dir="ltr">fn</code> to each component variable of <code translate="no" dir="ltr">var</code> and passes corresponding values from <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code>. Neither <code translate="no" dir="ltr">args</code> nor <code translate="no" dir="ltr">kwargs</code> may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling <code translate="no" dir="ltr">fn</code>. For example, <code translate="no" dir="ltr">fn</code> can be <code translate="no" dir="ltr">assign_add</code> and <code translate="no" dir="ltr">args</code> can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable <code translate="no" dir="ltr">var</code>. Calling <code translate="no" dir="ltr">update</code> will call <code translate="no" dir="ltr">assign_add</code> on each component variable of <code translate="no" dir="ltr">var</code> with the corresponding tensor value on that device.</p> <h4 id="example_usage_2" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1']) # With 2
devices
with strategy.scope():
  v = tf.Variable(5.0, aggregation=tf.VariableAggregation.SUM)
def update_fn(v):
  return v.assign(1.0)
result = strategy.extended.update(v, update_fn)
# result is
# Mirrored:{
#  0: tf.Tensor(1.0, shape=(), dtype=float32),
#  1: tf.Tensor(1.0, shape=(), dtype=float32)
# }
</pre> <p>If <code translate="no" dir="ltr">var</code> is mirrored across multiple devices, then this method implements logic as following:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">results = {}
for device, v in var:
  with tf.device(device):
    # args and kwargs will be unwrapped if they are mirrored.
    results[device] = fn(v, *args, **kwargs)
return merged(results)
</pre> <p>Otherwise, this method returns <code translate="no" dir="ltr">fn(var, *args, **kwargs)</code> colocated with <code translate="no" dir="ltr">var</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> Variable, possibly mirrored to multiple devices, to operate on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> Function to call. Should take the variable as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list. Additional positional arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">group</code> </td> <td> Boolean. Defaults to True. If False, the return value will be unwrapped. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> By default, the merged return value of <code translate="no" dir="ltr">fn</code> across all replicas. The merged result has dependencies to make sure that if it is evaluated at all, the side effects (updates) will happen on every replica. If instead "group=False" is specified, this function will return a nest of lists where each list has an element per replica, and the caller is responsible for ensuring all elements are executed. </td> </tr> 
</table> <h3 id="update_non_slot" data-text="update_non_slot"><code translate="no" dir="ltr">update_non_slot</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2750-L2775">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update_non_slot(
    colocate_with, fn, args=(), kwargs=None, group=True
)
</pre> <p>Runs <code translate="no" dir="ltr">fn(*args, **kwargs)</code> on <code translate="no" dir="ltr">colocate_with</code> devices.</p> <p>Used to update non-slot variables.</p> <p>DEPRECATED: TF 1.x ONLY.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">colocate_with</code> </td> <td> Devices returned by <code translate="no" dir="ltr">non_slot_devices()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> Function to execute. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list. Positional arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">group</code> </td> <td> Boolean. Defaults to True. If False, the return value will be unwrapped. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Return value of <code translate="no" dir="ltr">fn</code>, possibly merged across devices. </td> </tr> 
</table> <h3 id="value_container" data-text="value_container"><code translate="no" dir="ltr">value_container</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2502-L2515">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
value_container(
    value
)
</pre> <p>Returns the container that this per-replica <code translate="no" dir="ltr">value</code> belongs to.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A value returned by <code translate="no" dir="ltr">run()</code> or a variable created in <code translate="no" dir="ltr">scope()</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A container that <code translate="no" dir="ltr">value</code> belongs to. If value does not belong to any container (including the case of container having been destroyed), returns the value itself. <code translate="no" dir="ltr">value in experimental_local_results(value_container(value))</code> will always be true. </td> </tr> 
</table> <h3 id="variable_created_in_scope" data-text="variable_created_in_scope"><code translate="no" dir="ltr">variable_created_in_scope</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2119-L2143">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variable_created_in_scope(
    v
)
</pre> <p>Tests whether <code translate="no" dir="ltr">v</code> was created while this strategy scope was active.</p> <p>Variables created inside the strategy scope are "owned" by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
True
</pre> <p>Variables created outside the strategy are not owned by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
False
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">v</code> </td> <td> A <a href="../../../variable"><code translate="no" dir="ltr">tf.Variable</code></a> instance. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> True if <code translate="no" dir="ltr">v</code> was created inside the scope, False if not. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/distribute/StrategyExtended" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/compat/v1/distribute/StrategyExtended</a>
  </p>
</div>
