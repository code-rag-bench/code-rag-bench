<h1 class="devsite-page-title">tf.distribute.ReplicaContext</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3108-L3268">  View source on GitHub </a> </td> </table> <p>A class with a collection of APIs that can be called in a replica context.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.ReplicaContext(
    strategy, replica_id_in_sync_group
)
</pre>  <p>You can use <a href="get_replica_context"><code translate="no" dir="ltr">tf.distribute.get_replica_context</code></a> to get an instance of <code translate="no" dir="ltr">ReplicaContext</code>, which can only be called inside the function passed to <a href="strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
def func():
  replica_context = tf.distribute.get_replica_context()
  return replica_context.replica_id_in_sync_group
strategy.run(func)
PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=int32, numpy=0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;
}
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> A <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> An integer, a <code translate="no" dir="ltr">Tensor</code> or None. Prefer an integer whenever possible to avoid issues with nested <a href="../function"><code translate="no" dir="ltr">tf.function</code></a>. It accepts a <code translate="no" dir="ltr">Tensor</code> only to be compatible with <code translate="no" dir="ltr">tpu.replicate</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">devices</code> </td> <td> Returns the devices this replica is to be executed on, as a tuple of strings. (deprecated) <aside class="warning"><strong>Warning:</strong><span> THIS FUNCTION IS DEPRECATED. It will be removed in a future version. Instructions for updating: Please avoid relying on devices property.</span></aside><blockquote class="note">
<strong>Note:</strong><span> For <a href="mirroredstrategy"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a> and <a href="experimental/multiworkermirroredstrategy"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a>, this returns a nested list of device strings, e.g, [["GPU:0"]]. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_replicas_in_sync</code> </td> <td> Returns number of replicas that are kept in sync. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">replica_id_in_sync_group</code> </td> <td> Returns the id of the replica. <p>This identifies the replica among all replicas that are kept in sync. The value of the replica id can range from 0 to <a href="replicacontext#num_replicas_in_sync"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.num_replicas_in_sync</code></a> - 1.</p> <blockquote class="note">
<strong>Note:</strong><span> This is not guaranteed to be the same ID as the XLA replica ID use for low-level operations such as collective_permute. </span>
</blockquote>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">strategy</code> </td> <td> The current <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="all_gather" data-text="all_gather"><code translate="no" dir="ltr">all_gather</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3112-L3268">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
all_gather(
    value, axis, options=None
)
</pre> <p>All-gathers <code translate="no" dir="ltr">value</code> across all replicas along <code translate="no" dir="ltr">axis</code>.</p> <blockquote class="note">
<strong>Note:</strong><span> An <code translate="no" dir="ltr">all_gather</code> method can only be called in replica context. For a cross-replica context counterpart, see <a href="strategy#gather"><code translate="no" dir="ltr">tf.distribute.Strategy.gather</code></a>. All replicas need to participate in the all-gather, otherwise this operation hangs. So if <code translate="no" dir="ltr">all_gather</code> is called in any replica, it must be called in all replicas.</span>
</blockquote>
<blockquote class="note">
<strong>Note:</strong><span> If there are multiple <code translate="no" dir="ltr">all_gather</code> calls, they need to be executed in the same order on all replicas. Dispatching <code translate="no" dir="ltr">all_gather</code> based on conditions is usually error-prone.</span>
</blockquote> <p>For all strategies except <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, the input <code translate="no" dir="ltr">value</code> on different replicas must have the same rank, and their shapes must be the same in all dimensions except the <code translate="no" dir="ltr">axis</code>-th dimension. In other words, their shapes cannot be different in a dimension <code translate="no" dir="ltr">d</code> where <code translate="no" dir="ltr">d</code> does not equal to the <code translate="no" dir="ltr">axis</code> argument. For example, given a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> with component tensors of shape <code translate="no" dir="ltr">(1, 2, 3)</code> and <code translate="no" dir="ltr">(1, 3, 3)</code> on two replicas, you can call <code translate="no" dir="ltr">all_gather(..., axis=1, ...)</code> on it, but not <code translate="no" dir="ltr">all_gather(..., axis=0, ...)</code> or <code translate="no" dir="ltr">all_gather(..., axis=2, ...)</code>. However, with <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, all tensors must have exactly the same rank and same shape.</p> <blockquote class="note">
<strong>Note:</strong><span> The input <code translate="no" dir="ltr">value</code> must have a non-zero rank. Otherwise, consider using <a href="../expand_dims"><code translate="no" dir="ltr">tf.expand_dims</code></a> before gathering them.</span>
</blockquote> <p>You can pass in a single tensor to all-gather:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def gather_value():
  ctx = tf.distribute.get_replica_context()
  local_value = tf.constant([1, 2, 3])
  return ctx.all_gather(local_value, axis=0)
result = strategy.run(gather_value)
result
PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}
strategy.experimental_local_results(result)
(&lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],
dtype=int32)&gt;,
&lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3],
dtype=int32)&gt;)
</pre> <p>You can also pass in a nested structure of tensors to all-gather, say, a list:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
@tf.function
def gather_nest():
  ctx = tf.distribute.get_replica_context()
  value_1 = tf.constant([1, 2, 3])
  value_2 = tf.constant([[1, 2], [3, 4]])
  # all_gather a nest of `tf.distribute.DistributedValues`
  return ctx.all_gather([value_1, value_2], axis=0)
result = strategy.run(gather_nest)
result
[PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;
}]
strategy.experimental_local_results(result)
([PerReplica:{
  0: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(6,), dtype=int32, numpy=array([1, 2, 3, 1, 2, 3], dtype=int32)&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;,
  1: &lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=
array([[1, 2],
       [3, 4],
       [1, 2],
       [3, 4]], dtype=int32)&gt;
}],)
</pre> <p>What if you are all-gathering tensors with different shapes on different replicas? Consider the following example with two replicas, where you have <code translate="no" dir="ltr">value</code> as a nested structure consisting of two items to all-gather, <code translate="no" dir="ltr">a</code> and <code translate="no" dir="ltr">b</code>.</p> <p>On Replica 0, <code translate="no" dir="ltr">value</code> is {'a': [0], 'b': [[0, 1]]} On Replica 1, <code translate="no" dir="ltr">value</code> is {'a': [1], 'b': [[2, 3], [4, 5]]}</p> <p>Result for <code translate="no" dir="ltr">all_gather</code> with <code translate="no" dir="ltr">axis</code>=0: (on each of the replicas): {'a': [1, 2], 'b': [[0, 1], [2, 3], [4, 5]]}</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> which <a href="../nest/flatten"><code translate="no" dir="ltr">tf.nest.flatten</code></a> accepts, or a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> instance. The structure of the <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> need to be same on all replicas. The underlying tensor constructs can only be dense tensors with non-zero rank, NOT <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> 0-D int32 Tensor. Dimension along which to gather. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with the gathered values. The structure is the same as <code translate="no" dir="ltr">value</code>. </td> </tr> 
</table> <h3 id="all_reduce" data-text="all_reduce"><code translate="no" dir="ltr">all_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L3009-L3094">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
all_reduce(
    reduce_op, value, options=None
)
</pre> <p>All-reduces <code translate="no" dir="ltr">value</code> across all replicas.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value = tf.identity(1.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, value)
strategy.experimental_local_results(strategy.run(step_fn))
(&lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;)
</pre> <p>It supports batched operations. You can pass a list of values and it attempts to batch them when possible. You can also specify <code translate="no" dir="ltr">options</code> to indicate the desired batching behavior, e.g. batch the values into multiple packs so that they can better overlap with computations.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def step_fn():
  ctx = tf.distribute.get_replica_context()
  value1 = tf.identity(1.)
  value2 = tf.identity(2.)
  return ctx.all_reduce(tf.distribute.ReduceOp.SUM, [value1, value2])
strategy.experimental_local_results(strategy.run(step_fn))
([PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=2.0&gt;
}, PerReplica:{
  0: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;,
  1: &lt;tf.Tensor: shape=(), dtype=float32, numpy=4.0&gt;
}],)
</pre> <p>Note that all replicas need to participate in the all-reduce, otherwise this operation hangs. Note that if there're multiple all-reduces, they need to execute in the same order on all replicas. Dispatching all-reduce based on conditions is usually error-prone.</p> <p>This API currently can only be called in the replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>: the reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>: the batched reduce and all-reduce API in the cross-replica context.</li> <li>
<a href="strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> which <a href="../nest/flatten"><code translate="no" dir="ltr">tf.nest.flatten</code></a> accepts. The structure and the shapes of the <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> need to be same on all replicas. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A nested structure of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> with the reduced values. The structure is the same as <code translate="no" dir="ltr">value</code>. </td> </tr> 
</table> <h3 id="merge_call" data-text="merge_call"><code translate="no" dir="ltr">merge_call</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2908-L2941">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
merge_call(
    merge_fn, args=(), kwargs=None
)
</pre> <p>Merge args across replicas and run <code translate="no" dir="ltr">merge_fn</code> in a cross-replica context.</p> <p>This allows communication and coordination when there are multiple calls to the step_fn triggered by a call to <code translate="no" dir="ltr">strategy.run(step_fn, ...)</code>.</p> <p>See <a href="strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a> for an explanation.</p> <p>If not inside a distributed scope, this is equivalent to:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.get_strategy()
with cross-replica-context(strategy):
  return merge_fn(strategy, *args, **kwargs)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">merge_fn</code> </td> <td> Function that joins arguments from threads that are given as PerReplica. It accepts <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> object as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> List or tuple with positional per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword per-thread arguments for <code translate="no" dir="ltr">merge_fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The return value of <code translate="no" dir="ltr">merge_fn</code>, except for <code translate="no" dir="ltr">PerReplica</code> values which are unpacked. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/ReplicaContext" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/ReplicaContext</a>
  </p>
</div>
