<h1 class="devsite-page-title">tf.distribute.StrategyExtended</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L1972-L2580">  View source on GitHub </a> </td> </table> <p>Additional APIs for algorithms that need to be distribution-aware.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.StrategyExtended(
    container_strategy
)
</pre>  <blockquote class="note">
<strong>Note:</strong><span> For most usage of <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>, there should be no need to call these methods, since TensorFlow libraries (such as optimizers) already call these methods when needed on your behalf.</span>
</blockquote> <p>Some common use cases of functions on this page:</p> <ul> <li><em>Locality</em></li> </ul> <p><a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> can have the same <em>locality</em> as a <em>distributed variable</em>, which leads to a mirrored value residing on the same devices as the variable (as opposed to the compute devices). Such values may be passed to a call to <a href="strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> to update the value of a variable. You may use <a href="strategyextended#colocate_vars_with"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.colocate_vars_with</code></a> to give a variable the same locality as another variable. You may convert a "PerReplica" value to a variable's locality by using <a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a> or <a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>.</p> <ul> <li><em>How to update a distributed variable</em></li> </ul> <p>A distributed variable is variables created on multiple devices. As discussed in the <a href="https://www.tensorflow.org/api_docs/python/tf/distribute">glossary</a>, mirrored variable and SyncOnRead variable are two examples. The standard pattern for updating distributed variables is to:</p> <ol> <li>In your function passed to <a href="strategy#run"><code translate="no" dir="ltr">tf.distribute.Strategy.run</code></a>, compute a list of (update, variable) pairs. For example, the update might be a gradient of the loss with respect to the variable.</li> <li>Switch to cross-replica mode by calling <code translate="no" dir="ltr">tf.distribute.get_replica_context().merge_call()</code> with the updates and variables as arguments.</li> <li>Call <a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to(VariableAggregation.SUM, t, v)</code></a> (for one variable) or <a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a> (for a list of variables) to sum the updates.</li> <li>Call <a href="strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update(v)</code></a> for each variable to update its value.</li> </ol> <p>Steps 2 through 4 are done automatically by class <a href="../keras/optimizers/optimizer"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer</code></a> if you call its <a href="../keras/optimizers/optimizer#apply_gradients"><code translate="no" dir="ltr">tf.keras.optimizers.Optimizer.apply_gradients</code></a> method in a replica context.</p> <p>In fact, a higher-level solution to update a distributed variable is by calling <code translate="no" dir="ltr">assign</code> on the variable as you would do to a regular <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>. You can call the method in both <em>replica context</em> and <em>cross-replica context</em>. For a <em>mirrored variable</em>, calling <code translate="no" dir="ltr">assign</code> in <em>replica context</em> requires you to specify the <code translate="no" dir="ltr">aggregation</code> type in the variable constructor. In that case, the context switching and sync described in steps 2 through 4 are handled for you. If you call <code translate="no" dir="ltr">assign</code> on <em>mirrored variable</em> in <em>cross-replica context</em>, you can only assign a single value or assign values from another mirrored variable or a mirrored <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. For a <em>SyncOnRead variable</em>, in <em>replica context</em>, you can simply call <code translate="no" dir="ltr">assign</code> on it and no aggregation happens under the hood. In <em>cross-replica context</em>, you can only assign a single value to a SyncOnRead variable. One example case is restoring from a checkpoint: if the <code translate="no" dir="ltr">aggregation</code> type of the variable is <a href="../variableaggregation#SUM"><code translate="no" dir="ltr">tf.VariableAggregation.SUM</code></a>, it is assumed that replica values were added before checkpointing, so at the time of restoring, the value is divided by the number of replicas and then assigned to each replica; if the <code translate="no" dir="ltr">aggregation</code> type is <a href="../variableaggregation#MEAN"><code translate="no" dir="ltr">tf.VariableAggregation.MEAN</code></a>, the value is assigned to each replica directly.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">experimental_require_static_shapes</code> </td> <td> Returns <code translate="no" dir="ltr">True</code> if static shape is required; <code translate="no" dir="ltr">False</code> otherwise. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">parameter_devices</code> </td> <td> Returns the tuple of all devices used to place variables. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">worker_devices</code> </td> <td> Returns the tuple of all devices used to for compute replica execution. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="batch_reduce_to" data-text="batch_reduce_to"><code translate="no" dir="ltr">batch_reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2304-L2374">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
batch_reduce_to(
    reduce_op, value_destination_pairs, options=None
)
</pre> <p>Combine multiple <code translate="no" dir="ltr">reduce_to</code> calls into one for faster execution.</p> <p>Similar to <code translate="no" dir="ltr">reduce_to</code>, but accepts a list of (value, destinations) pairs. It's more efficient than reduce each value separately.</p> <p>This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>: the non-batch version of this API.</li> <li>
<a href="replicacontext#all_reduce"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.all_reduce</code></a>: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.</li> <li>
<a href="strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul> <p>See <code translate="no" dir="ltr">reduce_to</code> for more information.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
@tf.function
def step_fn(var):

  def merge_fn(strategy, value, var):
    # All-reduce the value. Note that `value` here is a
    # `tf.distribute.DistributedValues`.
    reduced = strategy.extended.batch_reduce_to(
        tf.distribute.ReduceOp.SUM, [(value, var)])[0]
    strategy.extended.update(var, lambda var, value: var.assign(value),
        args=(reduced,))

  value = tf.identity(1.)
  tf.distribute.get_replica_context().merge_call(merge_fn,
    args=(value, var))

def run(strategy):
  with strategy.scope():
    v = tf.Variable(0.)
    strategy.run(step_fn, args=(v,))
    return v

run(tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"]))
MirroredVariable:{
  0: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0&gt;
}
run(tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=["GPU:0", "GPU:1"], parameter_device="CPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;
run(tf.distribute.OneDeviceStrategy("GPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_destination_pairs</code> </td> <td> a sequence of (value, destinations) pairs. See <code translate="no" dir="ltr">tf.distribute.Strategy.reduce_to</code> for descriptions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of reduced values, one per pair in <code translate="no" dir="ltr">value_destination_pairs</code>. </td> </tr> 
</table> <h3 id="colocate_vars_with" data-text="colocate_vars_with"><code translate="no" dir="ltr">colocate_vars_with</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2145-L2190">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
colocate_vars_with(
    colocate_with_variable
)
</pre> <p>Scope that controls which devices variables will be created on.</p> <p>No operations should be added to the graph inside this scope, it should only be used when creating variables (some implementations work by changing variable creation, others work by using a tf.compat.v1.colocate_with() scope).</p> <p>This may only be used inside <code translate="no" dir="ltr">self.scope()</code>.</p> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">with strategy.scope():
  var1 = tf.Variable(...)
  with strategy.extended.colocate_vars_with(var1):
    # var2 and var3 will be created on the same device(s) as var1
    var2 = tf.Variable(...)
    var3 = tf.Variable(...)

  def fn(v1, v2, v3):
    # operates on v1 from var1, v2 from var2, and v3 from var3

  # `fn` runs on every device `var1` is on, `var2` and `var3` will be there
  # too.
  strategy.extended.update(var1, fn, args=(var2, var3))
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">colocate_with_variable</code> </td> <td> A variable created in this strategy's <code translate="no" dir="ltr">scope()</code>. Variables created while in the returned context manager will be on the same set of devices as <code translate="no" dir="ltr">colocate_with_variable</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A context manager. </td> </tr> 
</table> <h3 id="reduce_to" data-text="reduce_to"><code translate="no" dir="ltr">reduce_to</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2216-L2299">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reduce_to(
    reduce_op, value, destinations, options=None
)
</pre> <p>Combine (via e.g. sum or mean) values across replicas.</p> <p><code translate="no" dir="ltr">reduce_to</code> aggregates <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> and distributed variables. It supports both dense values and <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>.</p> <p>This API currently can only be called in cross-replica context. Other variants to reduce values across replicas are:</p> <ul> <li>
<a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>: the batch version of this API.</li> <li>
<a href="replicacontext#all_reduce"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.all_reduce</code></a>: the counterpart of this API in replica context. It supports both batched and non-batched all-reduce.</li> <li>
<a href="strategy#reduce"><code translate="no" dir="ltr">tf.distribute.Strategy.reduce</code></a>: a more convenient method to reduce to the host in cross-replica context.</li> </ul> <p><code translate="no" dir="ltr">destinations</code> specifies where to reduce the value to, e.g. "GPU:0". You can also pass in a <code translate="no" dir="ltr">Tensor</code>, and the destinations will be the device of that tensor. For all-reduce, pass the same to <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">destinations</code>.</p> <p>It can be used in <a href="replicacontext#merge_call"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.merge_call</code></a> to write code that works for all <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a>.</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
@tf.function
def step_fn(var):

  def merge_fn(strategy, value, var):
    # All-reduce the value. Note that `value` here is a
    # `tf.distribute.DistributedValues`.
    reduced = strategy.extended.reduce_to(tf.distribute.ReduceOp.SUM,
        value, destinations=var)
    strategy.extended.update(var, lambda var, value: var.assign(value),
        args=(reduced,))

  value = tf.identity(1.)
  tf.distribute.get_replica_context().merge_call(merge_fn,
    args=(value, var))

def run(strategy):
  with strategy.scope():
    v = tf.Variable(0.)
    strategy.run(step_fn, args=(v,))
    return v

run(tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"]))
MirroredVariable:{
  0: &lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;,
  1: &lt;tf.Variable 'Variable/replica_1:0' shape=() dtype=float32, numpy=2.0&gt;
}
run(tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=["GPU:0", "GPU:1"], parameter_device="CPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=2.0&gt;
run(tf.distribute.OneDeviceStrategy("GPU:0"))
&lt;tf.Variable 'Variable:0' shape=() dtype=float32, numpy=1.0&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. Allows using string representation of the enum such as "SUM", "MEAN". </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, or a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> like object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">destinations</code>. Note that if it's a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, the value is reduced to the devices of that variable, and this method doesn't update the variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. Options to perform collective operations. This overrides the default options if the <a href="strategy"><code translate="no" dir="ltr">tf.distribute.Strategy</code></a> takes one in the constructor. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details of the options. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tensor or value reduced to <code translate="no" dir="ltr">destinations</code>. </td> </tr> 
</table> <h3 id="update" data-text="update"><code translate="no" dir="ltr">update</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2426-L2494">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
update(
    var, fn, args=(), kwargs=None, group=True
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> to update <code translate="no" dir="ltr">var</code> using inputs mirrored to the same devices.</p> <p><a href="strategyextended#update"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.update</code></a> takes a distributed variable <code translate="no" dir="ltr">var</code> to be updated, an update function <code translate="no" dir="ltr">fn</code>, and <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code> for <code translate="no" dir="ltr">fn</code>. It applies <code translate="no" dir="ltr">fn</code> to each component variable of <code translate="no" dir="ltr">var</code> and passes corresponding values from <code translate="no" dir="ltr">args</code> and <code translate="no" dir="ltr">kwargs</code>. Neither <code translate="no" dir="ltr">args</code> nor <code translate="no" dir="ltr">kwargs</code> may contain per-replica values. If they contain mirrored values, they will be unwrapped before calling <code translate="no" dir="ltr">fn</code>. For example, <code translate="no" dir="ltr">fn</code> can be <code translate="no" dir="ltr">assign_add</code> and <code translate="no" dir="ltr">args</code> can be a mirrored DistributedValues where each component contains the value to be added to this mirrored variable <code translate="no" dir="ltr">var</code>. Calling <code translate="no" dir="ltr">update</code> will call <code translate="no" dir="ltr">assign_add</code> on each component variable of <code translate="no" dir="ltr">var</code> with the corresponding tensor value on that device.</p> <h4 id="example_usage_2" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1']) # With 2
devices
with strategy.scope():
  v = tf.Variable(5.0, aggregation=tf.VariableAggregation.SUM)
def update_fn(v):
  return v.assign(1.0)
result = strategy.extended.update(v, update_fn)
# result is
# Mirrored:{
#  0: tf.Tensor(1.0, shape=(), dtype=float32),
#  1: tf.Tensor(1.0, shape=(), dtype=float32)
# }
</pre> <p>If <code translate="no" dir="ltr">var</code> is mirrored across multiple devices, then this method implements logic as following:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">results = {}
for device, v in var:
  with tf.device(device):
    # args and kwargs will be unwrapped if they are mirrored.
    results[device] = fn(v, *args, **kwargs)
return merged(results)
</pre> <p>Otherwise, this method returns <code translate="no" dir="ltr">fn(var, *args, **kwargs)</code> colocated with <code translate="no" dir="ltr">var</code>.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">var</code> </td> <td> Variable, possibly mirrored to multiple devices, to operate on. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> Function to call. Should take the variable as the first argument. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> Tuple or list. Additional positional arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> Dict with keyword arguments to pass to <code translate="no" dir="ltr">fn()</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">group</code> </td> <td> Boolean. Defaults to True. If False, the return value will be unwrapped. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> By default, the merged return value of <code translate="no" dir="ltr">fn</code> across all replicas. The merged result has dependencies to make sure that if it is evaluated at all, the side effects (updates) will happen on every replica. If instead "group=False" is specified, this function will return a nest of lists where each list has an element per replica, and the caller is responsible for ensuring all elements are executed. </td> </tr> 
</table> <h3 id="value_container" data-text="value_container"><code translate="no" dir="ltr">value_container</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2502-L2515">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
value_container(
    value
)
</pre> <p>Returns the container that this per-replica <code translate="no" dir="ltr">value</code> belongs to.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A value returned by <code translate="no" dir="ltr">run()</code> or a variable created in <code translate="no" dir="ltr">scope()</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A container that <code translate="no" dir="ltr">value</code> belongs to. If value does not belong to any container (including the case of container having been destroyed), returns the value itself. <code translate="no" dir="ltr">value in experimental_local_results(value_container(value))</code> will always be true. </td> </tr> 
</table> <h3 id="variable_created_in_scope" data-text="variable_created_in_scope"><code translate="no" dir="ltr">variable_created_in_scope</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L2119-L2143">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
variable_created_in_scope(
    v
)
</pre> <p>Tests whether <code translate="no" dir="ltr">v</code> was created while this strategy scope was active.</p> <p>Variables created inside the strategy scope are "owned" by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
  v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
True
</pre> <p>Variables created outside the strategy are not owned by it:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy()
v = tf.Variable(1.)
strategy.extended.variable_created_in_scope(v)
False
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">v</code> </td> <td> A <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a> instance. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> True if <code translate="no" dir="ltr">v</code> was created inside the scope, False if not. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/StrategyExtended" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/StrategyExtended</a>
  </p>
</div>
