<h1 class="devsite-page-title">tf.distribute.HierarchicalCopyAllReduce</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/cross_device_ops.py#L938-L977">  View source on GitHub </a> </td> </table> <p>Hierarchical copy all-reduce implementation of CrossDeviceOps.</p> <p>Inherits From: <a href="crossdeviceops"><code translate="no" dir="ltr">CrossDeviceOps</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/distribute/HierarchicalCopyAllReduce"><code translate="no" dir="ltr">tf.compat.v1.distribute.HierarchicalCopyAllReduce</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.HierarchicalCopyAllReduce(
    num_packs=1
)
</pre>  <p>It reduces to one GPU along edges in some hierarchy and broadcasts back to each GPU along the same path. For the batch API, tensors will be repacked or aggregated for more efficient cross-device transportation.</p> <p>This is a reduction created for Nvidia DGX-1 which assumes GPUs connects like that on DGX-1 machine. If you have different GPU inter-connections, it is likely that it would be slower than <a href="reductiontoonedevice"><code translate="no" dir="ltr">tf.distribute.ReductionToOneDevice</code></a>.</p> <p>For reduces that are not all-reduce, it falls back to <a href="reductiontoonedevice"><code translate="no" dir="ltr">tf.distribute.ReductionToOneDevice</code></a>.</p> <p>Here is how you can use <code translate="no" dir="ltr">HierarchicalCopyAllReduce</code> in <a href="mirroredstrategy"><code translate="no" dir="ltr">tf.distribute.MirroredStrategy</code></a>:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.MirroredStrategy(
  cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">num_packs</code> </td> <td> a non-negative integer. The number of packs to split values into. If zero, no packing will be done. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> <tr class="alt"> <td colspan="2"> ValueError if <code translate="no" dir="ltr">num_packs</code> is negative. </td> </tr> 
</table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="batch_reduce" data-text="batch_reduce"><code translate="no" dir="ltr">batch_reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/cross_device_ops.py#L379-L426">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
batch_reduce(
    reduce_op, value_destination_pairs, options=None
)
</pre> <p>Reduce values to destinations in batches.</p> <p>See <a href="strategyextended#batch_reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.batch_reduce_to</code></a>. This can only be called in the cross-replica context.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> specifying how values should be combined. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value_destination_pairs</code> </td> <td> a sequence of (value, destinations) pairs. See <a href="crossdeviceops#reduce"><code translate="no" dir="ltr">tf.distribute.CrossDeviceOps.reduce</code></a> for descriptions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A list of <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> or <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, one per pair in <code translate="no" dir="ltr">value_destination_pairs</code>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">value_destination_pairs</code> is not an iterable of tuples of <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> and destinations. </td> </tr> </table> <h3 id="broadcast" data-text="broadcast"><code translate="no" dir="ltr">broadcast</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/cross_device_ops.py#L428-L445">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
broadcast(
    tensor, destinations
)
</pre> <p>Broadcast <code translate="no" dir="ltr">tensor</code> to <code translate="no" dir="ltr">destinations</code>.</p> <p>This can only be called in the cross-replica context.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">tensor</code> </td> <td> a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> like object. The value to broadcast. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> alike object, or a device string. It specifies the devices to broadcast to. Note that if it's a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, the value is broadcasted to the devices of that variable, this method doesn't update the variable. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> or <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. </td> </tr> 
</table> <h3 id="reduce" data-text="reduce"><code translate="no" dir="ltr">reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/cross_device_ops.py#L253-L299">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reduce(
    reduce_op, per_replica_value, destinations, options=None
)
</pre> <p>Reduce <code translate="no" dir="ltr">per_replica_value</code> to <code translate="no" dir="ltr">destinations</code>.</p> <p>See <a href="strategyextended#reduce_to"><code translate="no" dir="ltr">tf.distribute.StrategyExtended.reduce_to</code></a>. This can only be called in the cross-replica context.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> a <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> specifying how values should be combined. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">per_replica_value</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, or a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> like object. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">destinations</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>, a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, a <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> alike object, or a device string. It specifies the devices to reduce to. To perform an all-reduce, pass the same to <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">destinations</code>. Note that if it's a <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a>, the value is reduced to the devices of that variable, and this method doesn't update the variable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> a <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a>. See <a href="experimental/communicationoptions"><code translate="no" dir="ltr">tf.distribute.experimental.CommunicationOptions</code></a> for details. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a> or <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if per_replica_value can't be converted to a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> or if destinations is not a string, <a href="../variable"><code translate="no" dir="ltr">tf.Variable</code></a> or <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a>. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/HierarchicalCopyAllReduce" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/HierarchicalCopyAllReduce</a>
  </p>
</div>
