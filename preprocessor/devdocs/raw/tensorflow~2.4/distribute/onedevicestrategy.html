<h1 class="devsite-page-title">tf.distribute.OneDeviceStrategy</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L40-L237">  View source on GitHub </a> </td> </table> <p>A distribution strategy for running on a single device.</p> <p>Inherits From: <a href="strategy"><code translate="no" dir="ltr">Strategy</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.distribute.OneDeviceStrategy(
    device
)
</pre>  <p>Using this strategy will place any variables created in its scope on the specified device. Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via <code translate="no" dir="ltr">strategy.run</code> will also be placed on the specified device as well.</p> <p>Typical usage of this strategy could be testing your code with the tf.distribute.Strategy API before switching to other strategies which actually distribute to multiple devices/machines.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.OneDeviceStrategy(device="/gpu:0")

with strategy.scope():
  v = tf.Variable(1.0)
  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.run(step_fn, args=(i,))
print(result)  # 90
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">device</code> </td> <td> Device string identifier for the device on which the variables should be placed. See class docs for more details on how the device is used. Examples: "/cpu:0", "/gpu:0", "/device:CPU:0", "/device:GPU:0" </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">cluster_resolver</code> </td> <td> Returns the cluster resolver associated with this strategy. <p>In general, when using a multi-worker <a href="../distribute"><code translate="no" dir="ltr">tf.distribute</code></a> strategy such as <a href="experimental/multiworkermirroredstrategy"><code translate="no" dir="ltr">tf.distribute.experimental.MultiWorkerMirroredStrategy</code></a> or <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy()</code></a>, there is a <a href="cluster_resolver/clusterresolver"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a> associated with the strategy used, and such an instance is returned by this property.</p> <p>Strategies that intend to have an associated <a href="cluster_resolver/clusterresolver"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a> must set the relevant attribute, or override this property; otherwise, <code translate="no" dir="ltr">None</code> is returned by default. Those strategies should also provide information regarding what is returned by this property.</p> <p>Single-worker strategies usually do not have a <a href="cluster_resolver/clusterresolver"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a>, and in those cases this property will return <code translate="no" dir="ltr">None</code>.</p> <p>The <a href="cluster_resolver/clusterresolver"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a> may be useful when the user needs to access information such as the cluster spec, task type or task id. For example,</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">
os.environ['TF_CONFIG'] = json.dumps({
'cluster': {
'worker': ["localhost:12345", "localhost:23456"],
'ps': ["localhost:34567"]
},
'task': {'type': 'worker', 'index': 0}
})

# This implicitly uses TF_CONFIG for the cluster and current task info.
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

...

if strategy.cluster_resolver.task_type == 'worker':
# Perform something that's only applicable on workers. Since we set this
# as a worker above, this block will run on this particular instance.
elif strategy.cluster_resolver.task_type == 'ps':
# Perform something that's only applicable on parameter servers. Since we
# set this as a worker above, this block will not run on this particular
# instance.
</pre> <p>For more information, please see <a href="cluster_resolver/clusterresolver"><code translate="no" dir="ltr">tf.distribute.cluster_resolver.ClusterResolver</code></a>'s API docstring. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">extended</code> </td> <td> <a href="strategyextended"><code translate="no" dir="ltr">tf.distribute.StrategyExtended</code></a> with additional methods. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_replicas_in_sync</code> </td> <td> Returns number of replicas over which gradients are aggregated. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="distribute_datasets_from_function" data-text="distribute_datasets_from_function"><code translate="no" dir="ltr">distribute_datasets_from_function</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L112-L154">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
distribute_datasets_from_function(
    dataset_fn, options=None
)
</pre> <p>Distributes <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> instances created by calls to <code translate="no" dir="ltr">dataset_fn</code>.</p> <p><code translate="no" dir="ltr">dataset_fn</code> will be called once for each worker in the strategy. In this case, we only have one worker and one device so <code translate="no" dir="ltr">dataset_fn</code> is called once.</p> <p>The <code translate="no" dir="ltr">dataset_fn</code> should take an <a href="inputcontext"><code translate="no" dir="ltr">tf.distribute.InputContext</code></a> instance where information about batching and input replication can be accessed:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">def dataset_fn(input_context):
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  d = tf.data.Dataset.from_tensors([[1.]]).repeat().batch(batch_size)
  return d.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)

inputs = strategy.distribute_datasets_from_function(dataset_fn)

for batch in inputs:
  replica_results = strategy.run(replica_fn, args=(batch,))
</pre>
<aside class="key-point"><strong>Key Point:</strong><span> The <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> returned by <code translate="no" dir="ltr">dataset_fn</code> should have a per-replica batch size, unlike <code translate="no" dir="ltr">experimental_distribute_dataset</code>, which uses the global batch size. This may be computed using <code translate="no" dir="ltr">input_context.get_per_replica_batch_size</code>.</span></aside>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">dataset_fn</code> </td> <td> A function taking a <a href="inputcontext"><code translate="no" dir="ltr">tf.distribute.InputContext</code></a> instance and returning a <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> <a href="inputoptions"><code translate="no" dir="ltr">tf.distribute.InputOptions</code></a> used to control options on how this dataset is distributed. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A "distributed <code translate="no" dir="ltr">Dataset</code>", which the caller can iterate over like regular datasets. </td> </tr> 
</table> <h3 id="experimental_distribute_dataset" data-text="experimental_distribute_dataset"><code translate="no" dir="ltr">experimental_distribute_dataset</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L83-L110">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_distribute_dataset(
    dataset, options=None
)
</pre> <p>Distributes a tf.data.Dataset instance provided via dataset.</p> <p>In this case, there is only one device, so this is only a thin wrapper around the input dataset. It will, however, prefetch the input data to the specified device. The returned distributed dataset can be iterated over similar to how regular datasets can.</p> <blockquote class="note">
<strong>Note:</strong><span> Currently, the user cannot add any more transformations to a distributed dataset.</span>
</blockquote> <h4 id="example" data-text="Example:">Example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.OneDeviceStrategy()
dataset = tf.data.Dataset.range(10).batch(2)
dist_dataset = strategy.experimental_distribute_dataset(dataset)
for x in dist_dataset:
  print(x)  # [0, 1], [2, 3],...
</pre> <p>Args: dataset: <a href="../data/dataset"><code translate="no" dir="ltr">tf.data.Dataset</code></a> to be prefetched to device. options: <a href="inputoptions"><code translate="no" dir="ltr">tf.distribute.InputOptions</code></a> used to control options on how this dataset is distributed. Returns: A "distributed <code translate="no" dir="ltr">Dataset</code>" that the caller can iterate over.</p> <h3 id="experimental_distribute_values_from_function" data-text="experimental_distribute_values_from_function"><code translate="no" dir="ltr">experimental_distribute_values_from_function</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L1616-L1690">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_distribute_values_from_function(
    value_fn
)
</pre> <p>Generates <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> from <code translate="no" dir="ltr">value_fn</code>.</p> <p>This function is to generate <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> to pass into <code translate="no" dir="ltr">run</code>, <code translate="no" dir="ltr">reduce</code>, or other methods that take distributed values when not using datasets.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value_fn</code> </td> <td> The function to run to generate values. It is called for each replica with <code translate="no" dir="ltr">tf.distribute.ValueContext</code> as the sole argument. It must return a Tensor or a type that can be converted to a Tensor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> containing a value for each replica. </td> </tr> 
</table> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <ol> <li>Return constant value per replica:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def value_fn(ctx):
  return tf.constant(1.)
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result
(&lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;,
 &lt;tf.Tensor: shape=(), dtype=float32, numpy=1.0&gt;)
</pre> <ol> <li>Distribute values in array based on replica_id:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
array_value = np.array([3., 2., 1.])
def value_fn(ctx):
  return array_value[ctx.replica_id_in_sync_group]
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result
(3.0, 2.0)
</pre> <ol> <li>Specify values using num_replicas_in_sync:</li> </ol> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
def value_fn(ctx):
  return ctx.num_replicas_in_sync
distributed_values = (
     strategy.experimental_distribute_values_from_function(
       value_fn))
local_result = strategy.experimental_local_results(distributed_values)
local_result
(2, 2)
</pre> <ol> <li>Place values on devices and distribute:</li> </ol> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">strategy = tf.distribute.TPUStrategy()
worker_devices = strategy.extended.worker_devices
multiple_values = []
for i in range(strategy.num_replicas_in_sync):
  with tf.device(worker_devices[i]):
    multiple_values.append(tf.constant(1.0))

def value_fn(ctx):
  return multiple_values[ctx.replica_id_in_sync_group]

distributed_values = strategy.
  experimental_distribute_values_from_function(
  value_fn)
</pre> <h3 id="experimental_local_results" data-text="experimental_local_results"><code translate="no" dir="ltr">experimental_local_results</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L156-L170">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
experimental_local_results(
    value
)
</pre> <p>Returns the list of all local per-replica values contained in <code translate="no" dir="ltr">value</code>.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, the <code translate="no" dir="ltr">value</code> is always expected to be a single value, so the result is just the value in a tuple.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A value returned by <code translate="no" dir="ltr">experimental_run()</code>, <code translate="no" dir="ltr">run()</code>, <code translate="no" dir="ltr">extended.call_for_each_replica()</code>, or a variable created in <code translate="no" dir="ltr">scope</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A tuple of values contained in <code translate="no" dir="ltr">value</code>. If <code translate="no" dir="ltr">value</code> represents a single value, this returns <code translate="no" dir="ltr">(value,).</code> </td> </tr> 
</table> <h3 id="gather" data-text="gather"><code translate="no" dir="ltr">gather</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/distribute_lib.py#L1692-L1796">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
gather(
    value, axis
)
</pre> <p>Gather <code translate="no" dir="ltr">value</code> across replicas along <code translate="no" dir="ltr">axis</code> to the current device.</p> <p>Given a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> or <a href="../tensor"><code translate="no" dir="ltr">tf.Tensor</code></a>-like object <code translate="no" dir="ltr">value</code>, this API gathers and concatenates <code translate="no" dir="ltr">value</code> across replicas along the <code translate="no" dir="ltr">axis</code>-th dimension. The result is copied to the "current" device</p> <ul> <li>which would typically be the CPU of the worker on which the program is running. For <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, it is the first TPU host. For multi-client <code translate="no" dir="ltr">MultiWorkerMirroredStrategy</code>, this is CPU of each worker.</li> </ul> <p>This API can only be called in the cross-replica context. For a counterpart in the replica context, see <a href="replicacontext#all_gather"><code translate="no" dir="ltr">tf.distribute.ReplicaContext.all_gather</code></a>.</p> <blockquote class="note">
<strong>Note:</strong><span> For all strategies except <a href="tpustrategy"><code translate="no" dir="ltr">tf.distribute.TPUStrategy</code></a>, the input <code translate="no" dir="ltr">value</code> on different replicas must have the same rank, and their shapes must be the same in all dimensions except the <code translate="no" dir="ltr">axis</code>-th dimension. In other words, their shapes cannot be different in a dimension <code translate="no" dir="ltr">d</code> where <code translate="no" dir="ltr">d</code> does not equal to the <code translate="no" dir="ltr">axis</code> argument. For example, given a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> with component tensors of shape <code translate="no" dir="ltr">(1, 2, 3)</code> and <code translate="no" dir="ltr">(1, 3, 3)</code> on two replicas, you can call <code translate="no" dir="ltr">gather(..., axis=1, ...)</code> on it, but not <code translate="no" dir="ltr">gather(..., axis=0, ...)</code> or <code translate="no" dir="ltr">gather(..., axis=2, ...)</code>. However, for <a href="tpustrategy#gather"><code translate="no" dir="ltr">tf.distribute.TPUStrategy.gather</code></a>, all tensors must have exactly the same rank and same shape.</span>
</blockquote>
<blockquote class="note">
<strong>Note:</strong><span> Given a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> <code translate="no" dir="ltr">value</code>, its component tensors must have a non-zero rank. Otherwise, consider using <a href="../expand_dims"><code translate="no" dir="ltr">tf.expand_dims</code></a> before gathering them.</span>
</blockquote> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1"])
# A DistributedValues with component tensor of shape (2, 1) on each replica
distributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(tf.constant([[1], [2]])))
@tf.function
def run():
  return strategy.gather(distributed_values, axis=0)
run()
&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=
array([[1],
       [2],
       [1],
       [2]], dtype=int32)&gt;
</pre> <p>Consider the following example for more combinations:</p> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
strategy = tf.distribute.MirroredStrategy(["GPU:0", "GPU:1", "GPU:2", "GPU:3"])
single_tensor = tf.reshape(tf.range(6), shape=(1,2,3))
distributed_values = strategy.experimental_distribute_values_from_function(lambda _: tf.identity(single_tensor))
@tf.function
def run(axis):
  return strategy.gather(distributed_values, axis=axis)
axis=0
run(axis)
&lt;tf.Tensor: shape=(4, 2, 3), dtype=int32, numpy=
array([[[0, 1, 2],
        [3, 4, 5]],
       [[0, 1, 2],
        [3, 4, 5]],
       [[0, 1, 2],
        [3, 4, 5]],
       [[0, 1, 2],
        [3, 4, 5]]], dtype=int32)&gt;
axis=1
run(axis)
&lt;tf.Tensor: shape=(1, 8, 3), dtype=int32, numpy=
array([[[0, 1, 2],
        [3, 4, 5],
        [0, 1, 2],
        [3, 4, 5],
        [0, 1, 2],
        [3, 4, 5],
        [0, 1, 2],
        [3, 4, 5]]], dtype=int32)&gt;
axis=2
run(axis)
&lt;tf.Tensor: shape=(1, 2, 12), dtype=int32, numpy=
array([[[0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2],
        [3, 4, 5, 3, 4, 5, 3, 4, 5, 3, 4, 5]]], dtype=int32)&gt;
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> a <a href="distributedvalues"><code translate="no" dir="ltr">tf.distribute.DistributedValues</code></a> instance, e.g. returned by <a href="mirroredstrategy#run"><code translate="no" dir="ltr">Strategy.run</code></a>, to be combined into a single tensor. It can also be a regular tensor when used with <a href="onedevicestrategy"><code translate="no" dir="ltr">tf.distribute.OneDeviceStrategy</code></a> or the default strategy. The tensors that constitute the DistributedValues can only be dense tensors with non-zero rank, NOT a <a href="../indexedslices"><code translate="no" dir="ltr">tf.IndexedSlices</code></a>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> 0-D int32 Tensor. Dimension along which to gather. Must be in the range [0, rank(value)). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code> that's the concatenation of <code translate="no" dir="ltr">value</code> across replicas along <code translate="no" dir="ltr">axis</code> dimension. </td> </tr> 
</table> <h3 id="reduce" data-text="reduce"><code translate="no" dir="ltr">reduce</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L190-L221">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
reduce(
    reduce_op, value, axis
)
</pre> <p>Reduce <code translate="no" dir="ltr">value</code> across replicas.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, there is only one replica, so if axis=None, value is simply returned. If axis is specified as something other than None, such as axis=0, value is reduced along that axis and returned.</p> <h4 id="example_2" data-text="Example:">Example:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">t = tf.range(10)

result = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=None).numpy()
# result: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]

result = strategy.reduce(tf.distribute.ReduceOp.SUM, t, axis=0).numpy()
# result: 45
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">reduce_op</code> </td> <td> A <a href="reduceop"><code translate="no" dir="ltr">tf.distribute.ReduceOp</code></a> value specifying how values should be combined. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A "per replica" value, e.g. returned by <code translate="no" dir="ltr">run</code> to be combined into a single tensor. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">axis</code> </td> <td> Specifies the dimension to reduce along within each replica's tensor. Should typically be set to the batch dimension, or <code translate="no" dir="ltr">None</code> to only reduce across replicas (e.g. if the tensor has no batch dimension). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code>. </td> </tr> 
</table> <h3 id="run" data-text="run"><code translate="no" dir="ltr">run</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L172-L188">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
run(
    fn, args=(), kwargs=None, options=None
)
</pre> <p>Run <code translate="no" dir="ltr">fn</code> on each replica, with the given arguments.</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, <code translate="no" dir="ltr">fn</code> is simply called within a device scope for the given device, with the provided arguments.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">fn</code> </td> <td> The function to run. The output must be a <a href="../nest"><code translate="no" dir="ltr">tf.nest</code></a> of <code translate="no" dir="ltr">Tensor</code>s. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">args</code> </td> <td> (Optional) Positional arguments to <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kwargs</code> </td> <td> (Optional) Keyword arguments to <code translate="no" dir="ltr">fn</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">options</code> </td> <td> (Optional) An instance of <a href="runoptions"><code translate="no" dir="ltr">tf.distribute.RunOptions</code></a> specifying the options to run <code translate="no" dir="ltr">fn</code>. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> Return value from running <code translate="no" dir="ltr">fn</code>. </td> </tr> 
</table> <h3 id="scope" data-text="scope"><code translate="no" dir="ltr">scope</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/distribute/one_device_strategy.py#L223-L237">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
scope()
</pre> <p>Returns a context manager selecting this Strategy as current.</p> <p>Inside a <code translate="no" dir="ltr">with strategy.scope():</code> code block, this thread will use a variable creator set by <code translate="no" dir="ltr">strategy</code>, and will enter its "cross-replica context".</p> <p>In <code translate="no" dir="ltr">OneDeviceStrategy</code>, all variables created inside <code translate="no" dir="ltr">strategy.scope()</code> will be on <code translate="no" dir="ltr">device</code> specified at strategy construction time. See example in the docs for this class.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A context manager to use for creating variables with this strategy. </td> </tr> 
</table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/OneDeviceStrategy" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/distribute/OneDeviceStrategy</a>
  </p>
</div>
