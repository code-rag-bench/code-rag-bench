<h1 class="devsite-page-title">tf.keras.layers.experimental.RandomFourierFeatures</h1>       <p>Layer that projects its inputs into a random feature space.</p> <p>Inherits From: <a href="../layer"><code translate="no" dir="ltr">Layer</code></a>, <a href="../../../module"><code translate="no" dir="ltr">Module</code></a></p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/RandomFourierFeatures"><code translate="no" dir="ltr">tf.compat.v1.keras.layers.experimental.RandomFourierFeatures</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.layers.experimental.RandomFourierFeatures(
    output_dim, kernel_initializer='gaussian', scale=None,
    trainable=False, name=None, **kwargs
)
</pre>  <p>This layer implements a mapping from input space to a space with <code translate="no" dir="ltr">output_dim</code> dimensions, which approximates shift-invariant kernels. A kernel function <code translate="no" dir="ltr">K(x, y)</code> is shift-invariant if <code translate="no" dir="ltr">K(x, y) == k(x - y)</code> for some function <code translate="no" dir="ltr">k</code>. Many popular Radial Basis Functions (RBF), including Gaussian and Laplacian kernels, are shift-invariant.</p> <p>The implementation of this layer is based on the following paper: <a href="https://people.eecs.berkeley.edu/%7Ebrecht/papers/07.rah.rec.nips.pdf">"Random Features for Large-Scale Kernel Machines"</a> by Ali Rahimi and Ben Recht.</p> <p>The distribution from which the parameters of the random features map (layer) are sampled determines which shift-invariant kernel the layer approximates (see paper for more details). You can use the distribution of your choice. The layer supports out-of-the-box approximation sof the following two RBF kernels:</p> <ul> <li>Gaussian: <code translate="no" dir="ltr">K(x, y) == exp(- square(x - y) / (2 * square(scale)))</code>
</li> <li>Laplacian: <code translate="no" dir="ltr">K(x, y) = exp(-abs(x - y) / scale))</code>
</li> </ul> <blockquote class="note">
<strong>Note:</strong><span> Unlike what is described in the paper and unlike what is used in the Scikit-Learn implementation, the output of this layer does not apply the <code translate="no" dir="ltr">sqrt(2 / D)</code> normalization factor.</span>
</blockquote> <p><strong>Usage:</strong> Typically, this layer is used to "kernelize" linear models by applying a non-linear transformation (this layer) to the input features and then training a linear model on top of the transformed features. Depending on the loss function of the linear model, the composition of this layer and the linear model results to models that are equivalent (up to approximation) to kernel SVMs (for hinge loss), kernel logistic regression (for logistic loss), kernel linear regression (for squared loss), etc.</p> <h4 id="examples" data-text="Examples:">Examples:</h4> <p>A kernel multinomial logistic regression model with Gaussian kernel for MNIST:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)
</pre> <p>A quasi-SVM classifier for MNIST:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10),
])
model.compile(
    optimizer='adam',
    loss='hinge',
    metrics=['categorical_accuracy']
)
</pre> <p>To use another kernel, just replace the layer creation line with:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">random_features_layer = RandomFourierFeatures(
    output_dim=500,
    kernel_initializer=&lt;my_initializer&gt;,
    scale=...,
    ...)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Arguments</th></tr> 
<tr> <td> <code translate="no" dir="ltr">output_dim</code> </td> <td> Positive integer, the dimension of the layer's output, i.e., the number of random features used to approximate the kernel. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">kernel_initializer</code> </td> <td> Determines the distribution of the parameters of the random features map (and therefore the kernel approximated by the layer). It can be either a string identifier or a Keras <code translate="no" dir="ltr">Initializer</code> instance. Currently only 'gaussian' and 'laplacian' are supported string identifiers (case insensitive). Note that the kernel matrix is not trainable. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">scale</code> </td> <td> For Gaussian and Laplacian kernels, this corresponds to a scaling factor of the corresponding kernel approximated by the layer (see concrete definitions above). When provided, it should be a positive float. If None, a default value is used: if the kernel initializer is set to "gaussian", <code translate="no" dir="ltr">scale</code> defaults to <code translate="no" dir="ltr">sqrt(input_dim / 2)</code>, otherwise, it defaults to 1.0. Both the approximation error of the kernel and the classification quality are sensitive to this parameter. If <code translate="no" dir="ltr">trainable</code> is set to <code translate="no" dir="ltr">True</code>, this parameter is learned end-to-end during training and the provided value serves as the initial value. <strong>Note:</strong> When features from this layer are fed to a linear model, by making <code translate="no" dir="ltr">scale</code> trainable, the resulting optimization problem is no longer convex (even if the loss function used by the linear model is convex). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">trainable</code> </td> <td> Whether the scaling parameter of the layer should be trainable. Defaults to <code translate="no" dir="ltr">False</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> String, name to use for this layer. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/experimental/RandomFourierFeatures" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/experimental/RandomFourierFeatures</a>
  </p>
</div>
