<h1 class="devsite-page-title">tf.keras.mixed_precision.experimental.Policy</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/policy.py#L322-L410">  View source on GitHub </a> </td> </table> <p>A deprecated dtype policy for a Keras layer.</p> <p>Inherits From: <a href="../policy"><code translate="no" dir="ltr">Policy</code></a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.keras.mixed_precision.experimental.Policy(
    name, loss_scale='auto'
)
</pre>  <aside class="warning"><strong>Warning:</strong><span> This class is now deprecated and will be removed soon. Please use the non-experimental class <a href="../policy"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a> instead.</span></aside> <p>The difference between this class and the non-experimental class is that this class has a <code translate="no" dir="ltr">loss_scale</code> field and the non-experimental class does not. The loss scale is only used by <a href="../../model#compile"><code translate="no" dir="ltr">tf.keras.Model.compile</code></a>, which automatically wraps the optimizer with a <code translate="no" dir="ltr">LossScaleOptimizer</code> if the optimzier is not already a <code translate="no" dir="ltr">LossScaleOptimizer</code>. For the non-experimental Policy class, <a href="../../model#compile"><code translate="no" dir="ltr">Model.compile</code></a> instead wraps the optimizer with a <code translate="no" dir="ltr">LossScaleOptimizer</code> if <a href="../policy#name"><code translate="no" dir="ltr">Policy.name</code></a> is "mixed_float16".</p> <p>When deserializing objects with an experimental policy using functions like <a href="../../utils/deserialize_keras_object"><code translate="no" dir="ltr">tf.keras.utils.deserialize_keras_object</code></a>, the policy will be deserialized as the non-experimental <a href="../policy"><code translate="no" dir="ltr">tf.keras.mixed_precision.Policy</code></a>, and the loss scale will silently be dropped. This is so that SavedModels that are generated with an expeirmental policy can be restored after the experimental policy is removed.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A string. Can be one of the following values: <ul> <li>Any dtype name, such as 'float32' or 'float64'. Both the variable and compute dtypes will be that dtype.</li> <li>'mixed_float16' or 'mixed_bfloat16': The compute dtype is float16 or bfloat16, while the variable dtype is float32. With 'mixed_float16', a dynamic loss scale is used. These policies are used for mixed precision training. </li>
</ul>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale</code> </td> <td> A <a href="../../../mixed_precision/experimental/lossscale"><code translate="no" dir="ltr">tf.compat.v1.mixed_precision.LossScale</code></a>, an int (which uses a <code translate="no" dir="ltr">FixedLossScale</code>), the string "dynamic" (which uses a <code translate="no" dir="ltr">DynamicLossScale</code>), or None (which uses no loss scale). Defaults to <code translate="no" dir="ltr">"auto"</code>. In the <code translate="no" dir="ltr">"auto"</code> case: 1) if <code translate="no" dir="ltr">name</code> is <code translate="no" dir="ltr">"mixed_float16"</code>, then use <code translate="no" dir="ltr">loss_scale="dynamic"</code>. 2) otherwise, do not use a loss scale. Only <a href="../../model"><code translate="no" dir="ltr">tf.keras.Model</code></a>s, not layers, use the loss scale, and it is only used during <a href="../../model#fit"><code translate="no" dir="ltr">Model.fit</code></a>, <a href="../../model#train_on_batch"><code translate="no" dir="ltr">Model.train_on_batch</code></a>, and other similar methods. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">compute_dtype</code> </td> <td> The compute dtype of this policy. <p>This is the dtype layers will do their computations in. Typically layers output tensors with the compute dtype as well.</p> <p>Note that even if the compute dtype is float16 or bfloat16, hardware devices may not do individual adds, multiplies, and other fundamental operations in float16 or bfloat16, but instead may do some of them in float32 for numeric stability. The compute dtype is the dtype of the inputs and outputs of the TensorFlow ops that the layer executes. Internally, many TensorFlow ops will do certain internal calculations in float32 or some other device-internal intermediate format with higher precision than float16/bfloat16, to increase numeric stability.</p> <p>For example, a <a href="../../layers/dense"><code translate="no" dir="ltr">tf.keras.layers.Dense</code></a> layer, when run on a GPU with a float16 compute dtype, will pass float16 inputs to <a href="../../../linalg/matmul"><code translate="no" dir="ltr">tf.linalg.matmul</code></a>. But, <a href="../../../linalg/matmul"><code translate="no" dir="ltr">tf.linalg.matmul</code></a> will do use float32 intermediate math. The performance benefit of float16 is still apparent, due to increased memory bandwidth and the fact modern GPUs have specialized hardware for computing matmuls on float16 inputs while still keeping intermediate computations in float32. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">loss_scale</code> </td> <td> Returns the loss scale of this Policy. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Returns the name of this policy. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">variable_dtype</code> </td> <td> The variable dtype of this policy. <p>This is the dtype layers will create their variables in, unless a layer explicitly chooses a different dtype. If this is different than <a href="../policy#compute_dtype"><code translate="no" dir="ltr">Policy.compute_dtype</code></a>, Layers will cast variables to the compute dtype to avoid type errors.</p> <p>Variable regularizers are run in the variable dtype, not the compute dtype. </p>
</td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="from_config" data-text="from_config"><code translate="no" dir="ltr">from_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/policy.py#L404-L410">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
@classmethod
from_config(
    config, custom_objects=None
)
</pre> <h3 id="get_config" data-text="get_config"><code translate="no" dir="ltr">get_config</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/keras/mixed_precision/policy.py#L393-L402">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_config()
</pre>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/mixed_precision/experimental/Policy" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/mixed_precision/experimental/Policy</a>
  </p>
</div>
