<h1 class="devsite-page-title">tf.nn.atrous_conv2d</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/nn_ops.py#L1605-L1752">  View source on GitHub </a> </td> </table> <p>Atrous convolution (a.k.a. convolution with holes or dilated convolution).</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/atrous_conv2d"><code translate="no" dir="ltr">tf.compat.v1.nn.atrous_conv2d</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.atrous_conv2d(
    value, filters, rate, padding, name=None
)
</pre>  <p>This function is a simpler wrapper around the more general <a href="convolution"><code translate="no" dir="ltr">tf.nn.convolution</code></a>, and exists only for backwards compatibility. You can use <a href="convolution"><code translate="no" dir="ltr">tf.nn.convolution</code></a> to perform 1-D, 2-D, or 3-D atrous convolution.</p> <p>Computes a 2-D atrous convolution, also known as convolution with holes or dilated convolution, given 4-D <code translate="no" dir="ltr">value</code> and <code translate="no" dir="ltr">filters</code> tensors. If the <code translate="no" dir="ltr">rate</code> parameter is equal to one, it performs regular 2-D convolution. If the <code translate="no" dir="ltr">rate</code> parameter is greater than one, it performs convolution with holes, sampling the input values every <code translate="no" dir="ltr">rate</code> pixels in the <code translate="no" dir="ltr">height</code> and <code translate="no" dir="ltr">width</code> dimensions. This is equivalent to convolving the input with a set of upsampled filters, produced by inserting <code translate="no" dir="ltr">rate - 1</code> zeros between two consecutive values of the filters along the <code translate="no" dir="ltr">height</code> and <code translate="no" dir="ltr">width</code> dimensions, hence the name atrous convolution or convolution with holes (the French word trous means holes in English).</p> <h4 id="more_specifically" data-text="More specifically:">More specifically:</h4> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">output[batch, height, width, out_channel] =
    sum_{dheight, dwidth, in_channel} (
        filters[dheight, dwidth, in_channel, out_channel] *
        value[batch, height + rate*dheight, width + rate*dwidth, in_channel]
    )
</pre> <p>Atrous convolution allows us to explicitly control how densely to compute feature responses in fully convolutional networks. Used in conjunction with bilinear interpolation, it offers an alternative to <code translate="no" dir="ltr">conv2d_transpose</code> in dense prediction tasks such as semantic image segmentation, optical flow computation, or depth estimation. It also allows us to effectively enlarge the field of view of filters without increasing the number of parameters or the amount of computation.</p> <p>For a description of atrous convolution and how it can be used for dense feature extraction, please see: (Chen et al., 2015). The same operation is investigated further in (Yu et al., 2016). Previous works that effectively use atrous convolution in different ways are, among others, (Sermanet et al., 2014) and (Giusti et al., 2013). Atrous convolution is also closely related to the so-called noble identities in multi-rate signal processing.</p> <p>There are many different ways to implement atrous convolution (see the refs above). The implementation here reduces</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">atrous_conv2d(value, filters, rate, padding=padding)
</pre> <p>to the following three operations:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">paddings = ...
net = space_to_batch(value, paddings, block_size=rate)
net = conv2d(net, filters, strides=[1, 1, 1, 1], padding="VALID")
crops = ...
net = batch_to_space(net, crops, block_size=rate)
</pre> <p>Advanced usage. Note the following optimization: A sequence of <code translate="no" dir="ltr">atrous_conv2d</code> operations with identical <code translate="no" dir="ltr">rate</code> parameters, 'SAME' <code translate="no" dir="ltr">padding</code>, and filters with odd heights/ widths:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">net = atrous_conv2d(net, filters1, rate, padding="SAME")
net = atrous_conv2d(net, filters2, rate, padding="SAME")
...
net = atrous_conv2d(net, filtersK, rate, padding="SAME")
</pre> <p>can be equivalently performed cheaper in terms of computation and memory as:</p> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">pad = ...  # padding so that the input dims are multiples of rate
net = space_to_batch(net, paddings=pad, block_size=rate)
net = conv2d(net, filters1, strides=[1, 1, 1, 1], padding="SAME")
net = conv2d(net, filters2, strides=[1, 1, 1, 1], padding="SAME")
...
net = conv2d(net, filtersK, strides=[1, 1, 1, 1], padding="SAME")
net = batch_to_space(net, crops=pad, block_size=rate)
</pre> <p>because a pair of consecutive <code translate="no" dir="ltr">space_to_batch</code> and <code translate="no" dir="ltr">batch_to_space</code> ops with the same <code translate="no" dir="ltr">block_size</code> cancel out when their respective <code translate="no" dir="ltr">paddings</code> and <code translate="no" dir="ltr">crops</code> inputs are identical.</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">value</code> </td> <td> A 4-D <code translate="no" dir="ltr">Tensor</code> of type <code translate="no" dir="ltr">float</code>. It needs to be in the default "NHWC" format. Its shape is <code translate="no" dir="ltr">[batch, in_height, in_width, in_channels]</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">filters</code> </td> <td> A 4-D <code translate="no" dir="ltr">Tensor</code> with the same type as <code translate="no" dir="ltr">value</code> and shape <code translate="no" dir="ltr">[filter_height, filter_width, in_channels, out_channels]</code>. <code translate="no" dir="ltr">filters</code>' <code translate="no" dir="ltr">in_channels</code> dimension must match that of <code translate="no" dir="ltr">value</code>. Atrous convolution is equivalent to standard convolution with upsampled filters with effective height <code translate="no" dir="ltr">filter_height + (filter_height - 1) * (rate - 1)</code> and effective width <code translate="no" dir="ltr">filter_width + (filter_width - 1) * (rate - 1)</code>, produced by inserting <code translate="no" dir="ltr">rate - 1</code> zeros along consecutive elements across the <code translate="no" dir="ltr">filters</code>' spatial dimensions. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">rate</code> </td> <td> A positive int32. The stride with which we sample input values across the <code translate="no" dir="ltr">height</code> and <code translate="no" dir="ltr">width</code> dimensions. Equivalently, the rate by which we upsample the filter values by inserting zeros across the <code translate="no" dir="ltr">height</code> and <code translate="no" dir="ltr">width</code> dimensions. In the literature, the same parameter is sometimes called <code translate="no" dir="ltr">input stride</code> or <code translate="no" dir="ltr">dilation</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">padding</code> </td> <td> A string, either <code translate="no" dir="ltr">'VALID'</code> or <code translate="no" dir="ltr">'SAME'</code>. The padding algorithm. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> Optional name for the returned tensor. </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code> with the same type as <code translate="no" dir="ltr">value</code>. Output shape with <code translate="no" dir="ltr">'VALID'</code> padding is: <p>[batch, height - 2 * (filter_width - 1), width - 2 * (filter_height - 1), out_channels].</p> <p>Output shape with <code translate="no" dir="ltr">'SAME'</code> padding is:</p> <p>[batch, height, width, out_channels]. </p>
</td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> If input/output depth does not match <code translate="no" dir="ltr">filters</code>' shape, or if padding is other than <code translate="no" dir="ltr">'VALID'</code> or <code translate="no" dir="ltr">'SAME'</code>. </td> </tr> </table> <h4 id="references" data-text="References:">References:</h4> <p>Multi-Scale Context Aggregation by Dilated Convolutions: <a href="https://arxiv.org/abs/1511.07122">Yu et al., 2016</a> (<a href="https://arxiv.org/pdf/1511.07122.pdf">pdf</a>) Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs: <a href="http://arxiv.org/abs/1412.7062">Chen et al., 2015</a> (<a href="https://arxiv.org/pdf/1412.7062">pdf</a>) OverFeat - Integrated Recognition, Localization and Detection using Convolutional Networks: <a href="https://arxiv.org/abs/1312.6229">Sermanet et al., 2014</a> (<a href="https://arxiv.org/pdf/1312.6229.pdf">pdf</a>) Fast Image Scanning with Deep Max-Pooling Convolutional Neural Networks: <a href="https://ieeexplore.ieee.org/abstract/document/6738831">Giusti et al., 2013</a> (<a href="https://arxiv.org/pdf/1302.1700.pdf">pdf</a>)</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/atrous_conv2d" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/atrous_conv2d</a>
  </p>
</div>
