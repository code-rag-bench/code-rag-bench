<h1 class="devsite-page-title">tf.nn.with_space_to_batch</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/tensorflow/blob/v2.4.0/tensorflow/python/ops/nn_ops.py#L413-L571">  View source on GitHub </a> </td> </table> <p>Performs <code translate="no" dir="ltr">op</code> on the space-to-batch representation of <code translate="no" dir="ltr">input</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/nn/with_space_to_batch"><code translate="no" dir="ltr">tf.compat.v1.nn.with_space_to_batch</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.with_space_to_batch(
    input, dilation_rate, padding, op, filter_shape=None, spatial_dims=None,
    data_format=None
)
</pre>  <p>This has the effect of transforming sliding window operations into the corresponding "atrous" operation in which the input is sampled at the specified <code translate="no" dir="ltr">dilation_rate</code>.</p> <p>In the special case that <code translate="no" dir="ltr">dilation_rate</code> is uniformly 1, this simply returns:</p> <p>op(input, num_spatial_dims, padding)</p> <p>Otherwise, it returns:</p> <p>batch_to_space_nd( op(space_to_batch_nd(input, adjusted_dilation_rate, adjusted_paddings), num_spatial_dims, "VALID") adjusted_dilation_rate, adjusted_crops),</p> <p>where:</p> <p>adjusted_dilation_rate is an int64 tensor of shape [max(spatial<em>dims)], adjusted</em>{paddings,crops} are int64 tensors of shape [max(spatial_dims), 2]</p> <p>defined as follows:</p> <p>We first define two int64 tensors <code translate="no" dir="ltr">paddings</code> and <code translate="no" dir="ltr">crops</code> of shape <code translate="no" dir="ltr">[num_spatial_dims, 2]</code> based on the value of <code translate="no" dir="ltr">padding</code> and the spatial dimensions of the <code translate="no" dir="ltr">input</code>:</p> <p>If <code translate="no" dir="ltr">padding = "VALID"</code>, then:</p> <p>paddings, crops = required_space_to_batch_paddings( input_shape[spatial_dims], dilation_rate)</p> <p>If <code translate="no" dir="ltr">padding = "SAME"</code>, then:</p> <p>dilated_filter_shape = filter_shape + (filter_shape - 1) * (dilation_rate - 1)</p> <p>paddings, crops = required_space_to_batch_paddings( input_shape[spatial_dims], dilation_rate, [(dilated_filter_shape - 1) // 2, dilated_filter_shape - 1 - (dilated_filter_shape - 1) // 2])</p> <p>Because <code translate="no" dir="ltr">space_to_batch_nd</code> and <code translate="no" dir="ltr">batch_to_space_nd</code> assume that the spatial dimensions are contiguous starting at the second dimension, but the specified <code translate="no" dir="ltr">spatial_dims</code> may not be, we must adjust <code translate="no" dir="ltr">dilation_rate</code>, <code translate="no" dir="ltr">paddings</code> and <code translate="no" dir="ltr">crops</code> in order to be usable with these operations. For a given dimension, if the block size is 1, and both the starting and ending padding and crop amounts are 0, then space_to_batch_nd effectively leaves that dimension alone, which is what is needed for dimensions not part of <code translate="no" dir="ltr">spatial_dims</code>. Furthermore, <code translate="no" dir="ltr">space_to_batch_nd</code> and <code translate="no" dir="ltr">batch_to_space_nd</code> handle this case efficiently for any number of leading and trailing dimensions.</p> <p>For 0 &lt;= i &lt; len(spatial_dims), we assign:</p> <p>adjusted_dilation_rate[spatial_dims[i] - 1] = dilation_rate[i] adjusted_paddings[spatial_dims[i] - 1, :] = paddings[i, :] adjusted_crops[spatial_dims[i] - 1, :] = crops[i, :]</p> <p>All unassigned values of <code translate="no" dir="ltr">adjusted_dilation_rate</code> default to 1, while all unassigned values of <code translate="no" dir="ltr">adjusted_paddings</code> and <code translate="no" dir="ltr">adjusted_crops</code> default to 0.</p> <p>Note in the case that <code translate="no" dir="ltr">dilation_rate</code> is not uniformly 1, specifying "VALID" padding is equivalent to specifying <code translate="no" dir="ltr">padding = "SAME"</code> with a filter_shape of <code translate="no" dir="ltr">[1]*N</code>.</p> <p>Advanced usage. Note the following optimization: A sequence of <code translate="no" dir="ltr">with_space_to_batch</code> operations with identical (not uniformly 1) <code translate="no" dir="ltr">dilation_rate</code> parameters and "VALID" padding</p> <p>net = with_space_to_batch(net, dilation_rate, "VALID", op_1) ... net = with_space_to_batch(net, dilation_rate, "VALID", op_k)</p> <p>can be combined into a single <code translate="no" dir="ltr">with_space_to_batch</code> operation as follows:</p> <p>def combined_op(converted_input, num_spatial_dims, _): result = op_1(converted_input, num_spatial_dims, "VALID") ... result = op_k(result, num_spatial_dims, "VALID")</p> <p>net = with_space_to_batch(net, dilation_rate, "VALID", combined_op)</p> <p>This eliminates the overhead of <code translate="no" dir="ltr">k-1</code> calls to <code translate="no" dir="ltr">space_to_batch_nd</code> and <code translate="no" dir="ltr">batch_to_space_nd</code>.</p> <p>Similarly, a sequence of <code translate="no" dir="ltr">with_space_to_batch</code> operations with identical (not uniformly 1) <code translate="no" dir="ltr">dilation_rate</code> parameters, "SAME" padding, and odd filter dimensions</p> <p>net = with_space_to_batch(net, dilation_rate, "SAME", op_1, filter_shape_1) ... net = with_space_to_batch(net, dilation_rate, "SAME", op_k, filter_shape_k)</p> <p>can be combined into a single <code translate="no" dir="ltr">with_space_to_batch</code> operation as follows:</p> <p>def combined_op(converted_input, num_spatial_dims, _): result = op_1(converted_input, num_spatial_dims, "SAME") ... result = op_k(result, num_spatial_dims, "SAME")</p> <p>net = with_space_to_batch(net, dilation_rate, "VALID", combined_op)</p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">input</code> </td> <td> Tensor of rank &gt; max(spatial_dims). </td> </tr>
<tr> <td> <code translate="no" dir="ltr">dilation_rate</code> </td> <td> int32 Tensor of <em>known</em> shape [num_spatial_dims]. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">padding</code> </td> <td> str constant equal to "VALID" or "SAME" </td> </tr>
<tr> <td> <code translate="no" dir="ltr">op</code> </td> <td> Function that maps (input, num_spatial_dims, padding) -&gt; output </td> </tr>
<tr> <td> <code translate="no" dir="ltr">filter_shape</code> </td> <td> If padding = "SAME", specifies the shape of the convolution kernel/pooling window as an integer Tensor of shape [&gt;=num_spatial_dims]. If padding = "VALID", filter_shape is ignored and need not be specified. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">spatial_dims</code> </td> <td> Monotonically increasing sequence of <code translate="no" dir="ltr">num_spatial_dims</code> integers (which are &gt;= 1) specifying the spatial dimensions of <code translate="no" dir="ltr">input</code> and output. Defaults to: <code translate="no" dir="ltr">range(1, num_spatial_dims+1)</code>. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">data_format</code> </td> <td> A string or None. Specifies whether the channel dimension of the <code translate="no" dir="ltr">input</code> and output is the last dimension (default, or if <code translate="no" dir="ltr">data_format</code> does not start with "NC"), or the second dimension (if <code translate="no" dir="ltr">data_format</code> starts with "NC"). For N=1, the valid values are "NWC" (default) and "NCW". For N=2, the valid values are "NHWC" (default) and "NCHW". For N=3, the valid values are "NDHWC" (default) and "NCDHW". </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> The output Tensor as described above, dimensions will vary based on the op provided. </td> </tr> 
</table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Raises</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">padding</code> is invalid or the arguments are incompatible. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">ValueError</code> </td> <td> if <code translate="no" dir="ltr">spatial_dims</code> are invalid. </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/with_space_to_batch" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/with_space_to_batch</a>
  </p>
</div>
