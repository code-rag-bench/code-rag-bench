<h1 class="devsite-page-title">tf.nn.gelu</h1>       <p>Compute the Gaussian Error Linear Unit (GELU) activation function.</p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.nn.gelu(
    features, approximate=False, name=None
)
</pre>  <p>Gaussian error linear unit (GELU) computes <code translate="no" dir="ltr">x * P(X &lt;= x)</code>, where <code translate="no" dir="ltr">P(X) ~ N(0, 1)</code>. The (GELU) nonlinearity weights inputs by their value, rather than gates inputs by their sign as in ReLU.</p> <h4 id="for_example" data-text="For example:">For example:</h4> <pre class="devsite-click-to-copy prettyprint lang-py" translate="no" dir="ltr" data-language="cpp">
x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.nn.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.nn.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">features</code> </td> <td> A <code translate="no" dir="ltr">Tensor</code> representing preactivation values. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">approximate</code> </td> <td> An optional <code translate="no" dir="ltr">bool</code>. Defaults to <code translate="no" dir="ltr">False</code>. Whether to enable approximation. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">name</code> </td> <td> A name for the operation (optional). </td> </tr> </table>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Returns</th></tr> <tr class="alt"> <td colspan="2"> A <code translate="no" dir="ltr">Tensor</code> with the same type as <code translate="no" dir="ltr">features</code>. </td> </tr> 
</table> <h4 id="references" data-text="References:">References:</h4> <p><a href="https://arxiv.org/abs/1606.08415">Gaussian Error Linear Units (GELUs)</a>.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/gelu" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/nn/gelu</a>
  </p>
</div>
