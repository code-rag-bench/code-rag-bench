<h1 class="devsite-page-title">tf.estimator.experimental.LinearSDCA</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear.py#L49-L241">  View source on GitHub </a> </td> </table> <p>Stochastic Dual Coordinate Ascent helper for linear estimators.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/experimental/LinearSDCA"><code translate="no" dir="ltr">tf.compat.v1.estimator.experimental.LinearSDCA</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.estimator.experimental.LinearSDCA(
    example_id_column, num_loss_partitions=1, num_table_shards=None,
    symmetric_l1_regularization=0.0, symmetric_l2_regularization=1.0, adaptive=False
)
</pre>  <p>Objects of this class are intended to be provided as the optimizer argument (though LinearSDCA objects do not implement the <code translate="no" dir="ltr">tf.train.Optimizer</code> interface) when creating <a href="../linearclassifier"><code translate="no" dir="ltr">tf.estimator.LinearClassifier</code></a> or <a href="../linearregressor"><code translate="no" dir="ltr">tf.estimator.LinearRegressor</code></a>.</p> <p>SDCA can only be used with <code translate="no" dir="ltr">LinearClassifier</code> and <code translate="no" dir="ltr">LinearRegressor</code> under the following conditions:</p> <ul> <li>Feature columns are of type V2.</li> <li>Multivalent categorical columns are not normalized. In other words the <code translate="no" dir="ltr">sparse_combiner</code> argument in the estimator constructor should be "sum".</li> <li>For classification: binary label.</li> <li>For regression: one-dimensional label.</li> </ul> <h4 id="example_usage" data-text="Example usage:">Example usage:</h4> <pre class="prettyprint lang-python" translate="no" dir="ltr" data-language="python">real_feature_column = numeric_column(...)
sparse_feature_column = categorical_column_with_hash_bucket(...)
linear_sdca = tf.estimator.experimental.LinearSDCA(
    example_id_column='example_id',
    num_loss_partitions=1,
    num_table_shards=1,
    symmetric_l2_regularization=2.0)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[real_feature_column, sparse_feature_column],
    weight_column=...,
    optimizer=linear_sdca)
classifier.train(input_fn_train, steps=50)
classifier.evaluate(input_fn=input_fn_eval)
</pre> <p>Here the expectation is that the <code translate="no" dir="ltr">input_fn_*</code> functions passed to train and evaluate return a pair (dict, label_tensor) where dict has <code translate="no" dir="ltr">example_id_column</code> as <code translate="no" dir="ltr">key</code> whose value is a <code translate="no" dir="ltr">Tensor</code> of shape [batch_size] and dtype string. num_loss_partitions defines sigma' in eq (11) of [3]. Convergence of (global) loss is guaranteed if <code translate="no" dir="ltr">num_loss_partitions</code> is larger or equal to the product <code translate="no" dir="ltr">(#concurrent train ops/per worker) x (#workers)</code>. Larger values for <code translate="no" dir="ltr">num_loss_partitions</code> lead to slower convergence. The recommended value for <code translate="no" dir="ltr">num_loss_partitions</code> in <a href="../../estimator"><code translate="no" dir="ltr">tf.estimator</code></a> (where currently there is one process per worker) is the number of workers running the train steps. It defaults to 1 (single machine). <code translate="no" dir="ltr">num_table_shards</code> defines the number of shards for the internal state table, typically set to match the number of parameter servers for large data sets.</p> <p>The SDCA algorithm was originally introduced in [1] and it was followed by the L1 proximal step [2], a distributed version [3] and adaptive sampling [4]. [1] www.jmlr.org/papers/volume14/shalev-shwartz13a/shalev-shwartz13a.pdf [2] <a href="https://arxiv.org/pdf/1309.2375.pdf">https://arxiv.org/pdf/1309.2375.pdf</a> [3] <a href="https://arxiv.org/pdf/1502.03508.pdf">https://arxiv.org/pdf/1502.03508.pdf</a> [4] <a href="https://arxiv.org/pdf/1502.08053.pdf">https://arxiv.org/pdf/1502.08053.pdf</a> Details specific to this implementation are provided in: <a href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear_optimizer/doc/sdca.ipynb">https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear_optimizer/doc/sdca.ipynb</a></p>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Args</th></tr> 
<tr> <td> <code translate="no" dir="ltr">example_id_column</code> </td> <td> The column name containing the example ids. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_loss_partitions</code> </td> <td> Number of workers. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">num_table_shards</code> </td> <td> Number of shards of the internal state table, typically set to match the number of parameter servers. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">symmetric_l1_regularization</code> </td> <td> A float value, must be greater than or equal to zero. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">symmetric_l2_regularization</code> </td> <td> A float value, must be greater than zero and should typically be greater than 1. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">adaptive</code> </td> <td> A boolean indicating whether to use adaptive sampling. </td> </tr> </table> <h2 id="methods" data-text="Methods">Methods</h2> <h3 id="get_train_step" data-text="get_train_step"><code translate="no" dir="ltr">get_train_step</code></h3> <p><a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/canned/linear.py#L179-L241">View source</a></p> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
get_train_step(
    state_manager, weight_column_name, loss_type, feature_columns, features,
    targets, bias_var, global_step
)
</pre> <p>Returns the training operation of an SdcaModel optimizer.</p>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator/experimental/LinearSDCA" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator/experimental/LinearSDCA</a>
  </p>
</div>
