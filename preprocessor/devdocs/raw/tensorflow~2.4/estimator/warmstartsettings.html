<h1 class="devsite-page-title">tf.estimator.WarmStartSettings</h1>      <table class="tfo-notebook-buttons tfo-api nocontent" align="left">  <td> <a target="_blank" href="https://github.com/tensorflow/estimator/tree/master/tensorflow_estimator/python/estimator/estimator.py#L2157-L2351">  View source on GitHub </a> </td> </table> <p>Settings for warm-starting in <code translate="no" dir="ltr">tf.estimator.Estimators</code>.</p> <section class="expandable"> <h4 class="showalways" id="view-aliases" data-text="View aliases">View aliases</h4> <p> <b>Compat aliases for migration</b> </p>
<p>See <a href="https://www.tensorflow.org/guide/migrate">Migration guide</a> for more details.</p> <p><a href="https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings"><code translate="no" dir="ltr">tf.compat.v1.estimator.WarmStartSettings</code></a></p> </section> <pre class="devsite-click-to-copy prettyprint lang-py tfo-signature-link" translate="no" dir="ltr" data-language="cpp">
tf.estimator.WarmStartSettings(
    ckpt_to_initialize_from, vars_to_warm_start='.*',
    var_name_to_vocab_info=None, var_name_to_prev_var_name=None
)
</pre>  <p>Example Use with canned <a href="dnnestimator"><code translate="no" dir="ltr">tf.estimator.DNNEstimator</code></a>:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">emb_vocab_file = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_file(
        "sc_vocab_file", "new_vocab.txt", vocab_size=100),
    dimension=8)
emb_vocab_list = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        "sc_vocab_list", vocabulary_list=["a", "b"]),
    dimension=8)
estimator = tf.estimator.DNNClassifier(
  hidden_units=[128, 64], feature_columns=[emb_vocab_file, emb_vocab_list],
  warm_start_from=ws)
</pre> <p>where <code translate="no" dir="ltr">ws</code> could be defined as:</p> <p>Warm-start all weights in the model (input layer and hidden weights). Either the directory or a specific checkpoint can be provided (in the case of the former, the latest checkpoint will be used):</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ws = WarmStartSettings(ckpt_to_initialize_from="/tmp")
ws = WarmStartSettings(ckpt_to_initialize_from="/tmp/model-1000")
</pre> <p>Warm-start only the embeddings (input layer):</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ws = WarmStartSettings(ckpt_to_initialize_from="/tmp",
                       vars_to_warm_start=".*input_layer.*")
</pre> <p>Warm-start all weights but the embedding parameters corresponding to <code translate="no" dir="ltr">sc_vocab_file</code> have a different vocab from the one used in the current model:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">vocab_info = tf.estimator.VocabInfo(
    new_vocab=sc_vocab_file.vocabulary_file,
    new_vocab_size=sc_vocab_file.vocabulary_size,
    num_oov_buckets=sc_vocab_file.num_oov_buckets,
    old_vocab="old_vocab.txt"
)
ws = WarmStartSettings(
    ckpt_to_initialize_from="/tmp",
    var_name_to_vocab_info={
        "input_layer/sc_vocab_file_embedding/embedding_weights": vocab_info
    })
</pre> <p>Warm-start only <code translate="no" dir="ltr">sc_vocab_file</code> embeddings (and no other variables), which have a different vocab from the one used in the current model:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">vocab_info = tf.estimator.VocabInfo(
    new_vocab=sc_vocab_file.vocabulary_file,
    new_vocab_size=sc_vocab_file.vocabulary_size,
    num_oov_buckets=sc_vocab_file.num_oov_buckets,
    old_vocab="old_vocab.txt"
)
ws = WarmStartSettings(
    ckpt_to_initialize_from="/tmp",
    vars_to_warm_start=None,
    var_name_to_vocab_info={
        "input_layer/sc_vocab_file_embedding/embedding_weights": vocab_info
    })
</pre> <p>Warm-start all weights but the parameters corresponding to <code translate="no" dir="ltr">sc_vocab_file</code> have a different vocab from the one used in current checkpoint, and only 100 of those entries were used:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">vocab_info = tf.estimator.VocabInfo(
    new_vocab=sc_vocab_file.vocabulary_file,
    new_vocab_size=sc_vocab_file.vocabulary_size,
    num_oov_buckets=sc_vocab_file.num_oov_buckets,
    old_vocab="old_vocab.txt",
    old_vocab_size=100
)
ws = WarmStartSettings(
    ckpt_to_initialize_from="/tmp",
    var_name_to_vocab_info={
        "input_layer/sc_vocab_file_embedding/embedding_weights": vocab_info
    })
</pre> <p>Warm-start all weights but the parameters corresponding to <code translate="no" dir="ltr">sc_vocab_file</code> have a different vocab from the one used in current checkpoint and the parameters corresponding to <code translate="no" dir="ltr">sc_vocab_list</code> have a different name from the current checkpoint:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">vocab_info = tf.estimator.VocabInfo(
    new_vocab=sc_vocab_file.vocabulary_file,
    new_vocab_size=sc_vocab_file.vocabulary_size,
    num_oov_buckets=sc_vocab_file.num_oov_buckets,
    old_vocab="old_vocab.txt",
    old_vocab_size=100
)
ws = WarmStartSettings(
    ckpt_to_initialize_from="/tmp",
    var_name_to_vocab_info={
        "input_layer/sc_vocab_file_embedding/embedding_weights": vocab_info
    },
    var_name_to_prev_var_name={
        "input_layer/sc_vocab_list_embedding/embedding_weights":
            "old_tensor_name"
    })
</pre> <p>Warm-start all TRAINABLE variables:</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ws = WarmStartSettings(ckpt_to_initialize_from="/tmp",
                       vars_to_warm_start=".*")
</pre> <p>Warm-start all variables (including non-TRAINABLE):</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ws = WarmStartSettings(ckpt_to_initialize_from="/tmp",
                       vars_to_warm_start=[".*"])
</pre> <p>Warm-start non-TRAINABLE variables "v1", "v1/Momentum", and "v2" but not "v2/momentum":</p> <pre class="prettyprint" translate="no" dir="ltr" data-language="cpp">ws = WarmStartSettings(ckpt_to_initialize_from="/tmp",
                       vars_to_warm_start=["v1", "v2[^/]"])
</pre>  
<table class="responsive fixed orange"> <colgroup>
<col width="214px">
<col>
</colgroup> <tr><th colspan="2">Attributes</th></tr> 
<tr> <td> <code translate="no" dir="ltr">ckpt_to_initialize_from</code> </td> <td> [Required] A string specifying the directory with checkpoint file(s) or path to checkpoint from which to warm-start the model parameters. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">vars_to_warm_start</code> </td> <td> [Optional] One of the following: <ul> <li>A regular expression (string) that captures which variables to warm-start (see tf.compat.v1.get_collection). This expression will only consider variables in the TRAINABLE_VARIABLES collection -- if you need to warm-start non_TRAINABLE vars (such as optimizer accumulators or batch norm statistics), please use the below option.</li> <li>A list of strings, each a regex scope provided to tf.compat.v1.get_collection with GLOBAL_VARIABLES (please see tf.compat.v1.get_collection). For backwards compatibility reasons, this is separate from the single-string argument type.</li> <li>A list of Variables to warm-start. If you do not have access to the <code translate="no" dir="ltr">Variable</code> objects at the call site, please use the above option.</li> <li>
<code translate="no" dir="ltr">None</code>, in which case only TRAINABLE variables specified in <code translate="no" dir="ltr">var_name_to_vocab_info</code> will be warm-started.</li> </ul> <p>Defaults to <code translate="no" dir="ltr">'.*'</code>, which warm-starts all variables in the TRAINABLE_VARIABLES collection. Note that this excludes variables such as accumulators and moving statistics from batch norm. </p>
</td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_name_to_vocab_info</code> </td> <td> [Optional] Dict of variable names (strings) to <a href="vocabinfo"><code translate="no" dir="ltr">tf.estimator.VocabInfo</code></a>. The variable names should be "full" variables, not the names of the partitions. If not explicitly provided, the variable is assumed to have no (changes to) vocabulary. </td> </tr>
<tr> <td> <code translate="no" dir="ltr">var_name_to_prev_var_name</code> </td> <td> [Optional] Dict of variable names (strings) to name of the previously-trained variable in <code translate="no" dir="ltr">ckpt_to_initialize_from</code>. If not explicitly provided, the name of the variable is assumed to be same between previous checkpoint and current model. Note that this has no effect on the set of variables that is warm-started, and only controls name mapping (use <code translate="no" dir="ltr">vars_to_warm_start</code> for controlling what variables to warm-start). </td> </tr> </table>  <devsite-page-rating position="footer" selected-rating="0" hover-rating-star="0"> </devsite-page-rating><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2020 The TensorFlow Authors. All rights reserved.<br>Licensed under the Creative Commons Attribution License 3.0.<br>Code samples licensed under the Apache 2.0 License.<br>
    <a href="https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator/WarmStartSettings" class="_attribution-link">https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/estimator/WarmStartSettings</a>
  </p>
</div>
