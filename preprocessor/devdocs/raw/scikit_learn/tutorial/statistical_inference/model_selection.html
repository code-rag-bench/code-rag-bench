<h1 id="model-selection-tut">Model selection: choosing estimators and their parameters</h1>  <h2 id="score-and-cross-validated-scores">Score, and cross-validated scores</h2> <p>As we have seen, every estimator exposes a <code>score</code> method that can judge the quality of the fit (or the prediction) on new data. <strong>Bigger is better</strong>.</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import datasets, svm
&gt;&gt;&gt; X_digits, y_digits = datasets.load_digits(return_X_y=True)
&gt;&gt;&gt; svc = svm.SVC(C=1, kernel='linear')
&gt;&gt;&gt; svc.fit(X_digits[:-100], y_digits[:-100]).score(X_digits[-100:], y_digits[-100:])
0.98
</pre> <p>To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in <em>folds</em> that we use for training and testing:</p> <pre data-language="python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; X_folds = np.array_split(X_digits, 3)
&gt;&gt;&gt; y_folds = np.array_split(y_digits, 3)
&gt;&gt;&gt; scores = list()
&gt;&gt;&gt; for k in range(3):
...     # We use 'list' to copy, in order to 'pop' later on
...     X_train = list(X_folds)
...     X_test = X_train.pop(k)
...     X_train = np.concatenate(X_train)
...     y_train = list(y_folds)
...     y_test = y_train.pop(k)
...     y_train = np.concatenate(y_train)
...     scores.append(svc.fit(X_train, y_train).score(X_test, y_test))
&gt;&gt;&gt; print(scores)
[0.934..., 0.956..., 0.939...]
</pre> <p>This is called a <a class="reference internal" href="../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code>KFold</code></a> cross-validation.</p>   <h2 id="cv-generators-tut">Cross-validation generators</h2> <p id="cross-validation-generators">Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies.</p> <p>They expose a <code>split</code> method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy.</p> <p>This example shows an example usage of the <code>split</code> method.</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import KFold, cross_val_score
&gt;&gt;&gt; X = ["a", "a", "a", "b", "b", "c", "c", "c", "c", "c"]
&gt;&gt;&gt; k_fold = KFold(n_splits=5)
&gt;&gt;&gt; for train_indices, test_indices in k_fold.split(X):
...      print('Train: %s | test: %s' % (train_indices, test_indices))
Train: [2 3 4 5 6 7 8 9] | test: [0 1]
Train: [0 1 4 5 6 7 8 9] | test: [2 3]
Train: [0 1 2 3 6 7 8 9] | test: [4 5]
Train: [0 1 2 3 4 5 8 9] | test: [6 7]
Train: [0 1 2 3 4 5 6 7] | test: [8 9]
</pre> <p>The cross-validation can then be performed easily:</p> <pre data-language="python">&gt;&gt;&gt; [svc.fit(X_digits[train], y_digits[train]).score(X_digits[test], y_digits[test])
...  for train, test in k_fold.split(X_digits)]
[0.963..., 0.922..., 0.963..., 0.963..., 0.930...]
</pre> <p>The cross-validation score can be directly calculated using the <a class="reference internal" href="../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>cross_val_score</code></a> helper. Given an estimator, the cross-validation object and the input dataset, the <a class="reference internal" href="../../modules/generated/sklearn.model_selection.cross_val_score#sklearn.model_selection.cross_val_score" title="sklearn.model_selection.cross_val_score"><code>cross_val_score</code></a> splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation.</p> <p>By default the estimator’s <code>score</code> method is used to compute the individual scores.</p> <p>Refer the <a class="reference internal" href="../../modules/metrics#metrics"><span class="std std-ref">metrics module</span></a> to learn more on the available scoring methods.</p> <pre data-language="python">&gt;&gt;&gt; cross_val_score(svc, X_digits, y_digits, cv=k_fold, n_jobs=-1)
array([0.96388889, 0.92222222, 0.9637883 , 0.9637883 , 0.93036212])
</pre> <p><code>n_jobs=-1</code> means that the computation will be dispatched on all the CPUs of the computer.</p> <p>Alternatively, the <code>scoring</code> argument can be provided to specify an alternative scoring method.</p>  <pre data-language="python">&gt;&gt;&gt; cross_val_score(svc, X_digits, y_digits, cv=k_fold,
...                 scoring='precision_macro')
array([0.96578289, 0.92708922, 0.96681476, 0.96362897, 0.93192644])
</pre> <p><strong>Cross-validation generators</strong></p>  <table class="docutils align-default">   <tr>
<td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.kfold#sklearn.model_selection.KFold" title="sklearn.model_selection.KFold"><code>KFold</code></a> <strong>(n_splits, shuffle, random_state)</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.stratifiedkfold#sklearn.model_selection.StratifiedKFold" title="sklearn.model_selection.StratifiedKFold"><code>StratifiedKFold</code></a> <strong>(n_splits, shuffle, random_state)</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.groupkfold#sklearn.model_selection.GroupKFold" title="sklearn.model_selection.GroupKFold"><code>GroupKFold</code></a> <strong>(n_splits)</strong></p></td> </tr> <tr>
<td><p>Splits it into K folds, trains on K-1 and then tests on the left-out.</p></td> <td><p>Same as K-Fold but preserves the class distribution within each fold.</p></td> <td><p>Ensures that the same group is not in both testing and training sets.</p></td> </tr>  </table> <table class="docutils align-default">   <tr>
<td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.shufflesplit#sklearn.model_selection.ShuffleSplit" title="sklearn.model_selection.ShuffleSplit"><code>ShuffleSplit</code></a> <strong>(n_splits, test_size, train_size, random_state)</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.stratifiedshufflesplit#sklearn.model_selection.StratifiedShuffleSplit" title="sklearn.model_selection.StratifiedShuffleSplit"><code>StratifiedShuffleSplit</code></a></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.groupshufflesplit#sklearn.model_selection.GroupShuffleSplit" title="sklearn.model_selection.GroupShuffleSplit"><code>GroupShuffleSplit</code></a></p></td> </tr> <tr>
<td><p>Generates train/test indices based on random permutation.</p></td> <td><p>Same as shuffle split but preserves the class distribution within each iteration.</p></td> <td><p>Ensures that the same group is not in both testing and training sets.</p></td> </tr>  </table> <table class="docutils align-default">   <tr>
<td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.leaveonegroupout#sklearn.model_selection.LeaveOneGroupOut" title="sklearn.model_selection.LeaveOneGroupOut"><code>LeaveOneGroupOut</code></a> <strong>()</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.leavepgroupsout#sklearn.model_selection.LeavePGroupsOut" title="sklearn.model_selection.LeavePGroupsOut"><code>LeavePGroupsOut</code></a> <strong>(n_groups)</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.leaveoneout#sklearn.model_selection.LeaveOneOut" title="sklearn.model_selection.LeaveOneOut"><code>LeaveOneOut</code></a> <strong>()</strong></p></td> </tr> <tr>
<td><p>Takes a group array to group observations.</p></td> <td><p>Leave P groups out.</p></td> <td><p>Leave one observation out.</p></td> </tr>  </table> <table class="docutils align-default">   <tr>
<td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.leavepout#sklearn.model_selection.LeavePOut" title="sklearn.model_selection.LeavePOut"><code>LeavePOut</code></a> <strong>(p)</strong></p></td> <td><p><a class="reference internal" href="../../modules/generated/sklearn.model_selection.predefinedsplit#sklearn.model_selection.PredefinedSplit" title="sklearn.model_selection.PredefinedSplit"><code>PredefinedSplit</code></a></p></td> </tr> <tr>
<td><p>Leave P observations out.</p></td> <td><p>Generates train/test indices based on predefined splits.</p></td> </tr>  </table> <div class="topic"> <p class="topic-title"><strong>Exercise</strong></p> <p>On the digits dataset, plot the cross-validation score of a <a class="reference internal" href="../../modules/generated/sklearn.svm.svc#sklearn.svm.SVC" title="sklearn.svm.SVC"><code>SVC</code></a> estimator with an linear kernel as a function of parameter <code>C</code> (use a logarithmic grid of points, from 1 to 10).</p>  <pre data-language="python">
import numpy as np
from sklearn.model_selection import cross_val_score
from sklearn import datasets, svm

X, y = datasets.load_digits(return_X_y=True)

svc = svm.SVC(kernel='linear')
C_s = np.logspace(-10, 0, 10)

scores = list()
</pre>  <a class="reference external image-reference" href="../../auto_examples/exercises/plot_cv_digits"><img alt="../../_images/sphx_glr_plot_cv_digits_001.png" class="align-center" src="data:image/png;base64,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" style="width: 576.0px; height: 432.0px;"></a> <p><strong>Solution:</strong> <a class="reference internal" href="../../auto_examples/exercises/plot_cv_digits#sphx-glr-auto-examples-exercises-plot-cv-digits-py"><span class="std std-ref">Cross-validation on Digits Dataset Exercise</span></a></p> </div>   <h2 id="grid-search-and-cross-validated-estimators">Grid-search and cross-validated estimators</h2>  <h3 id="grid-search">Grid-search</h3> <p>scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn.model_selection import GridSearchCV, cross_val_score
&gt;&gt;&gt; Cs = np.logspace(-6, -1, 10)
&gt;&gt;&gt; clf = GridSearchCV(estimator=svc, param_grid=dict(C=Cs),
...                    n_jobs=-1)
&gt;&gt;&gt; clf.fit(X_digits[:1000], y_digits[:1000])        
GridSearchCV(cv=None,...
&gt;&gt;&gt; clf.best_score_                                  
0.925...
&gt;&gt;&gt; clf.best_estimator_.C                            
0.0077...

&gt;&gt;&gt; # Prediction performance on test set is not as good as on train set
&gt;&gt;&gt; clf.score(X_digits[1000:], y_digits[1000:])      
0.943...
</pre> <p>By default, the <a class="reference internal" href="../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>GridSearchCV</code></a> uses a 5-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 5-fold.</p> <div class="topic"> <p class="topic-title">Nested cross-validation</p> <pre data-language="python">&gt;&gt;&gt; cross_val_score(clf, X_digits, y_digits) 
array([0.938..., 0.963..., 0.944...])
</pre> <p>Two cross-validation loops are performed in parallel: one by the <a class="reference internal" href="../../modules/generated/sklearn.model_selection.gridsearchcv#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code>GridSearchCV</code></a> estimator to set <code>gamma</code> and the other one by <code>cross_val_score</code> to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>You cannot nest objects with parallel computing (<code>n_jobs</code> different than 1).</p> </div>   <h3 id="cv-estimators-tut">Cross-validated estimators</h3> <p id="cross-validated-estimators">Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why, for certain estimators, scikit-learn exposes <a class="reference internal" href="../../modules/cross_validation#cross-validation"><span class="std std-ref">Cross-validation: evaluating estimator performance</span></a> estimators that set their parameter automatically by cross-validation:</p> <pre data-language="python">&gt;&gt;&gt; from sklearn import linear_model, datasets
&gt;&gt;&gt; lasso = linear_model.LassoCV()
&gt;&gt;&gt; X_diabetes, y_diabetes = datasets.load_diabetes(return_X_y=True)
&gt;&gt;&gt; lasso.fit(X_diabetes, y_diabetes)
LassoCV()
&gt;&gt;&gt; # The estimator chose automatically its lambda:
&gt;&gt;&gt; lasso.alpha_
0.00375...
</pre> <p>These estimators are called similarly to their counterparts, with ‘CV’ appended to their name.</p> <div class="topic"> <p class="topic-title"><strong>Exercise</strong></p> <p>On the diabetes dataset, find the optimal regularization parameter alpha.</p> <p><strong>Bonus</strong>: How much can you trust the selection of alpha?</p> <pre data-language="python">from sklearn import datasets
from sklearn.linear_model import LassoCV
from sklearn.linear_model import Lasso
from sklearn.model_selection import KFold
from sklearn.model_selection import GridSearchCV

X, y = datasets.load_diabetes(return_X_y=True)
X = X[:150]
</pre> <p><strong>Solution:</strong> <a class="reference internal" href="../../auto_examples/exercises/plot_cv_diabetes#sphx-glr-auto-examples-exercises-plot-cv-diabetes-py"><span class="std std-ref">Cross-validation on diabetes Dataset Exercise</span></a></p> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/tutorial/statistical_inference/model_selection.html" class="_attribution-link">https://scikit-learn.org/0.24/tutorial/statistical_inference/model_selection.html</a>
  </p>
</div>
