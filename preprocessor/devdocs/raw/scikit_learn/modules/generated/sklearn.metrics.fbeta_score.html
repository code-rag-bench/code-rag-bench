<h1>sklearn.metrics.fbeta_score</h1> <dl class="py function"> <dt id="sklearn.metrics.fbeta_score">
<code>sklearn.metrics.fbeta_score(y_true, y_pred, *, beta, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')</code> <a class="reference external" href="https://github.com/scikit-learn/scikit-learn/blob/95119c13a/sklearn/metrics/_classification.py#L1074"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Compute the F-beta score.</p> <p>The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.</p> <p>The <code>beta</code> parameter determines the weight of recall in the combined score. <code>beta &lt; 1</code> lends more weight to precision, while <code>beta &gt; 1</code> favors recall (<code>beta -&gt; 0</code> considers only precision, <code>beta -&gt; +inf</code> only recall).</p> <p>Read more in the <a class="reference internal" href="../model_evaluation#precision-recall-f-measure-metrics"><span class="std std-ref">User Guide</span></a>.</p> <dl class="field-list"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<dl> <dt>
<code>y_true1d array-like, or label indicator array / sparse matrix</code> </dt>
<dd>
<p>Ground truth (correct) target values.</p> </dd> <dt>
<code>y_pred1d array-like, or label indicator array / sparse matrix</code> </dt>
<dd>
<p>Estimated targets as returned by a classifier.</p> </dd> <dt>
<code>betafloat</code> </dt>
<dd>
<p>Determines the weight of recall in the combined score.</p> </dd> <dt>
<code>labelsarray-like, default=None</code> </dt>
<dd>
<p>The set of labels to include when <code>average != 'binary'</code>, and their order if <code>average is None</code>. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in <code>y_true</code> and <code>y_pred</code> are used in sorted order.</p> <div class="versionchanged"> <p><span class="versionmodified changed">Changed in version 0.17: </span>Parameter <code>labels</code> improved for multiclass problem.</p> </div> </dd> <dt>
<code>pos_labelstr or int, default=1</code> </dt>
<dd>
<p>The class to report if <code>average='binary'</code> and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting <code>labels=[pos_label]</code> and <code>average != 'binary'</code> will report scores for that label only.</p> </dd> <dt>
<code>average{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} or None default=’binary’</code> </dt>
<dd>
<p>This parameter is required for multiclass/multilabel targets. If <code>None</code>, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:</p> <dl class="simple"> <dt>
<code>'binary':</code> </dt>
<dd>
<p>Only report results for the class specified by <code>pos_label</code>. This is applicable only if targets (<code>y_{true,pred}</code>) are binary.</p> </dd> <dt>
<code>'micro':</code> </dt>
<dd>
<p>Calculate metrics globally by counting the total true positives, false negatives and false positives.</p> </dd> <dt>
<code>'macro':</code> </dt>
<dd>
<p>Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.</p> </dd> <dt>
<code>'weighted':</code> </dt>
<dd>
<p>Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.</p> </dd> <dt>
<code>'samples':</code> </dt>
<dd>
<p>Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from <a class="reference internal" href="sklearn.metrics.accuracy_score#sklearn.metrics.accuracy_score" title="sklearn.metrics.accuracy_score"><code>accuracy_score</code></a>).</p> </dd> </dl> </dd> <dt>
<code>sample_weightarray-like of shape (n_samples,), default=None</code> </dt>
<dd>
<p>Sample weights.</p> </dd> <dt>
<code>zero_division“warn”, 0 or 1, default=”warn”</code> </dt>
<dd>
<p>Sets the value to return when there is a zero division, i.e. when all predictions and labels are negative. If set to “warn”, this acts as 0, but warnings are also raised.</p> </dd> </dl> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<dl class="simple"> <dt>
<code>fbeta_scorefloat (if average is not None) or array of float, shape = [n_unique_labels]</code> </dt>
<dd>
<p>F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task.</p> </dd> </dl> </dd> </dl> <div class="admonition seealso"> <p class="admonition-title">See also</p> <dl class="simple"> <dt>
<code>precision_recall_fscore_support,</code> <a class="reference internal" href="sklearn.metrics.multilabel_confusion_matrix#sklearn.metrics.multilabel_confusion_matrix" title="sklearn.metrics.multilabel_confusion_matrix"><code>multilabel_confusion_matrix</code></a>
</dt>
 </dl> </div> <h4 class="rubric">Notes</h4> <p>When <code>true positive + false positive == 0</code> or <code>true positive + false negative == 0</code>, f-score returns 0 and raises <code>UndefinedMetricWarning</code>. This behavior can be modified with <code>zero_division</code>.</p> <h4 class="rubric">References</h4> <dl class="citation"> <dt class="label" id="r7380c1e4fbce-1">
<code>1</code> </dt> <dd>
<p>R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 327-328.</p> </dd> <dt class="label" id="r7380c1e4fbce-2">
<code>2</code> </dt> <dd>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/F1_score">Wikipedia entry for the F1-score</a>.</p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; from sklearn.metrics import fbeta_score
&gt;&gt;&gt; y_true = [0, 1, 2, 0, 1, 2]
&gt;&gt;&gt; y_pred = [0, 2, 1, 0, 0, 1]
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='macro', beta=0.5)
0.23...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='micro', beta=0.5)
0.33...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average='weighted', beta=0.5)
0.23...
&gt;&gt;&gt; fbeta_score(y_true, y_pred, average=None, beta=0.5)
array([0.71..., 0.        , 0.        ])
</pre> </dd>
</dl> <div class="_attribution">
  <p class="_attribution-p">
    &copy; 2007&ndash;2020 The scikit-learn developers<br>Licensed under the 3-clause BSD License.<br>
    <a href="https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.fbeta_score.html" class="_attribution-link">https://scikit-learn.org/0.24/modules/generated/sklearn.metrics.fbeta_score.html</a>
  </p>
</div>
