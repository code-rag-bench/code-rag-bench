<h1 id="torch-cuda">torch.cuda</h1> <p id="module-torch.cuda">This package adds support for CUDA tensor types, that implement the same function as CPU tensors, but they utilize GPUs for computation.</p> <p>It is lazily initialized, so you can always import it, and use <a class="reference internal" href="generated/torch.cuda.is_available#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available()</code></a> to determine if your system supports CUDA.</p> <p><a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#cuda-semantics"><span class="std std-ref">CUDA semantics</span></a> has more details about working with CUDA.</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.streamcontext#torch.cuda.StreamContext" title="torch.cuda.StreamContext"><code>StreamContext</code></a>
</td> <td><p>Context-manager that selects a given stream.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.can_device_access_peer#torch.cuda.can_device_access_peer" title="torch.cuda.can_device_access_peer"><code>can_device_access_peer</code></a>
</td> <td><p>Checks if peer access between two devices is possible.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.current_blas_handle#torch.cuda.current_blas_handle" title="torch.cuda.current_blas_handle"><code>current_blas_handle</code></a>
</td> <td><p>Returns cublasHandle_t pointer to current cuBLAS handle</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.current_device#torch.cuda.current_device" title="torch.cuda.current_device"><code>current_device</code></a>
</td> <td><p>Returns the index of a currently selected device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.current_stream#torch.cuda.current_stream" title="torch.cuda.current_stream"><code>current_stream</code></a>
</td> <td><p>Returns the currently selected <a class="reference internal" href="generated/torch.cuda.stream#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.default_stream#torch.cuda.default_stream" title="torch.cuda.default_stream"><code>default_stream</code></a>
</td> <td><p>Returns the default <a class="reference internal" href="generated/torch.cuda.stream#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a> for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.device#torch.cuda.device" title="torch.cuda.device"><code>device</code></a>
</td> <td><p>Context-manager that changes the selected device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.device_count#torch.cuda.device_count" title="torch.cuda.device_count"><code>device_count</code></a>
</td> <td><p>Returns the number of GPUs available.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.device_of#torch.cuda.device_of" title="torch.cuda.device_of"><code>device_of</code></a>
</td> <td><p>Context-manager that changes the current device to that of given object.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_arch_list#torch.cuda.get_arch_list" title="torch.cuda.get_arch_list"><code>get_arch_list</code></a>
</td> <td><p>Returns list CUDA architectures this library was compiled for.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_device_capability#torch.cuda.get_device_capability" title="torch.cuda.get_device_capability"><code>get_device_capability</code></a>
</td> <td><p>Gets the cuda capability of a device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_device_name#torch.cuda.get_device_name" title="torch.cuda.get_device_name"><code>get_device_name</code></a>
</td> <td><p>Gets the name of a device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_device_properties#torch.cuda.get_device_properties" title="torch.cuda.get_device_properties"><code>get_device_properties</code></a>
</td> <td><p>Gets the properties of a device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_gencode_flags#torch.cuda.get_gencode_flags" title="torch.cuda.get_gencode_flags"><code>get_gencode_flags</code></a>
</td> <td><p>Returns NVCC gencode flags this library was compiled with.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_sync_debug_mode#torch.cuda.get_sync_debug_mode" title="torch.cuda.get_sync_debug_mode"><code>get_sync_debug_mode</code></a>
</td> <td><p>Returns current value of debug mode for cuda synchronizing operations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.init#torch.cuda.init" title="torch.cuda.init"><code>init</code></a>
</td> <td><p>Initialize PyTorch's CUDA state.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.ipc_collect#torch.cuda.ipc_collect" title="torch.cuda.ipc_collect"><code>ipc_collect</code></a>
</td> <td><p>Force collects GPU memory after it has been released by CUDA IPC.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.is_available#torch.cuda.is_available" title="torch.cuda.is_available"><code>is_available</code></a>
</td> <td><p>Returns a bool indicating if CUDA is currently available.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.is_initialized#torch.cuda.is_initialized" title="torch.cuda.is_initialized"><code>is_initialized</code></a>
</td> <td><p>Returns whether PyTorch's CUDA state has been initialized.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_usage#torch.cuda.memory_usage" title="torch.cuda.memory_usage"><code>memory_usage</code></a>
</td> <td><p>Returns the percent of time over the past sample period during which global (device) memory was being read or written.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_device#torch.cuda.set_device" title="torch.cuda.set_device"><code>set_device</code></a>
</td> <td><p>Sets the current device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_stream#torch.cuda.set_stream" title="torch.cuda.set_stream"><code>set_stream</code></a>
</td> <td><p>Sets the current stream.This is a wrapper API to set the stream.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_sync_debug_mode#torch.cuda.set_sync_debug_mode" title="torch.cuda.set_sync_debug_mode"><code>set_sync_debug_mode</code></a>
</td> <td><p>Sets the debug mode for cuda synchronizing operations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.stream#torch.cuda.stream" title="torch.cuda.stream"><code>stream</code></a>
</td> <td><p>Wrapper around the Context-manager StreamContext that selects a given stream.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.synchronize#torch.cuda.synchronize" title="torch.cuda.synchronize"><code>synchronize</code></a>
</td> <td><p>Waits for all kernels in all streams on a CUDA device to complete.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.utilization#torch.cuda.utilization" title="torch.cuda.utilization"><code>utilization</code></a>
</td> <td><p>Returns the percent of time over the past sample period during which one or more kernels was executing on the GPU as given by <code>nvidia-smi</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.temperature#torch.cuda.temperature" title="torch.cuda.temperature"><code>temperature</code></a>
</td> <td><p>Returns the average temperature of the GPU sensor in Degrees C (Centigrades)</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.power_draw#torch.cuda.power_draw" title="torch.cuda.power_draw"><code>power_draw</code></a>
</td> <td><p>Returns the average power draw of the GPU sensor in mW (MilliWatts)</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.clock_rate#torch.cuda.clock_rate" title="torch.cuda.clock_rate"><code>clock_rate</code></a>
</td> <td><p>Returns the clock speed of the GPU SM in Hz Hertz over the past sample period as given by <code>nvidia-smi</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.outofmemoryerror#torch.cuda.OutOfMemoryError" title="torch.cuda.OutOfMemoryError"><code>OutOfMemoryError</code></a>
</td> <td><p>Exception raised when CUDA is out of memory</p></td> </tr>  </table>  <h2 id="random-number-generator">Random Number Generator</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_rng_state#torch.cuda.get_rng_state" title="torch.cuda.get_rng_state"><code>get_rng_state</code></a>
</td> <td><p>Returns the random number generator state of the specified GPU as a ByteTensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_rng_state_all#torch.cuda.get_rng_state_all" title="torch.cuda.get_rng_state_all"><code>get_rng_state_all</code></a>
</td> <td><p>Returns a list of ByteTensor representing the random number states of all devices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_rng_state#torch.cuda.set_rng_state" title="torch.cuda.set_rng_state"><code>set_rng_state</code></a>
</td> <td><p>Sets the random number generator state of the specified GPU.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_rng_state_all#torch.cuda.set_rng_state_all" title="torch.cuda.set_rng_state_all"><code>set_rng_state_all</code></a>
</td> <td><p>Sets the random number generator state of all devices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.manual_seed#torch.cuda.manual_seed" title="torch.cuda.manual_seed"><code>manual_seed</code></a>
</td> <td><p>Sets the seed for generating random numbers for the current GPU.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.manual_seed_all#torch.cuda.manual_seed_all" title="torch.cuda.manual_seed_all"><code>manual_seed_all</code></a>
</td> <td><p>Sets the seed for generating random numbers on all GPUs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.seed#torch.cuda.seed" title="torch.cuda.seed"><code>seed</code></a>
</td> <td><p>Sets the seed for generating random numbers to a random number for the current GPU.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.seed_all#torch.cuda.seed_all" title="torch.cuda.seed_all"><code>seed_all</code></a>
</td> <td><p>Sets the seed for generating random numbers to a random number on all GPUs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.initial_seed#torch.cuda.initial_seed" title="torch.cuda.initial_seed"><code>initial_seed</code></a>
</td> <td><p>Returns the current random seed of the current GPU.</p></td> </tr>  </table>   <h2 id="communication-collectives">Communication collectives</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.comm.broadcast#torch.cuda.comm.broadcast" title="torch.cuda.comm.broadcast"><code>comm.broadcast</code></a></p></td> <td><p>Broadcasts a tensor to specified GPU devices.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.comm.broadcast_coalesced#torch.cuda.comm.broadcast_coalesced" title="torch.cuda.comm.broadcast_coalesced"><code>comm.broadcast_coalesced</code></a></p></td> <td><p>Broadcasts a sequence tensors to the specified GPUs.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.comm.reduce_add#torch.cuda.comm.reduce_add" title="torch.cuda.comm.reduce_add"><code>comm.reduce_add</code></a></p></td> <td><p>Sums tensors from multiple GPUs.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.comm.scatter#torch.cuda.comm.scatter" title="torch.cuda.comm.scatter"><code>comm.scatter</code></a></p></td> <td><p>Scatters tensor across multiple GPUs.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.comm.gather#torch.cuda.comm.gather" title="torch.cuda.comm.gather"><code>comm.gather</code></a></p></td> <td><p>Gathers tensors from multiple GPU devices.</p></td> </tr>  </table>   <h2 id="streams-and-events">Streams and events</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.stream#torch.cuda.Stream" title="torch.cuda.Stream"><code>Stream</code></a>
</td> <td><p>Wrapper around a CUDA stream.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.externalstream#torch.cuda.ExternalStream" title="torch.cuda.ExternalStream"><code>ExternalStream</code></a>
</td> <td><p>Wrapper around an externally allocated CUDA stream.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.event#torch.cuda.Event" title="torch.cuda.Event"><code>Event</code></a>
</td> <td><p>Wrapper around a CUDA event.</p></td> </tr>  </table>   <h2 id="graphs-beta">Graphs (beta)</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.is_current_stream_capturing#torch.cuda.is_current_stream_capturing" title="torch.cuda.is_current_stream_capturing"><code>is_current_stream_capturing</code></a>
</td> <td><p>Returns True if CUDA graph capture is underway on the current CUDA stream, False otherwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.graph_pool_handle#torch.cuda.graph_pool_handle" title="torch.cuda.graph_pool_handle"><code>graph_pool_handle</code></a>
</td> <td><p>Returns an opaque token representing the id of a graph memory pool.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code>CUDAGraph</code></a>
</td> <td><p>Wrapper around a CUDA graph.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.graph#torch.cuda.graph" title="torch.cuda.graph"><code>graph</code></a>
</td> <td><p>Context-manager that captures CUDA work into a <a class="reference internal" href="generated/torch.cuda.cudagraph#torch.cuda.CUDAGraph" title="torch.cuda.CUDAGraph"><code>torch.cuda.CUDAGraph</code></a> object for later replay.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.make_graphed_callables#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables</code></a>
</td> <td><p>Accepts callables (functions or <a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>nn.Module</code></a>s) and returns graphed versions.</p></td> </tr>  </table>   <h2 id="cuda-memory-management-api">Memory management</h2> <table class="autosummary longtable docutils colwidths-auto align-default" id="memory-management">  <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.empty_cache#torch.cuda.empty_cache" title="torch.cuda.empty_cache"><code>empty_cache</code></a>
</td> <td><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in <code>nvidia-smi</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.list_gpu_processes#torch.cuda.list_gpu_processes" title="torch.cuda.list_gpu_processes"><code>list_gpu_processes</code></a>
</td> <td><p>Returns a human-readable printout of the running processes and their GPU memory use for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.mem_get_info#torch.cuda.mem_get_info" title="torch.cuda.mem_get_info"><code>mem_get_info</code></a>
</td> <td><p>Returns the global free and total GPU memory for a given device using cudaMemGetInfo.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_stats#torch.cuda.memory_stats" title="torch.cuda.memory_stats"><code>memory_stats</code></a>
</td> <td><p>Returns a dictionary of CUDA memory allocator statistics for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_summary#torch.cuda.memory_summary" title="torch.cuda.memory_summary"><code>memory_summary</code></a>
</td> <td><p>Returns a human-readable printout of the current memory allocator statistics for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_snapshot#torch.cuda.memory_snapshot" title="torch.cuda.memory_snapshot"><code>memory_snapshot</code></a>
</td> <td><p>Returns a snapshot of the CUDA memory allocator state across all devices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_allocated#torch.cuda.memory_allocated" title="torch.cuda.memory_allocated"><code>memory_allocated</code></a>
</td> <td><p>Returns the current GPU memory occupied by tensors in bytes for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.max_memory_allocated#torch.cuda.max_memory_allocated" title="torch.cuda.max_memory_allocated"><code>max_memory_allocated</code></a>
</td> <td><p>Returns the maximum GPU memory occupied by tensors in bytes for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.reset_max_memory_allocated#torch.cuda.reset_max_memory_allocated" title="torch.cuda.reset_max_memory_allocated"><code>reset_max_memory_allocated</code></a>
</td> <td><p>Resets the starting point in tracking maximum GPU memory occupied by tensors for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code>memory_reserved</code></a>
</td> <td><p>Returns the current GPU memory managed by the caching allocator in bytes for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code>max_memory_reserved</code></a>
</td> <td><p>Returns the maximum GPU memory managed by the caching allocator in bytes for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.set_per_process_memory_fraction#torch.cuda.set_per_process_memory_fraction" title="torch.cuda.set_per_process_memory_fraction"><code>set_per_process_memory_fraction</code></a>
</td> <td><p>Set memory fraction for a process.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.memory_cached#torch.cuda.memory_cached" title="torch.cuda.memory_cached"><code>memory_cached</code></a>
</td> <td><p>Deprecated; see <a class="reference internal" href="generated/torch.cuda.memory_reserved#torch.cuda.memory_reserved" title="torch.cuda.memory_reserved"><code>memory_reserved()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.max_memory_cached#torch.cuda.max_memory_cached" title="torch.cuda.max_memory_cached"><code>max_memory_cached</code></a>
</td> <td><p>Deprecated; see <a class="reference internal" href="generated/torch.cuda.max_memory_reserved#torch.cuda.max_memory_reserved" title="torch.cuda.max_memory_reserved"><code>max_memory_reserved()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.reset_max_memory_cached#torch.cuda.reset_max_memory_cached" title="torch.cuda.reset_max_memory_cached"><code>reset_max_memory_cached</code></a>
</td> <td><p>Resets the starting point in tracking maximum GPU memory managed by the caching allocator for a given device.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.reset_peak_memory_stats#torch.cuda.reset_peak_memory_stats" title="torch.cuda.reset_peak_memory_stats"><code>reset_peak_memory_stats</code></a>
</td> <td><p>Resets the "peak" stats tracked by the CUDA memory allocator.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.caching_allocator_alloc#torch.cuda.caching_allocator_alloc" title="torch.cuda.caching_allocator_alloc"><code>caching_allocator_alloc</code></a>
</td> <td><p>Performs a memory allocation using the CUDA memory allocator.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.caching_allocator_delete#torch.cuda.caching_allocator_delete" title="torch.cuda.caching_allocator_delete"><code>caching_allocator_delete</code></a>
</td> <td><p>Deletes memory allocated using the CUDA memory allocator.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.get_allocator_backend#torch.cuda.get_allocator_backend" title="torch.cuda.get_allocator_backend"><code>get_allocator_backend</code></a>
</td> <td><p>Returns a string describing the active allocator backend as set by <code>PYTORCH_CUDA_ALLOC_CONF</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.cudapluggableallocator#torch.cuda.CUDAPluggableAllocator" title="torch.cuda.CUDAPluggableAllocator"><code>CUDAPluggableAllocator</code></a>
</td> <td><p>CUDA memory allocator loaded from a so file.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cuda.change_current_allocator#torch.cuda.change_current_allocator" title="torch.cuda.change_current_allocator"><code>change_current_allocator</code></a>
</td> <td><p>Changes the currently used memory allocator to be the one provided.</p></td> </tr>  </table>   <h2 id="nvidia-tools-extension-nvtx">NVIDIA Tools Extension (NVTX)</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.nvtx.mark#torch.cuda.nvtx.mark" title="torch.cuda.nvtx.mark"><code>nvtx.mark</code></a></p></td> <td><p>Describe an instantaneous event that occurred at some point.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.nvtx.range_push#torch.cuda.nvtx.range_push" title="torch.cuda.nvtx.range_push"><code>nvtx.range_push</code></a></p></td> <td><p>Pushes a range onto a stack of nested range span.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.nvtx.range_pop#torch.cuda.nvtx.range_pop" title="torch.cuda.nvtx.range_pop"><code>nvtx.range_pop</code></a></p></td> <td><p>Pops a range off of a stack of nested range spans.</p></td> </tr>  </table>   <h2 id="jiterator-beta">Jiterator (beta)</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.jiterator._create_jit_fn#torch.cuda.jiterator._create_jit_fn" title="torch.cuda.jiterator._create_jit_fn"><code>jiterator._create_jit_fn</code></a></p></td> <td><p>Create a jiterator-generated cuda kernel for an elementwise op.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.cuda.jiterator._create_multi_output_jit_fn#torch.cuda.jiterator._create_multi_output_jit_fn" title="torch.cuda.jiterator._create_multi_output_jit_fn"><code>jiterator._create_multi_output_jit_fn</code></a></p></td> <td><p>Create a jiterator-generated cuda kernel for an elementwise op that supports returning one or more outputs.</p></td> </tr>  </table>   <h2 id="stream-sanitizer-prototype">Stream Sanitizer (prototype)</h2> <p>CUDA Sanitizer is a prototype tool for detecting synchronization errors between streams in PyTorch. See the <a class="reference internal" href="cuda._sanitizer"><span class="doc">documentation</span></a> for information on how to use it.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/cuda.html" class="_attribution-link">https://pytorch.org/docs/2.1/cuda.html</a>
  </p>
</div>
