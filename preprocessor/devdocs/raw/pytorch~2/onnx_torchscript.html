<h1 id="torchscript-based-onnx-exporter">TorchScript-based ONNX Exporter</h1> <div class="admonition note"> <p class="admonition-title">Note</p> <p>To export an ONNX model using TorchDynamo instead of TorchScript, see <a class="reference internal" href="onnx_dynamo#torch.onnx.dynamo_export" title="torch.onnx.dynamo_export"><code>torch.onnx.dynamo_export()</code></a>.</p> </div>  <ul class="simple"> <li><a class="reference internal" href="#example-alexnet-from-pytorch-to-onnx" id="id4">Example: AlexNet from PyTorch to ONNX</a></li> <li><a class="reference internal" href="#tracing-vs-scripting" id="id5">Tracing vs Scripting</a></li> <li>
<p><a class="reference internal" href="#avoiding-pitfalls" id="id6">Avoiding Pitfalls</a></p> <ul> <li><a class="reference internal" href="#avoid-numpy-and-built-in-python-types" id="id7">Avoid NumPy and built-in Python types</a></li> <li><a class="reference internal" href="#avoid-tensor-data" id="id8">Avoid Tensor.data</a></li> <li><a class="reference internal" href="#avoid-in-place-operations-when-using-tensor-shape-in-tracing-mode" id="id9">Avoid in-place operations when using tensor.shape in tracing mode</a></li> </ul> </li> <li>
<p><a class="reference internal" href="#limitations" id="id10">Limitations</a></p> <ul> <li><a class="reference internal" href="#types" id="id11">Types</a></li> <li><a class="reference internal" href="#differences-in-operator-implementations" id="id12">Differences in Operator Implementations</a></li> <li>
<p><a class="reference internal" href="#unsupported-tensor-indexing-patterns" id="id13">Unsupported Tensor Indexing Patterns</a></p> <ul> <li><a class="reference internal" href="#reads-gets" id="id14">Reads / Gets</a></li> <li><a class="reference internal" href="#writes-sets" id="id15">Writes / Sets</a></li> </ul> </li> </ul> </li> <li>
<p><a class="reference internal" href="#adding-support-for-operators" id="id16">Adding support for operators</a></p> <ul> <li><a class="reference internal" href="#onnx-exporter-internals" id="id17">ONNX exporter internals</a></li> <li>
<p><a class="reference internal" href="#aten-operators" id="id18">ATen operators</a></p> <ul> <li><a class="reference internal" href="#list-of-supported-operators" id="id19">List of supported operators</a></li> <li><a class="reference internal" href="#adding-support-for-an-aten-or-quantized-operator" id="id20">Adding support for an aten or quantized operator</a></li> </ul> </li> <li>
<p><a class="reference internal" href="#torch-autograd-functions" id="id21">torch.autograd.Functions</a></p> <ul> <li><a class="reference internal" href="#static-symbolic-method" id="id22">Static Symbolic Method</a></li> <li><a class="reference internal" href="#inline-autograd-function" id="id23">Inline Autograd Function</a></li> </ul> </li> <li>
<p><a class="reference internal" href="#custom-operators" id="id24">Custom operators</a></p> <ul> <li><a class="reference internal" href="#onnx-script-functions" id="id25">ONNX-script functions</a></li> <li><a class="reference internal" href="#c-operators" id="id26">C++ Operators</a></li> </ul> </li> <li><a class="reference internal" href="#discovering-all-unconvertible-aten-ops-at-once" id="id27">Discovering all unconvertible ATen ops at once</a></li> </ul> </li> <li><a class="reference internal" href="#frequently-asked-questions" id="id28">Frequently Asked Questions</a></li> <li>
<p><a class="reference internal" href="#module-torch.onnx" id="id29">Python API</a></p> <ul> <li><a class="reference internal" href="#functions" id="id30">Functions</a></li> <li><a class="reference internal" href="#classes" id="id31">Classes</a></li> </ul> </li> </ul>   <h2 id="example-alexnet-from-pytorch-to-onnx">Example: AlexNet from PyTorch to ONNX</h2> <p>Here is a simple script which exports a pretrained AlexNet to an ONNX file named <code>alexnet.onnx</code>. The call to <code>torch.onnx.export</code> runs the model once to trace its execution and then exports the traced model to the specified file:</p> <pre data-language="python">import torch
import torchvision

dummy_input = torch.randn(10, 3, 224, 224, device="cuda")
model = torchvision.models.alexnet(pretrained=True).cuda()

# Providing input and output names sets the display names for values
# within the model's graph. Setting these does not change the semantics
# of the graph; it is only for readability.
#
# The inputs to the network consist of the flat list of inputs (i.e.
# the values you would pass to the forward() method) followed by the
# flat list of parameters. You can partially specify names, i.e. provide
# a list here shorter than the number of inputs to the model, and we will
# only set that subset of names, starting from the beginning.
input_names = [ "actual_input_1" ] + [ "learned_%d" % i for i in range(16) ]
output_names = [ "output1" ]

torch.onnx.export(model, dummy_input, "alexnet.onnx", verbose=True, input_names=input_names, output_names=output_names)
</pre> <p>The resulting <code>alexnet.onnx</code> file contains a binary <a class="reference external" href="https://developers.google.com/protocol-buffers/">protocol buffer</a> which contains both the network structure and parameters of the model you exported (in this case, AlexNet). The argument <code>verbose=True</code> causes the exporter to print out a human-readable representation of the model:</p> <pre data-language="python"># These are the inputs and parameters to the network, which have taken on
# the names we specified earlier.
graph(%actual_input_1 : Float(10, 3, 224, 224)
      %learned_0 : Float(64, 3, 11, 11)
      %learned_1 : Float(64)
      %learned_2 : Float(192, 64, 5, 5)
      %learned_3 : Float(192)
      # ---- omitted for brevity ----
      %learned_14 : Float(1000, 4096)
      %learned_15 : Float(1000)) {
  # Every statement consists of some output tensors (and their types),
  # the operator to be run (with its attributes, e.g., kernels, strides,
  # etc.), its input tensors (%actual_input_1, %learned_0, %learned_1)
  %17 : Float(10, 64, 55, 55) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=[11, 11], pads=[2, 2, 2, 2], strides=[4, 4]](%actual_input_1, %learned_0, %learned_1), scope: AlexNet/Sequential[features]/Conv2d[0]
  %18 : Float(10, 64, 55, 55) = onnx::Relu(%17), scope: AlexNet/Sequential[features]/ReLU[1]
  %19 : Float(10, 64, 27, 27) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%18), scope: AlexNet/Sequential[features]/MaxPool2d[2]
  # ---- omitted for brevity ----
  %29 : Float(10, 256, 6, 6) = onnx::MaxPool[kernel_shape=[3, 3], pads=[0, 0, 0, 0], strides=[2, 2]](%28), scope: AlexNet/Sequential[features]/MaxPool2d[12]
  # Dynamic means that the shape is not known. This may be because of a
  # limitation of our implementation (which we would like to fix in a
  # future release) or shapes which are truly dynamic.
  %30 : Dynamic = onnx::Shape(%29), scope: AlexNet
  %31 : Dynamic = onnx::Slice[axes=[0], ends=[1], starts=[0]](%30), scope: AlexNet
  %32 : Long() = onnx::Squeeze[axes=[0]](%31), scope: AlexNet
  %33 : Long() = onnx::Constant[value={9216}](), scope: AlexNet
  # ---- omitted for brevity ----
  %output1 : Float(10, 1000) = onnx::Gemm[alpha=1, beta=1, broadcast=1, transB=1](%45, %learned_14, %learned_15), scope: AlexNet/Sequential[classifier]/Linear[6]
  return (%output1);
}
</pre> <p>You can also verify the output using the <a class="reference external" href="https://github.com/onnx/onnx/">ONNX</a> library, which you can install using <code>pip</code>:</p> <pre data-language="python">pip install onnx
</pre> <p>Then, you can run:</p> <pre data-language="python">import onnx

# Load the ONNX model
model = onnx.load("alexnet.onnx")

# Check that the model is well formed
onnx.checker.check_model(model)

# Print a human readable representation of the graph
print(onnx.helper.printable_graph(model.graph))
</pre> <p>You can also run the exported model with one of the many <a class="reference external" href="https://onnx.ai/supported-tools.html#deployModel">runtimes that support ONNX</a>. For example after installing <a class="reference external" href="https://www.onnxruntime.ai">ONNX Runtime</a>, you can load and run the model:</p> <pre data-language="python">import onnxruntime as ort
import numpy as np

ort_session = ort.InferenceSession("alexnet.onnx")

outputs = ort_session.run(
    None,
    {"actual_input_1": np.random.randn(10, 3, 224, 224).astype(np.float32)},
)
print(outputs[0])
</pre> <p>Here is a more involved <a class="reference external" href="https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html">tutorial on exporting a model and running it with ONNX Runtime</a>.</p>   <h2 id="id1">Tracing vs Scripting</h2> <p id="tracing-vs-scripting">Internally, <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a> requires a <a class="reference internal" href="generated/torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>torch.jit.ScriptModule</code></a> rather than a <a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a>. If the passed-in model is not already a <code>ScriptModule</code>, <code>export()</code> will use <em>tracing</em> to convert it to one:</p> <ul class="simple"> <li>
<strong>Tracing</strong>: If <code>torch.onnx.export()</code> is called with a Module that is not already a <code>ScriptModule</code>, it first does the equivalent of <a class="reference internal" href="generated/torch.jit.trace#torch.jit.trace" title="torch.jit.trace"><code>torch.jit.trace()</code></a>, which executes the model once with the given <code>args</code> and records all operations that happen during that execution. This means that if your model is dynamic, e.g., changes behavior depending on input data, the exported model will <em>not</em> capture this dynamic behavior. We recommend examining the exported model and making sure the operators look reasonable. Tracing will unroll loops and if statements, exporting a static graph that is exactly the same as the traced run. If you want to export your model with dynamic control flow, you will need to use <em>scripting</em>.</li> <li>
<p><strong>Scripting</strong>: Compiling a model via scripting preserves dynamic control flow and is valid for inputs of different sizes. To use scripting:</p> <ul> <li>Use <a class="reference internal" href="generated/torch.jit.script#torch.jit.script" title="torch.jit.script"><code>torch.jit.script()</code></a> to produce a <code>ScriptModule</code>.</li> <li>Call <code>torch.onnx.export()</code> with the <code>ScriptModule</code> as the model. The <code>args</code> are still required, but they will be used internally only to produce example outputs, so that the types and shapes of the outputs can be captured. No tracing will be performed.</li> </ul> </li> </ul> <p>See <a class="reference external" href="https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html">Introduction to TorchScript</a> and <a class="reference external" href="jit">TorchScript</a> for more details, including how to compose tracing and scripting to suit the particular requirements of different models.</p>   <h2 id="avoiding-pitfalls">Avoiding Pitfalls</h2>  <h3 id="avoid-numpy-and-built-in-python-types">Avoid NumPy and built-in Python types</h3> <p>PyTorch models can be written using NumPy or Python types and functions, but during <a class="reference internal" href="#tracing-vs-scripting"><span class="std std-ref">tracing</span></a>, any variables of NumPy or Python types (rather than torch.Tensor) are converted to constants, which will produce the wrong result if those values should change depending on the inputs.</p> <p>For example, rather than using numpy functions on numpy.ndarrays:</p> <pre data-language="python"># Bad! Will be replaced with constants during tracing.
x, y = np.random.rand(1, 2), np.random.rand(1, 2)
np.concatenate((x, y), axis=1)
</pre> <p>Use torch operators on torch.Tensors:</p> <pre data-language="python"># Good! Tensor operations will be captured during tracing.
x, y = torch.randn(1, 2), torch.randn(1, 2)
torch.cat((x, y), dim=1)
</pre> <p>And rather than use <a class="reference internal" href="generated/torch.tensor.item#torch.Tensor.item" title="torch.Tensor.item"><code>torch.Tensor.item()</code></a> (which converts a Tensor to a Python built-in number):</p> <pre data-language="python"># Bad! y.item() will be replaced with a constant during tracing.
def forward(self, x, y):
    return x.reshape(y.item(), -1)
</pre> <p>Use torch’s support for implicit casting of single-element tensors:</p> <pre data-language="python"># Good! y will be preserved as a variable during tracing.
def forward(self, x, y):
    return x.reshape(y, -1)
</pre>   <h3 id="avoid-tensor-data">Avoid Tensor.data</h3> <p>Using the Tensor.data field can produce an incorrect trace and therefore an incorrect ONNX graph. Use <a class="reference internal" href="generated/torch.tensor.detach#torch.Tensor.detach" title="torch.Tensor.detach"><code>torch.Tensor.detach()</code></a> instead. (Work is ongoing to <a class="reference external" href="https://github.com/pytorch/pytorch/issues/30987">remove Tensor.data entirely</a>).</p>   <h3 id="avoid-in-place-operations-when-using-tensor-shape-in-tracing-mode">Avoid in-place operations when using tensor.shape in tracing mode</h3> <p>In tracing mode, shapes obtained from <code>tensor.shape</code> are traced as tensors, and share the same memory. This might cause a mismatch the final output values. As a workaround, avoid the use of inplace operations in these scenarios. For example, in the model:</p> <pre data-language="python">class Model(torch.nn.Module):
  def forward(self, states):
      batch_size, seq_length = states.shape[:2]
      real_seq_length = seq_length
      real_seq_length += 2
      return real_seq_length + seq_length
</pre> <p><code>real_seq_length</code> and <code>seq_length</code> share the same memory in tracing mode. This could be avoided by rewriting the inplace operation:</p> <pre data-language="python">real_seq_length = real_seq_length + 2
</pre>    <h2 id="limitations">Limitations</h2>  <h3 id="types">Types</h3> <ul class="simple"> <li>
<p>Only <code>torch.Tensors</code>, numeric types that can be trivially converted to torch.Tensors (e.g. float, int), and tuples and lists of those types are supported as model inputs or outputs. Dict and str inputs and outputs are accepted in <a class="reference internal" href="#tracing-vs-scripting"><span class="std std-ref">tracing</span></a> mode, but:</p> <ul> <li>Any computation that depends on the value of a dict or a str input <strong>will be replaced with the constant value</strong> seen during the one traced execution.</li> <li>Any output that is a dict will be silently replaced with a <strong>flattened sequence of its values (keys will be removed)</strong>. E.g. <code>{"foo": 1, "bar": 2}</code> becomes <code>(1, 2)</code>.</li> <li>Any output that is a str will be silently removed.</li> </ul> </li> <li>Certain operations involving tuples and lists are not supported in <a class="reference internal" href="#tracing-vs-scripting"><span class="std std-ref">scripting</span></a> mode due to limited support in ONNX for nested sequences. In particular appending a tuple to a list is not supported. In tracing mode, the nested sequences will be flattened automatically during the tracing.</li> </ul>   <h3 id="differences-in-operator-implementations">Differences in Operator Implementations</h3> <p>Due to differences in implementations of operators, running the exported model on different runtimes may produce different results from each other or from PyTorch. Normally these differences are numerically small, so this should only be a concern if your application is sensitive to these small differences.</p>   <h3 id="tensor-indexing">Unsupported Tensor Indexing Patterns</h3> <p id="unsupported-tensor-indexing-patterns">Tensor indexing patterns that cannot be exported are listed below. If you are experiencing issues exporting a model that does not include any of the unsupported patterns below, please double check that you are exporting with the latest <code>opset_version</code>.</p>  <h4 id="reads-gets">Reads / Gets</h4> <p>When indexing into a tensor for reading, the following patterns are not supported:</p> <pre data-language="python"># Tensor indices that includes negative values.
data[torch.tensor([[1, 2], [2, -3]]), torch.tensor([-2, 3])]
# Workarounds: use positive index values.
</pre>   <h4 id="writes-sets">Writes / Sets</h4> <p>When indexing into a Tensor for writing, the following patterns are not supported:</p> <pre data-language="python"># Multiple tensor indices if any has rank &gt;= 2
data[torch.tensor([[1, 2], [2, 3]]), torch.tensor([2, 3])] = new_data
# Workarounds: use single tensor index with rank &gt;= 2,
#              or multiple consecutive tensor indices with rank == 1.

# Multiple tensor indices that are not consecutive
data[torch.tensor([2, 3]), :, torch.tensor([1, 2])] = new_data
# Workarounds: transpose `data` such that tensor indices are consecutive.

# Tensor indices that includes negative values.
data[torch.tensor([1, -2]), torch.tensor([-2, 3])] = new_data
# Workarounds: use positive index values.

# Implicit broadcasting required for new_data.
data[torch.tensor([[0, 2], [1, 1]]), 1:3] = new_data
# Workarounds: expand new_data explicitly.
# Example:
#   data shape: [3, 4, 5]
#   new_data shape: [5]
#   expected new_data shape after broadcasting: [2, 2, 2, 5]
</pre>     <h2 id="adding-support-for-operators">Adding support for operators</h2> <p>When exporting a model that includes unsupported operators, you’ll see an error message like:</p> <pre data-language="text">RuntimeError: ONNX export failed: Couldn't export operator foo
</pre> <p>When that happens, there are a few things you can do:</p> <ol class="arabic simple"> <li>Change the model to not use that operator.</li> <li>Create a symbolic function to convert the operator and register it as a custom symbolic function.</li> <li>Contribute to PyTorch to add the same symbolic function to <a class="reference internal" href="#module-torch.onnx" title="torch.onnx"><code>torch.onnx</code></a> itself.</li> </ol> <p>If you decided to implement a symbolic function (we hope you will contribute it back to PyTorch!), here is how you can get started:</p>  <h3 id="onnx-exporter-internals">ONNX exporter internals</h3> <p>A “symbolic function” is a function that decomposes a PyTorch operator into a composition of a series of ONNX operators.</p> <p>During export, each node (which contains a PyTorch operator) in the TorchScript graph is visited by the exporter in topological order. Upon visiting a node, the exporter looks for a registered symbolic functions for that operator. Symbolic functions are implemented in Python. A symbolic function for an op named <code>foo</code> would look something like:</p> <pre data-language="python">def foo(
  g,
  input_0: torch._C.Value,
  input_1: torch._C.Value) -&gt; Union[None, torch._C.Value, List[torch._C.Value]]:
  """
  Adds the ONNX operations representing this PyTorch function by updating the
  graph g with `g.op()` calls.

  Args:
    g (Graph): graph to write the ONNX representation into.
    input_0 (Value): value representing the variables which contain
        the first input for this operator.
    input_1 (Value): value representing the variables which contain
        the second input for this operator.

  Returns:
    A Value or List of Values specifying the ONNX nodes that compute something
    equivalent to the original PyTorch operator with the given inputs.

    None if it cannot be converted to ONNX.
  """
  ...
</pre> <p>The <code>torch._C</code> types are Python wrappers around the types defined in C++ in <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/jit/ir/ir.h">ir.h</a>.</p> <p>The process for adding a symbolic function depends on the type of operator.</p>   <h3 id="adding-support-aten">ATen operators</h3> <p id="aten-operators"><a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen</a> is PyTorch’s built-in tensor library. If the operator is an ATen operator (shows up in the TorchScript graph with the prefix <code>aten::</code>), make sure it is not supported already.</p>  <h4 id="list-of-supported-operators">List of supported operators</h4> <p>Visit the auto generated <a class="reference internal" href="onnx_torchscript_supported_aten_ops"><span class="doc">list of supported TorchScript operators</span></a> for details on which operator are supported in each <code>opset_version</code>.</p>   <h4 id="adding-support-for-an-aten-or-quantized-operator">Adding support for an aten or quantized operator</h4> <p>If the operator is not in the list above:</p> <ul class="simple"> <li>Define the symbolic function in <code>torch/onnx/symbolic_opset&lt;version&gt;.py</code>, for example <a class="reference external" href="https://github.com/pytorch/pytorch/blob/main/torch/onnx/symbolic_opset9.py">torch/onnx/symbolic_opset9.py</a>. Make sure the function has the same name as the ATen function, which may be declared in <code>torch/_C/_VariableFunctions.pyi</code> or <code>torch/nn/functional.pyi</code> (these files are generated at build time, so will not appear in your checkout until you build PyTorch).</li> <li>By default, the first arg is the ONNX graph. Other arg names must EXACTLY match the names in the <code>.pyi</code> file, because dispatch is done with keyword arguments.</li> <li>In the symbolic function, if the operator is in the <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md">ONNX standard operator set</a>, we only need to create a node to represent the ONNX operator in the graph. If not, we can compose several standard operators that have the equivalent semantics to the ATen operator.</li> </ul> <p>Here is an example of handling missing symbolic function for the <code>ELU</code> operator.</p> <p>If we run the following code:</p> <pre data-language="python">print(
    torch.jit.trace(
        torch.nn.ELU(), # module
        torch.ones(1)   # example input
    ).graph
)
</pre> <p>We see something like:</p> <pre data-language="python">graph(%self : __torch__.torch.nn.modules.activation.___torch_mangle_0.ELU,
      %input : Float(1, strides=[1], requires_grad=0, device=cpu)):
  %4 : float = prim::Constant[value=1.]()
  %5 : int = prim::Constant[value=1]()
  %6 : int = prim::Constant[value=1]()
  %7 : Float(1, strides=[1], requires_grad=0, device=cpu) = aten::elu(%input, %4, %5, %6)
  return (%7)
</pre> <p>Since we see <code>aten::elu</code> in the graph, we know this is an ATen operator.</p> <p>We check the <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md">ONNX operator list</a>, and confirm that <code>Elu</code> is standardized in ONNX.</p> <p>We find a signature for <code>elu</code> in <code>torch/nn/functional.pyi</code>:</p> <pre data-language="python">def elu(input: Tensor, alpha: float = ..., inplace: bool = ...) -&gt; Tensor: ...
</pre> <p>We add the following lines to <code>symbolic_opset9.py</code>:</p> <pre data-language="python">def elu(g, input: torch.Value, alpha: torch.Value, inplace: bool = False):
    return g.op("Elu", input, alpha_f=alpha)
</pre> <p>Now PyTorch is able to export models containing the <code>aten::elu</code> operator!</p> <p>See the <code>torch/onnx/symbolic_opset*.py</code> files for more examples.</p>    <h3 id="torch-autograd-functions">torch.autograd.Functions</h3> <p>If the operator is a sub-class of <a class="reference internal" href="autograd#torch.autograd.Function" title="torch.autograd.Function"><code>torch.autograd.Function</code></a>, there are three ways to export it.</p>  <h4 id="static-symbolic-method">Static Symbolic Method</h4> <p>You can add a static method named <code>symbolic</code> to your function class. It should return ONNX operators that represent the function’s behavior in ONNX. For example:</p> <pre data-language="python">class MyRelu(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor) -&gt; torch.Tensor:
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    @staticmethod
    def symbolic(g: torch.Graph, input: torch.Value) -&gt; torch.Value:
        return g.op("Clip", input, g.op("Constant", value_t=torch.tensor(0, dtype=torch.float)))
</pre>   <h4 id="inline-autograd-function">Inline Autograd Function</h4> <p>In cases where a static symbolic method is not provided for its subsequent <a class="reference internal" href="autograd#torch.autograd.Function" title="torch.autograd.Function"><code>torch.autograd.Function</code></a> or where a function to register <code>prim::PythonOp</code> as custom symbolic functions is not provided, <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a> tries to inline the graph that corresponds to that <a class="reference internal" href="autograd#torch.autograd.Function" title="torch.autograd.Function"><code>torch.autograd.Function</code></a> such that this function is broken down into individual operators that were used within the function. The export should be successful as long as these individual operators are supported. For example:</p> <pre data-language="python">class MyLogExp(torch.autograd.Function):
    @staticmethod
    def forward(ctx, input: torch.Tensor) -&gt; torch.Tensor:
        ctx.save_for_backward(input)
        h = input.exp()
        return h.log().log()
</pre> <p>There is no static symbolic method present for this model, yet it is exported as follows:</p> <pre data-language="python">graph(%input : Float(1, strides=[1], requires_grad=0, device=cpu)):
    %1 : float = onnx::Exp[](%input)
    %2 : float = onnx::Log[](%1)
    %3 : float = onnx::Log[](%2)
    return (%3)
</pre> <p>If you need to avoid inlining of <a class="reference internal" href="autograd#torch.autograd.Function" title="torch.autograd.Function"><code>torch.autograd.Function</code></a>, you should export models with <code>operator_export_type</code> set to <code>ONNX_FALLTHROUGH</code> or <code>ONNX_ATEN_FALLBACK</code>.</p>    <h3 id="custom-operators">Custom operators</h3> <p>You can export your model with custom operators that includes a combination of many standard ONNX ops, or are driven by self-defined C++ backend.</p>  <h4 id="onnx-script-functions">ONNX-script functions</h4> <p>If an operator is not a standard ONNX op, but can be composed of multiple existing ONNX ops, you can utilize <a class="reference external" href="https://github.com/microsoft/onnx-script">ONNX-script</a> to create an external ONNX function to support the operator. You can export it by following this example:</p> <pre data-language="python">import onnxscript
# There are three opset version needed to be aligned
# This is (1) the opset version in ONNX function
from onnxscript.onnx_opset import opset15 as op
opset_version = 15

x = torch.randn(1, 2, 3, 4, requires_grad=True)
model = torch.nn.SELU()

custom_opset = onnxscript.values.Opset(domain="onnx-script", version=1)

@onnxscript.script(custom_opset)
def Selu(X):
    alpha = 1.67326  # auto wrapped as Constants
    gamma = 1.0507
    alphaX = op.CastLike(alpha, X)
    gammaX = op.CastLike(gamma, X)
    neg = gammaX * (alphaX * op.Exp(X) - alphaX)
    pos = gammaX * X
    zero = op.CastLike(0, X)
    return op.Where(X &lt;= zero, neg, pos)

# setType API provides shape/type to ONNX shape/type inference
def custom_selu(g: jit_utils.GraphContext, X):
    return g.onnxscript_op(Selu, X).setType(X.type())

# Register custom symbolic function
# There are three opset version needed to be aligned
# This is (2) the opset version in registry
torch.onnx.register_custom_op_symbolic(
    symbolic_name="aten::selu",
    symbolic_fn=custom_selu,
    opset_version=opset_version,
)

# There are three opset version needed to be aligned
# This is (2) the opset version in exporter
torch.onnx.export(
    model,
    x,
    "model.onnx",
    opset_version=opset_version,
    # only needed if you want to specify an opset version &gt; 1.
    custom_opsets={"onnx-script": 2}
)
</pre> <p>The example above exports it as a custom operator in the “onnx-script” opset. When exporting a custom operator, you can specify the custom domain version using the <code>custom_opsets</code> dictionary at export. If not specified, the custom opset version defaults to 1.</p> <p>NOTE: Be careful to align the opset version mentioned in the above example, and make sure they are consumed in exporter step. The example usage of how to write a onnx-script function is a beta version in terms of the active development on onnx-script. Please follow the latest <a class="reference external" href="https://github.com/microsoft/onnx-script">ONNX-script</a></p>   <h4 id="c-operators">C++ Operators</h4> <p>If a model uses a custom operator implemented in C++ as described in <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">Extending TorchScript with Custom C++ Operators</a>, you can export it by following this example:</p> <pre data-language="python">from torch.onnx import symbolic_helper


# Define custom symbolic function
@symbolic_helper.parse_args("v", "v", "f", "i")
def symbolic_foo_forward(g, input1, input2, attr1, attr2):
    return g.op("custom_domain::Foo", input1, input2, attr1_f=attr1, attr2_i=attr2)


# Register custom symbolic function
torch.onnx.register_custom_op_symbolic("custom_ops::foo_forward", symbolic_foo_forward, 9)


class FooModel(torch.nn.Module):
    def __init__(self, attr1, attr2):
        super().__init__()
        self.attr1 = attr1
        self.attr2 = attr2

    def forward(self, input1, input2):
        # Calling custom op
        return torch.ops.custom_ops.foo_forward(input1, input2, self.attr1, self.attr2)


model = FooModel(attr1, attr2)
torch.onnx.export(
    model,
    (example_input1, example_input1),
    "model.onnx",
    # only needed if you want to specify an opset version &gt; 1.
    custom_opsets={"custom_domain": 2}
)
</pre> <p>The example above exports it as a custom operator in the “custom_domain” opset. When exporting a custom operator, you can specify the custom domain version using the <code>custom_opsets</code> dictionary at export. If not specified, the custom opset version defaults to 1.</p> <p>The runtime that consumes the model needs to support the custom op. See <a class="reference external" href="https://caffe2.ai/docs/custom-operators.html">Caffe2 custom ops</a>, <a class="reference external" href="https://onnxruntime.ai/docs/reference/operators/add-custom-op.html">ONNX Runtime custom ops</a>, or your runtime of choice’s documentation.</p>    <h3 id="discovering-all-unconvertible-aten-ops-at-once">Discovering all unconvertible ATen ops at once</h3> <p>When export fails due to an unconvertible ATen op, there may in fact be more than one such op but the error message only mentions the first. To discover all of the unconvertible ops in one go you can:</p> <pre data-language="python"># prepare model, args, opset_version
...

torch_script_graph, unconvertible_ops = torch.onnx.utils.unconvertible_ops(
    model, args, opset_version=opset_version
)

print(set(unconvertible_ops))
</pre> <p>The set is approximated because some ops may be removed during the conversion process and don’t need to be converted. Some other ops may have partial support that will fail conversion with particular inputs, but this should give you a general idea of what ops are not supported. Please feel free to open GitHub Issues for op support requests.</p>    <h2 id="frequently-asked-questions">Frequently Asked Questions</h2> <p>Q: I have exported my LSTM model, but its input size seems to be fixed?</p>  <p>The tracer records the shapes of the example inputs. If the model should accept inputs of dynamic shapes, set <code>dynamic_axes</code> when calling <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</p>  <p>Q: How to export models containing loops?</p>  <p>See <a class="reference internal" href="#id1">Tracing vs Scripting</a>.</p>  <p>Q: How to export models with primitive type inputs (e.g. int, float)?</p>  <p>Support for primitive numeric type inputs was added in PyTorch 1.9. However, the exporter does not support models with str inputs.</p>  <p>Q: Does ONNX support implicit scalar datatype casting?</p>  <p>The ONNX standard does not, but the exporter will try to handle that part. Scalars are exported as constant tensors. The exporter will figure out the right data type for scalars. In rare cases when it is unable to do so, you will need to manually specify the datatype with e.g. <code>dtype=torch.float32</code>. If you see any errors, please [create a GitHub issue](<a class="reference external" href="https://github.com/pytorch/pytorch/issues">https://github.com/pytorch/pytorch/issues</a>).</p>  <p>Q: Are lists of Tensors exportable to ONNX?</p>  <p>Yes, for <code>opset_version</code> &gt;= 11, since ONNX introduced the Sequence type in opset 11.</p>    <h2 id="python-api">Python API</h2>  <h3 id="module-torch.onnx">Functions</h3> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.export">
<code>torch.onnx.export(model, args, f, export_params=True, verbose=False, training=&lt;TrainingMode.EVAL: 0&gt;, input_names=None, output_names=None, operator_export_type=&lt;OperatorExportTypes.ONNX: 0&gt;, opset_version=None, do_constant_folding=True, dynamic_axes=None, keep_initializers_as_inputs=None, custom_opsets=None, export_modules_as_functions=False, autograd_inlining=True)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#export"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Exports a model into ONNX format.</p> <p>If <code>model</code> is not a <a class="reference internal" href="generated/torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>torch.jit.ScriptModule</code></a> nor a <a class="reference internal" href="generated/torch.jit.scriptfunction#torch.jit.ScriptFunction" title="torch.jit.ScriptFunction"><code>torch.jit.ScriptFunction</code></a>, this runs <code>model</code> once in order to convert it to a TorchScript graph to be exported (the equivalent of <a class="reference internal" href="generated/torch.jit.trace#torch.jit.trace" title="torch.jit.trace"><code>torch.jit.trace()</code></a>). Thus this has the same limited support for dynamic control flow as <a class="reference internal" href="generated/torch.jit.trace#torch.jit.trace" title="torch.jit.trace"><code>torch.jit.trace()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> (<a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a>, <a class="reference internal" href="generated/torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>torch.jit.ScriptModule</code></a> or <a class="reference internal" href="generated/torch.jit.scriptfunction#torch.jit.ScriptFunction" title="torch.jit.ScriptFunction"><code>torch.jit.ScriptFunction</code></a>) – the model to be exported.</li> <li>
<p><strong>args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> or </em><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">torch.Tensor</a>) – </p>
<p>args can be structured either as:</p> <ol class="arabic"> <li>
<p>ONLY A TUPLE OF ARGUMENTS:</p> <pre data-language="python">args = (x, y, z)
</pre> </li> </ol> <p>The tuple should contain model inputs such that <code>model(*args)</code> is a valid invocation of the model. Any non-Tensor arguments will be hard-coded into the exported model; any Tensor arguments will become inputs of the exported model, in the order they occur in the tuple.</p> <ol class="arabic" start="2"> <li>
<p>A TENSOR:</p> <pre data-language="python">args = torch.Tensor([1])
</pre> </li> </ol> <p>This is equivalent to a 1-ary tuple of that Tensor.</p> <ol class="arabic" start="3"> <li>
<p>A TUPLE OF ARGUMENTS ENDING WITH A DICTIONARY OF NAMED ARGUMENTS:</p> <pre data-language="python">args = (
    x,
    {
        "y": input_y,
        "z": input_z
    }
)
</pre> </li> </ol> <p>All but the last element of the tuple will be passed as non-keyword arguments, and named arguments will be set from the last element. If a named argument is not present in the dictionary, it is assigned the default value, or None if a default value is not provided.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If a dictionary is the last element of the args tuple, it will be interpreted as containing named arguments. In order to pass a dict as the last non-keyword arg, provide an empty dict as the last element of the args tuple. For example, instead of:</p> <pre data-language="python">torch.onnx.export(
    model,
    (
        x,
        # WRONG: will be interpreted as named arguments
        {y: z}
    ),
    "test.onnx.pb"
)
</pre> <p>Write:</p> <pre data-language="python">torch.onnx.export(
    model,
    (
        x,
        {y: z},
        {}
    ),
    "test.onnx.pb"
)
</pre> </div> </li> <li>
<strong>f</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>BytesIO</em><em>]</em>) – a file-like object (such that <code>f.fileno()</code> returns a file descriptor) or a string containing a file name. A binary protocol buffer will be written to this file.</li> <li>
<strong>export_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default True</em>) – if True, all parameters will be exported. Set this to False if you want to export an untrained model. In this case, the exported model will first take all of its parameters as arguments, with the ordering as specified by <code>model.state_dict().values()</code>
</li> <li>
<strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default False</em>) – if True, prints a description of the model being exported to stdout. In addition, the final ONNX graph will include the field <code>doc_string`</code> from the exported model which mentions the source code locations for <code>model</code>. If True, ONNX exporter logging will be turned on.</li> <li>
<p><strong>training</strong> (<em>enum</em><em>, </em><em>default TrainingMode.EVAL</em>) – </p>
<ul> <li>
<code>TrainingMode.EVAL</code>: export the model in inference mode.</li> <li>
<dl class="simple"> <dt>
<code>TrainingMode.PRESERVE: export the model in inference mode if model.training is</code> </dt>
<dd>
<p>False and in training mode if model.training is True.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>TrainingMode.TRAINING: export the model in training mode. Disables optimizations</code> </dt>
<dd>
<p>which might interfere with training.</p> </dd> </dl> </li> </ul> </li> <li>
<strong>input_names</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>default empty list</em>) – names to assign to the input nodes of the graph, in order.</li> <li>
<strong>output_names</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>default empty list</em>) – names to assign to the output nodes of the graph, in order.</li> <li>
<p><strong>operator_export_type</strong> (<em>enum</em><em>, </em><em>default OperatorExportTypes.ONNX</em>) – </p>
<ul> <li>
<dl class="simple"> <dt>
<code>OperatorExportTypes.ONNX: Export all ops as regular ONNX ops</code> </dt>
<dd>
<p>(in the default opset domain).</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>
<code>OperatorExportTypes.ONNX_FALLTHROUGH: Try to convert all ops</code> </dt>
<dd>
<p>to standard ONNX ops in the default opset domain. If unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting the op into a custom opset domain without conversion. Applies to <a class="reference external" href="https://pytorch.org/tutorials/advanced/torch_script_custom_ops.html">custom ops</a> as well as ATen ops. For the exported model to be usable, the runtime must support these non-standard ops.</p> </dd> </dl> </li> <li>
<dl> <dt>
<code>OperatorExportTypes.ONNX_ATEN: All ATen ops (in the TorchScript namespace “aten”)</code> </dt>
<dd>
<p>are exported as ATen ops (in opset domain “org.pytorch.aten”). <a class="reference external" href="https://pytorch.org/cppdocs/#aten">ATen</a> is PyTorch’s built-in tensor library, so this instructs the runtime to use PyTorch’s implementation of these ops.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Models exported this way are probably runnable only by Caffe2.</p> <p>This may be useful if the numeric differences in implementations of operators are causing large differences in behavior between PyTorch and Caffe2 (which is more common on untrained models).</p> </div> </dd> </dl> </li> <li>
<dl> <dt>
<code>OperatorExportTypes.ONNX_ATEN_FALLBACK: Try to export each ATen op</code> </dt>
<dd>
<p>(in the TorchScript namespace “aten”) as a regular ONNX op. If we are unable to do so (e.g. because support has not been added to convert a particular torch op to ONNX), fall back to exporting an ATen op. See documentation on OperatorExportTypes.ONNX_ATEN for context. For example:</p> <pre data-language="python">graph(%0 : Float):
%3 : int = prim::Constant[value=0]()
# conversion unsupported
%4 : Float = aten::triu(%0, %3)
# conversion supported
%5 : Float = aten::mul(%4, %0)
return (%5)
</pre> <p>Assuming <code>aten::triu</code> is not supported in ONNX, this will be exported as:</p> <pre data-language="python">graph(%0 : Float):
%1 : Long() = onnx::Constant[value={0}]()
# not converted
%2 : Float = aten::ATen[operator="triu"](%0, %1)
# converted
%3 : Float = onnx::Mul(%2, %0)
return (%3)
</pre> <p>If PyTorch was built with Caffe2 (i.e. with <code>BUILD_CAFFE2=1</code>), then Caffe2-specific behavior will be enabled, including special support for ops are produced by the modules described in <a class="reference external" href="https://pytorch.org/docs/stable/quantization.html">Quantization</a>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Models exported this way are probably runnable only by Caffe2.</p> </div> </dd> </dl> </li> </ul> </li> <li>
<strong>opset_version</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>default 17</em>) – The version of the <a class="reference external" href="https://github.com/onnx/onnx/blob/master/docs/Operators.md">default (ai.onnx) opset</a> to target. Must be &gt;= 7 and &lt;= 17.</li> <li>
<strong>do_constant_folding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default True</em>) – Apply the constant-folding optimization. Constant-folding will replace some of the ops that have all constant inputs with pre-computed constant nodes.</li> <li>
<p><strong>dynamic_axes</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a><em>[</em><em>string</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>string</em><em>]</em><em>] or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a><em>[</em><em>string</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a><em>(</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>)</em><em>]</em><em>, </em><em>default empty dict</em>) – </p>
<p>By default the exported model will have the shapes of all input and output tensors set to exactly match those given in <code>args</code>. To specify axes of tensors as dynamic (i.e. known only at run-time), set <code>dynamic_axes</code> to a dict with schema:</p> <ul> <li>
<dl class="simple"> <dt>
<code>KEY (str): an input or output name. Each name must also be provided in input_names or</code> </dt>
<dd>
<p><code>output_names</code>.</p> </dd> </dl> </li> <li>
<dl class="simple"> <dt>VALUE (dict or list): If a dict, keys are axis indices and values are axis names. If a</dt>
<dd>
<p>list, each element is an axis index.</p> </dd> </dl> </li> </ul> <p>For example:</p> <pre data-language="python">class SumModule(torch.nn.Module):
    def forward(self, x):
        return torch.sum(x, dim=1)

torch.onnx.export(
    SumModule(),
    (torch.ones(2, 2),),
    "onnx.pb",
    input_names=["x"],
    output_names=["sum"]
)
</pre> <p>Produces:</p> <pre data-language="python">input {
  name: "x"
  ...
      shape {
        dim {
          dim_value: 2  # axis 0
        }
        dim {
          dim_value: 2  # axis 1
...
output {
  name: "sum"
  ...
      shape {
        dim {
          dim_value: 2  # axis 0
...
</pre> <p>While:</p> <pre data-language="python">torch.onnx.export(
    SumModule(),
    (torch.ones(2, 2),),
    "onnx.pb",
    input_names=["x"],
    output_names=["sum"],
    dynamic_axes={
        # dict value: manually named axes
        "x": {0: "my_custom_axis_name"},
        # list value: automatic names
        "sum": [0],
    }
)
</pre> <p>Produces:</p> <pre data-language="python">input {
  name: "x"
  ...
      shape {
        dim {
          dim_param: "my_custom_axis_name"  # axis 0
        }
        dim {
          dim_value: 2  # axis 1
...
output {
  name: "sum"
  ...
      shape {
        dim {
          dim_param: "sum_dynamic_axes_1"  # axis 0
...
</pre> </li> <li>
<p><strong>keep_initializers_as_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default None</em>) – </p>
<p>If True, all the initializers (typically corresponding to parameters) in the exported graph will also be added as inputs to the graph. If False, then initializers are not added as inputs to the graph, and only the non-parameter inputs are added as inputs. This may allow for better optimizations (e.g. constant folding) by backends/runtimes.</p> <p>If True, <code>deduplicate_initializers</code> pass will not be executed. This means initializers with duplicated values will not be deduplicated and will be treated as distinct inputs to the graph. This allows different input initializers to be supplied at the runtime following export.</p> <p>If <code>opset_version &lt; 9</code>, initializers MUST be part of graph inputs and this argument will be ignored and the behavior will be equivalent to setting this argument to True.</p> <p>If None, then the behavior is chosen automatically as follows:</p> <ul> <li>
<dl class="simple"> <dt>
<code>If operator_export_type=OperatorExportTypes.ONNX, the behavior is equivalent</code> </dt>
<dd>
<p>to setting this argument to False.</p> </dd> </dl> </li> <li>Else, the behavior is equivalent to setting this argument to True.</li> </ul> </li> <li>
<p><strong>custom_opsets</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>]</em><em>, </em><em>default empty dict</em>) – </p>
<p>A dict with schema:</p> <ul> <li>KEY (str): opset domain name</li> <li>VALUE (int): opset version</li> </ul> <p>If a custom opset is referenced by <code>model</code> but not mentioned in this dictionary, the opset version is set to 1. Only custom opset domain name and version should be indicated through this argument.</p> </li> <li>
<p><strong>export_modules_as_functions</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#set" title="(in Python v3.12)">set</a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.12)">type</a><em> of </em><a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.Module">nn.Module</a><em>, </em><em>default False</em>) – </p>
<p>Flag to enable exporting all <code>nn.Module</code> forward calls as local functions in ONNX. Or a set to indicate the particular types of modules to export as local functions in ONNX. This feature requires <code>opset_version</code> &gt;= 15, otherwise the export will fail. This is because <code>opset_version</code> &lt; 15 implies IR version &lt; 8, which means no local function support. Module variables will be exported as function attributes. There are two categories of function attributes.</p> <p>1. Annotated attributes: class variables that have type annotations via <a class="reference external" href="https://www.python.org/dev/peps/pep-0526/#class-and-instance-variable-annotations">PEP 526-style</a> will be exported as attributes. Annotated attributes are not used inside the subgraph of ONNX local function because they are not created by PyTorch JIT tracing, but they may be used by consumers to determine whether or not to replace the function with a particular fused kernel.</p> <p>2. Inferred attributes: variables that are used by operators inside the module. Attribute names will have prefix “inferred::”. This is to differentiate from predefined attributes retrieved from python module annotations. Inferred attributes are used inside the subgraph of ONNX local function.</p> <ul> <li>
<code>False</code> (default): export <code>nn.Module</code> forward calls as fine grained nodes.</li> <li>
<code>True</code>: export all <code>nn.Module</code> forward calls as local function nodes.</li> <li>
<dl class="simple"> <dt>
<code>Set of type of nn.Module: export nn.Module forward calls as local function nodes,</code> </dt>
<dd>
<p>only if the type of the <code>nn.Module</code> is found in the set.</p> </dd> </dl> </li> </ul> </li> <li>
<strong>autograd_inlining</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default True</em>) – Flag used to control whether to inline autograd functions. Refer to <a class="reference external" href="https://github.com/pytorch/pytorch/pull/74765">https://github.com/pytorch/pytorch/pull/74765</a> for more details.</li> </ul> </dd> <dt class="field-even">Raises</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>torch.onnx.errors.CheckerError</strong> – If the ONNX checker detects an invalid ONNX graph.</li> <li>
<strong>torch.onnx.errors.UnsupportedOperatorError</strong> – If the ONNX graph cannot be exported because it uses an operator that is not supported by the exporter.</li> <li>
<strong>torch.onnx.errors.OnnxExporterError</strong> – Other errors that can occur during export. All errors are subclasses of <code>errors.OnnxExporterError</code>.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.export_to_pretty_string">
<code>torch.onnx.export_to_pretty_string(model, args, export_params=True, verbose=False, training=&lt;TrainingMode.EVAL: 0&gt;, input_names=None, output_names=None, operator_export_type=&lt;OperatorExportTypes.ONNX: 0&gt;, export_type=None, google_printer=False, opset_version=None, keep_initializers_as_inputs=None, custom_opsets=None, add_node_names=True, do_constant_folding=True, dynamic_axes=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#export_to_pretty_string"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Similar to <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>export()</code></a>, but returns a text representation of the ONNX model. Only differences in args listed below. All other args are the same as <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>export()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>add_node_names</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default True</em>) – Whether or not to set NodeProto.name. This makes no difference unless <code>google_printer=True</code>.</li> <li>
<strong>google_printer</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>default False</em>) – If False, will return a custom, compact representation of the model. If True will return the protobuf’s <code>Message::DebugString()</code>, which is more verbose.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A UTF-8 str containing a human-readable representation of the ONNX model.</p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.register_custom_op_symbolic">
<code>torch.onnx.register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#register_custom_op_symbolic"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Registers a symbolic function for a custom operator.</p> <p>When the user registers symbolic for custom/contrib ops, it is highly recommended to add shape inference for that operator via setType API, otherwise the exported graph may have incorrect shape inference in some extreme cases. An example of setType is <code>test_aten_embedding_2</code> in <code>test_operators.py</code>.</p> <p>See “Custom Operators” in the module documentation for an example usage.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>symbolic_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – The name of the custom operator in “&lt;domain&gt;::&lt;op&gt;” format.</li> <li>
<strong>symbolic_fn</strong> (<em>Callable</em>) – A function that takes in the ONNX graph and the input arguments to the current operator, and returns new operator nodes to add to the graph.</li> <li>
<strong>opset_version</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The ONNX opset version in which to register.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.unregister_custom_op_symbolic">
<code>torch.onnx.unregister_custom_op_symbolic(symbolic_name, opset_version)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#unregister_custom_op_symbolic"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Unregisters <code>symbolic_name</code>.</p> <p>See “Custom Operators” in the module documentation for an example usage.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>symbolic_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – The name of the custom operator in “&lt;domain&gt;::&lt;op&gt;” format.</li> <li>
<strong>opset_version</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The ONNX opset version in which to unregister.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.select_model_mode_for_export">
<code>torch.onnx.select_model_mode_for_export(model, mode)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#select_model_mode_for_export"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A context manager to temporarily set the training mode of <code>model</code> to <code>mode</code>, resetting it when we exit the with-block.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> – Same type and meaning as <code>model</code> arg to <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>export()</code></a>.</li> <li>
<strong>mode</strong> (<em>TrainingMode</em>) – Same type and meaning as <code>training</code> arg to <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>export()</code></a>.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.is_in_onnx_export">
<code>torch.onnx.is_in_onnx_export()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/utils.html#is_in_onnx_export"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns whether it is in the middle of ONNX export.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.enable_log">
<code>torch.onnx.enable_log()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx.html#enable_log"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Enables ONNX logging.</p>  </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.disable_log">
<code>torch.onnx.disable_log()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx.html#disable_log"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Disables ONNX logging.</p>  </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.onnx.verification.find_mismatch">
<code>torch.onnx.verification.find_mismatch(model, input_args, do_constant_folding=True, training=&lt;TrainingMode.EVAL: 0&gt;, opset_version=None, keep_initializers_as_inputs=True, verbose=False, options=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/onnx/verification.html#find_mismatch"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Find all mismatches between the original model and the exported model.</p> <p>Experimental. The API is subject to change.</p> <p>This tool helps debug the mismatch between the original PyTorch model and exported ONNX model. It binary searches the model graph to find the minimal subgraph that exhibits the mismatch.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>model</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a><em>[</em><a class="reference internal" href="generated/torch.nn.module#torch.nn.Module" title="torch.nn.modules.module.Module">Module</a><em>, </em><a class="reference internal" href="generated/torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit._script.ScriptModule">ScriptModule</a><em>]</em>) – The model to be exported.</li> <li>
<strong>input_args</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a><em>, </em><em>...</em><em>]</em>) – The input arguments to the model.</li> <li>
<strong>do_constant_folding</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Same as <code>do_constant_folding</code> in <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</li> <li>
<strong>training</strong> (<em>TrainingMode</em>) – Same as <code>training</code> in <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</li> <li>
<strong>opset_version</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>]</em>) – Same as <code>opset_version</code> in <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</li> <li>
<strong>keep_initializers_as_inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Same as <code>keep_initializers_as_inputs</code> in <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</li> <li>
<strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Same as <code>verbose</code> in <a class="reference internal" href="#torch.onnx.export" title="torch.onnx.export"><code>torch.onnx.export()</code></a>.</li> <li>
<strong>options</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="generated/torch.onnx.verification.verificationoptions#torch.onnx.verification.VerificationOptions" title="torch.onnx.verification.VerificationOptions">VerificationOptions</a><em>]</em>) – The options for the mismatch verification.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A GraphInfo object that contains the mismatch information.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">


<a class="reference internal" href="generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo" title="torch.onnx.verification.GraphInfo">GraphInfo</a> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; import torch
&gt;&gt;&gt; import torch.onnx.verification
&gt;&gt;&gt; torch.manual_seed(0)
&gt;&gt;&gt; opset_version = 15
&gt;&gt;&gt; # Define a custom symbolic function for aten::relu.
&gt;&gt;&gt; # The custom symbolic function is incorrect, which will result in mismatches.
&gt;&gt;&gt; def incorrect_relu_symbolic_function(g, self):
...     return self
&gt;&gt;&gt; torch.onnx.register_custom_op_symbolic(
...     "aten::relu",
...     incorrect_relu_symbolic_function,
...     opset_version=opset_version,
... )
&gt;&gt;&gt; class Model(torch.nn.Module):
...     def __init__(self):
...         super().__init__()
...         self.layers = torch.nn.Sequential(
...             torch.nn.Linear(3, 4),
...             torch.nn.ReLU(),
...             torch.nn.Linear(4, 5),
...             torch.nn.ReLU(),
...             torch.nn.Linear(5, 6),
...         )
...     def forward(self, x):
...         return self.layers(x)
&gt;&gt;&gt; graph_info = torch.onnx.verification.find_mismatch(
...     Model(),
...     (torch.randn(2, 3),),
...     opset_version=opset_version,
... )
===================== Mismatch info for graph partition : ======================
================================ Mismatch error ================================
Tensor-likes are not close!
Mismatched elements: 12 / 12 (100.0%)
Greatest absolute difference: 0.2328854203224182 at index (1, 2) (up to 1e-07 allowed)
Greatest relative difference: 0.699536174352349 at index (1, 3) (up to 0.001 allowed)
==================================== Tree: =====================================
5 X   __2 X    __1 ✓
id:  |  id: 0 |  id: 00
     |        |
     |        |__1 X (aten::relu)
     |           id: 01
     |
     |__3 X    __1 ✓
        id: 1 |  id: 10
              |
              |__2 X     __1 X (aten::relu)
                 id: 11 |  id: 110
                        |
                        |__1 ✓
                           id: 111
=========================== Mismatch leaf subgraphs: ===========================
['01', '110']
============================= Mismatch node kinds: =============================
{'aten::relu': 2}
</pre> </dd>
</dl>   <h3 id="classes">Classes</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.onnx.jitscalartype#torch.onnx.JitScalarType" title="torch.onnx.JitScalarType"><code>JitScalarType</code></a>
</td> <td><p>Scalar types defined in torch.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.onnx.verification.graphinfo#torch.onnx.verification.GraphInfo" title="torch.onnx.verification.GraphInfo"><code>torch.onnx.verification.GraphInfo</code></a>
</td> <td><p>GraphInfo contains validation information of a TorchScript graph and its converted ONNX graph.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.onnx.verification.verificationoptions#torch.onnx.verification.VerificationOptions" title="torch.onnx.verification.VerificationOptions"><code>torch.onnx.verification.VerificationOptions</code></a>
</td> <td><p>Options for ONNX export verification.</p></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/onnx_torchscript.html" class="_attribution-link">https://pytorch.org/docs/2.1/onnx_torchscript.html</a>
  </p>
</div>
