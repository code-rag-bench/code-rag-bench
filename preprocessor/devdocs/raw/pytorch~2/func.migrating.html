<h1 id="migrating-from-functorch-to-torch-func">Migrating from functorch to torch.func</h1> <p>torch.func, previously known as “functorch”, is <a class="reference external" href="https://github.com/google/jax">JAX-like</a> composable function transforms for PyTorch.</p> <p>functorch started as an out-of-tree library over at the <a class="reference external" href="https://github.com/pytorch/functorch">pytorch/functorch</a> repository. Our goal has always been to upstream functorch directly into PyTorch and provide it as a core PyTorch library.</p> <p>As the final step of the upstream, we’ve decided to migrate from being a top level package (<code>functorch</code>) to being a part of PyTorch to reflect how the function transforms are integrated directly into PyTorch core. As of PyTorch 2.0, we are deprecating <code>import functorch</code> and ask that users migrate to the newest APIs, which we will maintain going forward. <code>import functorch</code> will be kept around to maintain backwards compatibility for a couple of releases.</p>  <h2 id="function-transforms">function transforms</h2> <p>The following APIs are a drop-in replacement for the following <a class="reference external" href="https://pytorch.org/functorch/1.13/functorch.html">functorch APIs</a>. They are fully backwards compatible.</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>functorch API</p></th> <th class="head"><p>PyTorch API (as of PyTorch 2.0)</p></th> </tr> </thead>  <tr>
<td><p>functorch.vmap</p></td> <td><p><a class="reference internal" href="generated/torch.vmap#torch.vmap" title="torch.vmap"><code>torch.vmap()</code></a> or <a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>torch.func.vmap()</code></a></p></td> </tr> <tr>
<td><p>functorch.grad</p></td> <td><p><a class="reference internal" href="generated/torch.func.grad#torch.func.grad" title="torch.func.grad"><code>torch.func.grad()</code></a></p></td> </tr> <tr>
<td><p>functorch.vjp</p></td> <td><p><a class="reference internal" href="generated/torch.func.vjp#torch.func.vjp" title="torch.func.vjp"><code>torch.func.vjp()</code></a></p></td> </tr> <tr>
<td><p>functorch.jvp</p></td> <td><p><a class="reference internal" href="generated/torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>torch.func.jvp()</code></a></p></td> </tr> <tr>
<td><p>functorch.jacrev</p></td> <td><p><a class="reference internal" href="generated/torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>torch.func.jacrev()</code></a></p></td> </tr> <tr>
<td><p>functorch.jacfwd</p></td> <td><p><a class="reference internal" href="generated/torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>torch.func.jacfwd()</code></a></p></td> </tr> <tr>
<td><p>functorch.hessian</p></td> <td><p><a class="reference internal" href="generated/torch.func.hessian#torch.func.hessian" title="torch.func.hessian"><code>torch.func.hessian()</code></a></p></td> </tr> <tr>
<td><p>functorch.functionalize</p></td> <td><p><a class="reference internal" href="generated/torch.func.functionalize#torch.func.functionalize" title="torch.func.functionalize"><code>torch.func.functionalize()</code></a></p></td> </tr>  </table> <p>Furthermore, if you are using torch.autograd.functional APIs, please try out the <a class="reference internal" href="func.api#module-torch.func" title="torch.func"><code>torch.func</code></a> equivalents instead. <a class="reference internal" href="func.api#module-torch.func" title="torch.func"><code>torch.func</code></a> function transforms are more composable and more performant in many cases.</p> <table class="docutils colwidths-auto align-default"> <thead> <tr>
<th class="head"><p>torch.autograd.functional API</p></th> <th class="head"><p>torch.func API (as of PyTorch 2.0)</p></th> </tr> </thead>  <tr>
<td><p><a class="reference internal" href="generated/torch.autograd.functional.vjp#torch.autograd.functional.vjp" title="torch.autograd.functional.vjp"><code>torch.autograd.functional.vjp()</code></a></p></td> <td><p><a class="reference internal" href="generated/torch.func.grad#torch.func.grad" title="torch.func.grad"><code>torch.func.grad()</code></a> or <a class="reference internal" href="generated/torch.func.vjp#torch.func.vjp" title="torch.func.vjp"><code>torch.func.vjp()</code></a></p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.autograd.functional.jvp#torch.autograd.functional.jvp" title="torch.autograd.functional.jvp"><code>torch.autograd.functional.jvp()</code></a></p></td> <td><p><a class="reference internal" href="generated/torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>torch.func.jvp()</code></a></p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.autograd.functional.jacobian#torch.autograd.functional.jacobian" title="torch.autograd.functional.jacobian"><code>torch.autograd.functional.jacobian()</code></a></p></td> <td><p><a class="reference internal" href="generated/torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>torch.func.jacrev()</code></a> or <a class="reference internal" href="generated/torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>torch.func.jacfwd()</code></a></p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.autograd.functional.hessian#torch.autograd.functional.hessian" title="torch.autograd.functional.hessian"><code>torch.autograd.functional.hessian()</code></a></p></td> <td><p><a class="reference internal" href="generated/torch.func.hessian#torch.func.hessian" title="torch.func.hessian"><code>torch.func.hessian()</code></a></p></td> </tr>  </table>   <h2 id="nn-module-utilities">NN module utilities</h2> <p>We’ve changed the APIs to apply function transforms over NN modules to make them fit better into the PyTorch design philosophy. The new API is different, so please read this section carefully.</p>  <h3 id="functorch-make-functional">functorch.make_functional</h3> <p><a class="reference internal" href="generated/torch.func.functional_call#torch.func.functional_call" title="torch.func.functional_call"><code>torch.func.functional_call()</code></a> is the replacement for <a class="reference external" href="https://pytorch.org/functorch/1.13/generated/functorch.make_functional.html#functorch.make_functional">functorch.make_functional</a> and <a class="reference external" href="https://pytorch.org/functorch/1.13/generated/functorch.make_functional_with_buffers.html#functorch.make_functional_with_buffers">functorch.make_functional_with_buffers</a>. However, it is not a drop-in replacement.</p> <p>If you’re in a hurry, you can use <a class="reference external" href="https://gist.github.com/zou3519/7769506acc899d83ef1464e28f22e6cf">helper functions in this gist</a> that emulate the behavior of functorch.make_functional and functorch.make_functional_with_buffers. We recommend using <a class="reference internal" href="generated/torch.func.functional_call#torch.func.functional_call" title="torch.func.functional_call"><code>torch.func.functional_call()</code></a> directly because it is a more explicit and flexible API.</p> <p>Concretely, functorch.make_functional returns a functional module and parameters. The functional module accepts parameters and inputs to the model as arguments. <a class="reference internal" href="generated/torch.func.functional_call#torch.func.functional_call" title="torch.func.functional_call"><code>torch.func.functional_call()</code></a> allows one to call the forward pass of an existing module using new parameters and buffers and inputs.</p> <p>Here’s an example of how to compute gradients of parameters of a model using functorch vs <a class="reference internal" href="func.api#module-torch.func" title="torch.func"><code>torch.func</code></a>:</p> <pre data-language="python"># ---------------
# using functorch
# ---------------
import torch
import functorch
inputs = torch.randn(64, 3)
targets = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

fmodel, params = functorch.make_functional(model)

def compute_loss(params, inputs, targets):
    prediction = fmodel(params, inputs)
    return torch.nn.functional.mse_loss(prediction, targets)

grads = functorch.grad(compute_loss)(params, inputs, targets)

# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import torch
inputs = torch.randn(64, 3)
targets = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

params = dict(model.named_parameters())

def compute_loss(params, inputs, targets):
    prediction = torch.func.functional_call(model, params, (inputs,))
    return torch.nn.functional.mse_loss(prediction, targets)

grads = torch.func.grad(compute_loss)(params, inputs, targets)
</pre> <p>And here’s an example of how to compute jacobians of model parameters:</p> <pre data-language="python"># ---------------
# using functorch
# ---------------
import torch
import functorch
inputs = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

fmodel, params = functorch.make_functional(model)
jacobians = functorch.jacrev(fmodel)(params, inputs)

# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import torch
from torch.func import jacrev, functional_call
inputs = torch.randn(64, 3)
model = torch.nn.Linear(3, 3)

params = dict(model.named_parameters())
# jacrev computes jacobians of argnums=0 by default.
# We set it to 1 to compute jacobians of params
jacobians = jacrev(functional_call, argnums=1)(model, params, (inputs,))
</pre> <p>Note that it is important for memory consumption that you should only carry around a single copy of your parameters. <code>model.named_parameters()</code> does not copy the parameters. If in your model training you update the parameters of the model in-place, then the <code>nn.Module</code> that is your model has the single copy of the parameters and everything is OK.</p> <p>However, if you want to carry your parameters around in a dictionary and update them out-of-place, then there are two copies of parameters: the one in the dictionary and the one in the <code>model</code>. In this case, you should change <code>model</code> to not hold memory by converting it to the meta device via <code>model.to('meta')</code>.</p>   <h3 id="functorch-combine-state-for-ensemble">functorch.combine_state_for_ensemble</h3> <p>Please use <a class="reference internal" href="generated/torch.func.stack_module_state#torch.func.stack_module_state" title="torch.func.stack_module_state"><code>torch.func.stack_module_state()</code></a> instead of <a class="reference external" href="https://pytorch.org/functorch/1.13/generated/functorch.combine_state_for_ensemble.html">functorch.combine_state_for_ensemble</a> <a class="reference internal" href="generated/torch.func.stack_module_state#torch.func.stack_module_state" title="torch.func.stack_module_state"><code>torch.func.stack_module_state()</code></a> returns two dictionaries, one of stacked parameters, and one of stacked buffers, that can then be used with <a class="reference internal" href="generated/torch.vmap#torch.vmap" title="torch.vmap"><code>torch.vmap()</code></a> and <a class="reference internal" href="generated/torch.func.functional_call#torch.func.functional_call" title="torch.func.functional_call"><code>torch.func.functional_call()</code></a> for ensembling.</p> <p>For example, here is an example of how to ensemble over a very simple model:</p> <pre data-language="python">import torch
num_models = 5
batch_size = 64
in_features, out_features = 3, 3
models = [torch.nn.Linear(in_features, out_features) for i in range(num_models)]
data = torch.randn(batch_size, 3)

# ---------------
# using functorch
# ---------------
import functorch
fmodel, params, buffers = functorch.combine_state_for_ensemble(models)
output = functorch.vmap(fmodel, (0, 0, None))(params, buffers, data)
assert output.shape == (num_models, batch_size, out_features)

# ------------------------------------
# using torch.func (as of PyTorch 2.0)
# ------------------------------------
import copy

# Construct a version of the model with no memory by putting the Tensors on
# the meta device.
base_model = copy.deepcopy(models[0])
base_model.to('meta')

params, buffers = torch.func.stack_module_state(models)

# It is possible to vmap directly over torch.func.functional_call,
# but wrapping it in a function makes it clearer what is going on.
def call_single_model(params, buffers, data):
    return torch.func.functional_call(base_model, (params, buffers), (data,))

output = torch.vmap(call_single_model, (0, 0, None))(params, buffers, data)
assert output.shape == (num_models, batch_size, out_features)
</pre>    <h2 id="functorch-compile">functorch.compile</h2> <p>We are no longer supporting functorch.compile (also known as AOTAutograd) as a frontend for compilation in PyTorch; we have integrated AOTAutograd into PyTorch’s compilation story. If you are a user, please use <a class="reference internal" href="generated/torch.compile#torch.compile" title="torch.compile"><code>torch.compile()</code></a> instead.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/func.migrating.html" class="_attribution-link">https://pytorch.org/docs/2.1/func.migrating.html</a>
  </p>
</div>
