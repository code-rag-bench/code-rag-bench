<h1 id="torch">torch</h1> <p id="module-torch">The torch package contains data structures for multi-dimensional tensors and defines mathematical operations over these tensors. Additionally, it provides many utilities for efficient serialization of Tensors and arbitrary types, and other useful utilities.</p> <p>It has a CUDA counterpart, that enables you to run your tensor computations on an NVIDIA GPU with compute capability &gt;= 3.0.</p>  <h2 id="tensors">Tensors</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.is_tensor#torch.is_tensor" title="torch.is_tensor"><code>is_tensor</code></a>
</td> <td><p>Returns True if <code>obj</code> is a PyTorch tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_storage#torch.is_storage" title="torch.is_storage"><code>is_storage</code></a>
</td> <td><p>Returns True if <code>obj</code> is a PyTorch storage object.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_complex#torch.is_complex" title="torch.is_complex"><code>is_complex</code></a>
</td> <td><p>Returns True if the data type of <code>input</code> is a complex data type i.e., one of <code>torch.complex64</code>, and <code>torch.complex128</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_conj#torch.is_conj" title="torch.is_conj"><code>is_conj</code></a>
</td> <td><p>Returns True if the <code>input</code> is a conjugated tensor, i.e. its conjugate bit is set to <code>True</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_floating_point#torch.is_floating_point" title="torch.is_floating_point"><code>is_floating_point</code></a>
</td> <td><p>Returns True if the data type of <code>input</code> is a floating point data type i.e., one of <code>torch.float64</code>, <code>torch.float32</code>, <code>torch.float16</code>, and <code>torch.bfloat16</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_nonzero#torch.is_nonzero" title="torch.is_nonzero"><code>is_nonzero</code></a>
</td> <td><p>Returns True if the <code>input</code> is a single element tensor which is not equal to zero after type conversions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_default_dtype#torch.set_default_dtype" title="torch.set_default_dtype"><code>set_default_dtype</code></a>
</td> <td><p>Sets the default floating point dtype to <code>d</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.get_default_dtype#torch.get_default_dtype" title="torch.get_default_dtype"><code>get_default_dtype</code></a>
</td> <td><p>Get the current default floating point <a class="reference internal" href="tensor_attributes#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_default_device#torch.set_default_device" title="torch.set_default_device"><code>set_default_device</code></a>
</td> <td><p>Sets the default <code>torch.Tensor</code> to be allocated on <code>device</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_default_tensor_type#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>set_default_tensor_type</code></a>
</td> <td><p>Sets the default <code>torch.Tensor</code> type to floating point tensor type <code>t</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.numel#torch.numel" title="torch.numel"><code>numel</code></a>
</td> <td><p>Returns the total number of elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_printoptions#torch.set_printoptions" title="torch.set_printoptions"><code>set_printoptions</code></a>
</td> <td><p>Set options for printing.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_flush_denormal#torch.set_flush_denormal" title="torch.set_flush_denormal"><code>set_flush_denormal</code></a>
</td> <td><p>Disables denormal floating numbers on CPU.</p></td> </tr>  </table>  <h3 id="tensor-creation-ops">Creation Ops</h3> <div class="admonition note" id="creation-ops"> <p class="admonition-title">Note</p> <p>Random sampling creation ops are listed under <a class="reference internal" href="#random-sampling"><span class="std std-ref">Random sampling</span></a> and include: <a class="reference internal" href="generated/torch.rand#torch.rand" title="torch.rand"><code>torch.rand()</code></a> <a class="reference internal" href="generated/torch.rand_like#torch.rand_like" title="torch.rand_like"><code>torch.rand_like()</code></a> <a class="reference internal" href="generated/torch.randn#torch.randn" title="torch.randn"><code>torch.randn()</code></a> <a class="reference internal" href="generated/torch.randn_like#torch.randn_like" title="torch.randn_like"><code>torch.randn_like()</code></a> <a class="reference internal" href="generated/torch.randint#torch.randint" title="torch.randint"><code>torch.randint()</code></a> <a class="reference internal" href="generated/torch.randint_like#torch.randint_like" title="torch.randint_like"><code>torch.randint_like()</code></a> <a class="reference internal" href="generated/torch.randperm#torch.randperm" title="torch.randperm"><code>torch.randperm()</code></a> You may also use <a class="reference internal" href="generated/torch.empty#torch.empty" title="torch.empty"><code>torch.empty()</code></a> with the <a class="reference internal" href="#inplace-random-sampling"><span class="std std-ref">In-place random sampling</span></a> methods to create <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> s with values sampled from a broader range of distributions.</p> </div> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.tensor#torch.tensor" title="torch.tensor"><code>tensor</code></a>
</td> <td><p>Constructs a tensor with no autograd history (also known as a "leaf tensor", see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html"><span class="doc">Autograd mechanics</span></a>) by copying <code>data</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sparse_coo_tensor#torch.sparse_coo_tensor" title="torch.sparse_coo_tensor"><code>sparse_coo_tensor</code></a>
</td> <td><p>Constructs a <a class="reference internal" href="sparse#sparse-coo-docs"><span class="std std-ref">sparse tensor in COO(rdinate) format</span></a> with specified values at the given <code>indices</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sparse_csr_tensor#torch.sparse_csr_tensor" title="torch.sparse_csr_tensor"><code>sparse_csr_tensor</code></a>
</td> <td><p>Constructs a <a class="reference internal" href="sparse#sparse-csr-docs"><span class="std std-ref">sparse tensor in CSR (Compressed Sparse Row)</span></a> with specified values at the given <code>crow_indices</code> and <code>col_indices</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sparse_csc_tensor#torch.sparse_csc_tensor" title="torch.sparse_csc_tensor"><code>sparse_csc_tensor</code></a>
</td> <td><p>Constructs a <a class="reference internal" href="sparse#sparse-csc-docs"><span class="std std-ref">sparse tensor in CSC (Compressed Sparse Column)</span></a> with specified values at the given <code>ccol_indices</code> and <code>row_indices</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sparse_bsr_tensor#torch.sparse_bsr_tensor" title="torch.sparse_bsr_tensor"><code>sparse_bsr_tensor</code></a>
</td> <td><p>Constructs a <a class="reference internal" href="sparse#sparse-bsr-docs"><span class="std std-ref">sparse tensor in BSR (Block Compressed Sparse Row))</span></a> with specified 2-dimensional blocks at the given <code>crow_indices</code> and <code>col_indices</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sparse_bsc_tensor#torch.sparse_bsc_tensor" title="torch.sparse_bsc_tensor"><code>sparse_bsc_tensor</code></a>
</td> <td><p>Constructs a <a class="reference internal" href="sparse#sparse-bsc-docs"><span class="std std-ref">sparse tensor in BSC (Block Compressed Sparse Column))</span></a> with specified 2-dimensional blocks at the given <code>ccol_indices</code> and <code>row_indices</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.asarray#torch.asarray" title="torch.asarray"><code>asarray</code></a>
</td> <td><p>Converts <code>obj</code> to a tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.as_tensor#torch.as_tensor" title="torch.as_tensor"><code>as_tensor</code></a>
</td> <td><p>Converts <code>data</code> into a tensor, sharing data and preserving autograd history if possible.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.as_strided#torch.as_strided" title="torch.as_strided"><code>as_strided</code></a>
</td> <td><p>Create a view of an existing <code>torch.Tensor</code> <code>input</code> with specified <code>size</code>, <code>stride</code> and <code>storage_offset</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.from_numpy#torch.from_numpy" title="torch.from_numpy"><code>from_numpy</code></a>
</td> <td><p>Creates a <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> from a <a class="reference external" href="https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html#numpy.ndarray" title="(in NumPy v1.26)"><code>numpy.ndarray</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.from_dlpack#torch.from_dlpack" title="torch.from_dlpack"><code>from_dlpack</code></a>
</td> <td><p>Converts a tensor from an external library into a <code>torch.Tensor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.frombuffer#torch.frombuffer" title="torch.frombuffer"><code>frombuffer</code></a>
</td> <td><p>Creates a 1-dimensional <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>Tensor</code></a> from an object that implements the Python buffer protocol.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.zeros#torch.zeros" title="torch.zeros"><code>zeros</code></a>
</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.zeros_like#torch.zeros_like" title="torch.zeros_like"><code>zeros_like</code></a>
</td> <td><p>Returns a tensor filled with the scalar value <code>0</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ones#torch.ones" title="torch.ones"><code>ones</code></a>
</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the shape defined by the variable argument <code>size</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ones_like#torch.ones_like" title="torch.ones_like"><code>ones_like</code></a>
</td> <td><p>Returns a tensor filled with the scalar value <code>1</code>, with the same size as <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arange#torch.arange" title="torch.arange"><code>arange</code></a>
</td> <td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">⌈</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌉</mo></mrow><annotation encoding="application/x-tex">\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</annotation></semantics></math></span></span></span> with values from the interval <code>[start, end)</code> taken with common difference <code>step</code> beginning from <code>start</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.range#torch.range" title="torch.range"><code>range</code></a>
</td> <td><p>Returns a 1-D tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo fence="true">⌊</mo><mfrac><mrow><mtext>end</mtext><mo>−</mo><mtext>start</mtext></mrow><mtext>step</mtext></mfrac><mo fence="true">⌋</mo></mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\left\lfloor \frac{\text{end} - \text{start}}{\text{step}} \right\rfloor + 1</annotation></semantics></math></span></span></span> with values from <code>start</code> to <code>end</code> with step <code>step</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.linspace#torch.linspace" title="torch.linspace"><code>linspace</code></a>
</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <code>start</code> to <code>end</code>, inclusive.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logspace#torch.logspace" title="torch.logspace"><code>logspace</code></a>
</td> <td><p>Creates a one-dimensional tensor of size <code>steps</code> whose values are evenly spaced from <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>start</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{start}}}}</annotation></semantics></math></span></span></span> to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>base</mtext><mtext>end</mtext></msup></mrow><annotation encoding="application/x-tex">{{\text{{base}}}}^{{\text{{end}}}}</annotation></semantics></math></span></span></span>, inclusive, on a logarithmic scale with base <code>base</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.eye#torch.eye" title="torch.eye"><code>eye</code></a>
</td> <td><p>Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.empty#torch.empty" title="torch.empty"><code>empty</code></a>
</td> <td><p>Returns a tensor filled with uninitialized data.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.empty_like#torch.empty_like" title="torch.empty_like"><code>empty_like</code></a>
</td> <td><p>Returns an uninitialized tensor with the same size as <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.empty_strided#torch.empty_strided" title="torch.empty_strided"><code>empty_strided</code></a>
</td> <td><p>Creates a tensor with the specified <code>size</code> and <code>stride</code> and filled with undefined data.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.full#torch.full" title="torch.full"><code>full</code></a>
</td> <td><p>Creates a tensor of size <code>size</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.full_like#torch.full_like" title="torch.full_like"><code>full_like</code></a>
</td> <td><p>Returns a tensor with the same size as <code>input</code> filled with <code>fill_value</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantize_per_tensor#torch.quantize_per_tensor" title="torch.quantize_per_tensor"><code>quantize_per_tensor</code></a>
</td> <td><p>Converts a float tensor to a quantized tensor with given scale and zero point.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantize_per_channel#torch.quantize_per_channel" title="torch.quantize_per_channel"><code>quantize_per_channel</code></a>
</td> <td><p>Converts a float tensor to a per-channel quantized tensor with given scales and zero points.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dequantize#torch.dequantize" title="torch.dequantize"><code>dequantize</code></a>
</td> <td><p>Returns an fp32 Tensor by dequantizing a quantized Tensor</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.complex#torch.complex" title="torch.complex"><code>complex</code></a>
</td> <td><p>Constructs a complex tensor with its real part equal to <a class="reference internal" href="generated/torch.real#torch.real" title="torch.real"><code>real</code></a> and its imaginary part equal to <a class="reference internal" href="generated/torch.imag#torch.imag" title="torch.imag"><code>imag</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.polar#torch.polar" title="torch.polar"><code>polar</code></a>
</td> <td><p>Constructs a complex tensor whose elements are Cartesian coordinates corresponding to the polar coordinates with absolute value <a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>abs</code></a> and angle <a class="reference internal" href="generated/torch.angle#torch.angle" title="torch.angle"><code>angle</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.heaviside#torch.heaviside" title="torch.heaviside"><code>heaviside</code></a>
</td> <td><p>Computes the Heaviside step function for each element in <code>input</code>.</p></td> </tr>  </table>   <h3 id="indexing-slicing-joining">Indexing, Slicing, Joining, Mutating Ops</h3> <table class="autosummary longtable docutils colwidths-auto align-default" id="indexing-slicing-joining-mutating-ops">  <tr>
<td>


<a class="reference internal" href="generated/torch.adjoint#torch.adjoint" title="torch.adjoint"><code>adjoint</code></a>
</td> <td><p>Returns a view of the tensor conjugated and with the last two dimensions transposed.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.argwhere#torch.argwhere" title="torch.argwhere"><code>argwhere</code></a>
</td> <td><p>Returns a tensor containing the indices of all non-zero elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cat#torch.cat" title="torch.cat"><code>cat</code></a>
</td> <td><p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.concat#torch.concat" title="torch.concat"><code>concat</code></a>
</td> <td><p>Alias of <a class="reference internal" href="generated/torch.cat#torch.cat" title="torch.cat"><code>torch.cat()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.concatenate#torch.concatenate" title="torch.concatenate"><code>concatenate</code></a>
</td> <td><p>Alias of <a class="reference internal" href="generated/torch.cat#torch.cat" title="torch.cat"><code>torch.cat()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.conj#torch.conj" title="torch.conj"><code>conj</code></a>
</td> <td><p>Returns a view of <code>input</code> with a flipped conjugate bit.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.chunk#torch.chunk" title="torch.chunk"><code>chunk</code></a>
</td> <td><p>Attempts to split a tensor into the specified number of chunks.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dsplit#torch.dsplit" title="torch.dsplit"><code>dsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.column_stack#torch.column_stack" title="torch.column_stack"><code>column_stack</code></a>
</td> <td><p>Creates a new tensor by horizontally stacking the tensors in <code>tensors</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dstack#torch.dstack" title="torch.dstack"><code>dstack</code></a>
</td> <td><p>Stack tensors in sequence depthwise (along third axis).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.gather#torch.gather" title="torch.gather"><code>gather</code></a>
</td> <td><p>Gathers values along an axis specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hsplit#torch.hsplit" title="torch.hsplit"><code>hsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hstack#torch.hstack" title="torch.hstack"><code>hstack</code></a>
</td> <td><p>Stack tensors in sequence horizontally (column wise).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.index_add#torch.index_add" title="torch.index_add"><code>index_add</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.tensor.index_add_#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code>index_add_()</code></a> for function description.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.index_copy#torch.index_copy" title="torch.index_copy"><code>index_copy</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.tensor.index_add_#torch.Tensor.index_add_" title="torch.Tensor.index_add_"><code>index_add_()</code></a> for function description.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.index_reduce#torch.index_reduce" title="torch.index_reduce"><code>index_reduce</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.tensor.index_reduce_#torch.Tensor.index_reduce_" title="torch.Tensor.index_reduce_"><code>index_reduce_()</code></a> for function description.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.index_select#torch.index_select" title="torch.index_select"><code>index_select</code></a>
</td> <td><p>Returns a new tensor which indexes the <code>input</code> tensor along dimension <code>dim</code> using the entries in <code>index</code> which is a <code>LongTensor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.masked_select#torch.masked_select" title="torch.masked_select"><code>masked_select</code></a>
</td> <td><p>Returns a new 1-D tensor which indexes the <code>input</code> tensor according to the boolean mask <code>mask</code> which is a <code>BoolTensor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.movedim#torch.movedim" title="torch.movedim"><code>movedim</code></a>
</td> <td><p>Moves the dimension(s) of <code>input</code> at the position(s) in <code>source</code> to the position(s) in <code>destination</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.moveaxis#torch.moveaxis" title="torch.moveaxis"><code>moveaxis</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.movedim#torch.movedim" title="torch.movedim"><code>torch.movedim()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.narrow#torch.narrow" title="torch.narrow"><code>narrow</code></a>
</td> <td><p>Returns a new tensor that is a narrowed version of <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.narrow_copy#torch.narrow_copy" title="torch.narrow_copy"><code>narrow_copy</code></a>
</td> <td><p>Same as <a class="reference internal" href="generated/torch.tensor.narrow#torch.Tensor.narrow" title="torch.Tensor.narrow"><code>Tensor.narrow()</code></a> except this returns a copy rather than shared storage.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nonzero#torch.nonzero" title="torch.nonzero"><code>nonzero</code></a>
</td> <td></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.permute#torch.permute" title="torch.permute"><code>permute</code></a>
</td> <td><p>Returns a view of the original tensor <code>input</code> with its dimensions permuted.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.reshape#torch.reshape" title="torch.reshape"><code>reshape</code></a>
</td> <td><p>Returns a tensor with the same data and number of elements as <code>input</code>, but with the specified shape.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.row_stack#torch.row_stack" title="torch.row_stack"><code>row_stack</code></a>
</td> <td><p>Alias of <a class="reference internal" href="generated/torch.vstack#torch.vstack" title="torch.vstack"><code>torch.vstack()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.select#torch.select" title="torch.select"><code>select</code></a>
</td> <td><p>Slices the <code>input</code> tensor along the selected dimension at the given index.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.scatter#torch.scatter" title="torch.scatter"><code>scatter</code></a>
</td> <td><p>Out-of-place version of <a class="reference internal" href="generated/torch.tensor.scatter_#torch.Tensor.scatter_" title="torch.Tensor.scatter_"><code>torch.Tensor.scatter_()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diagonal_scatter#torch.diagonal_scatter" title="torch.diagonal_scatter"><code>diagonal_scatter</code></a>
</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> along the diagonal elements of <code>input</code>, with respect to <code>dim1</code> and <code>dim2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.select_scatter#torch.select_scatter" title="torch.select_scatter"><code>select_scatter</code></a>
</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> at the given index.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.slice_scatter#torch.slice_scatter" title="torch.slice_scatter"><code>slice_scatter</code></a>
</td> <td><p>Embeds the values of the <code>src</code> tensor into <code>input</code> at the given dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.scatter_add#torch.scatter_add" title="torch.scatter_add"><code>scatter_add</code></a>
</td> <td><p>Out-of-place version of <a class="reference internal" href="generated/torch.tensor.scatter_add_#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code>torch.Tensor.scatter_add_()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.scatter_reduce#torch.scatter_reduce" title="torch.scatter_reduce"><code>scatter_reduce</code></a>
</td> <td><p>Out-of-place version of <a class="reference internal" href="generated/torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code>torch.Tensor.scatter_reduce_()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.split#torch.split" title="torch.split"><code>split</code></a>
</td> <td><p>Splits the tensor into chunks.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.squeeze#torch.squeeze" title="torch.squeeze"><code>squeeze</code></a>
</td> <td><p>Returns a tensor with all specified dimensions of <code>input</code> of size <code>1</code> removed.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.stack#torch.stack" title="torch.stack"><code>stack</code></a>
</td> <td><p>Concatenates a sequence of tensors along a new dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.swapaxes#torch.swapaxes" title="torch.swapaxes"><code>swapaxes</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.transpose#torch.transpose" title="torch.transpose"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.swapdims#torch.swapdims" title="torch.swapdims"><code>swapdims</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.transpose#torch.transpose" title="torch.transpose"><code>torch.transpose()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.t#torch.t" title="torch.t"><code>t</code></a>
</td> <td><p>Expects <code>input</code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.take#torch.take" title="torch.take"><code>take</code></a>
</td> <td><p>Returns a new tensor with the elements of <code>input</code> at the given indices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.take_along_dim#torch.take_along_dim" title="torch.take_along_dim"><code>take_along_dim</code></a>
</td> <td><p>Selects values from <code>input</code> at the 1-dimensional indices from <code>indices</code> along the given <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tensor_split#torch.tensor_split" title="torch.tensor_split"><code>tensor_split</code></a>
</td> <td><p>Splits a tensor into multiple sub-tensors, all of which are views of <code>input</code>, along dimension <code>dim</code> according to the indices or number of sections specified by <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tile#torch.tile" title="torch.tile"><code>tile</code></a>
</td> <td><p>Constructs a tensor by repeating the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.transpose#torch.transpose" title="torch.transpose"><code>transpose</code></a>
</td> <td><p>Returns a tensor that is a transposed version of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.unbind#torch.unbind" title="torch.unbind"><code>unbind</code></a>
</td> <td><p>Removes a tensor dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.unsqueeze#torch.unsqueeze" title="torch.unsqueeze"><code>unsqueeze</code></a>
</td> <td><p>Returns a new tensor with a dimension of size one inserted at the specified position.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vsplit#torch.vsplit" title="torch.vsplit"><code>vsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vstack#torch.vstack" title="torch.vstack"><code>vstack</code></a>
</td> <td><p>Stack tensors in sequence vertically (row wise).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.where#torch.where" title="torch.where"><code>where</code></a>
</td> <td><p>Return a tensor of elements selected from either <code>input</code> or <code>other</code>, depending on <code>condition</code>.</p></td> </tr>  </table>    <h2 id="id1">Generators</h2> <table class="autosummary longtable docutils colwidths-auto align-default" id="generators">  <tr>
<td>


<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator"><code>Generator</code></a>
</td> <td><p>Creates and returns a generator object that manages the state of the algorithm which produces pseudo random numbers.</p></td> </tr>  </table>   <h2 id="id2">Random sampling</h2> <table class="autosummary longtable docutils colwidths-auto align-default" id="random-sampling">  <tr>
<td>


<a class="reference internal" href="generated/torch.seed#torch.seed" title="torch.seed"><code>seed</code></a>
</td> <td><p>Sets the seed for generating random numbers to a non-deterministic random number.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.manual_seed#torch.manual_seed" title="torch.manual_seed"><code>manual_seed</code></a>
</td> <td><p>Sets the seed for generating random numbers.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.initial_seed#torch.initial_seed" title="torch.initial_seed"><code>initial_seed</code></a>
</td> <td><p>Returns the initial seed for generating random numbers as a Python <code>long</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.get_rng_state#torch.get_rng_state" title="torch.get_rng_state"><code>get_rng_state</code></a>
</td> <td><p>Returns the random number generator state as a <code>torch.ByteTensor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_rng_state#torch.set_rng_state" title="torch.set_rng_state"><code>set_rng_state</code></a>
</td> <td><p>Sets the random number generator state.</p></td> </tr>  </table> <dl class="py attribute"> <dt class="sig sig-object py" id="torch.torch.default_generator">
<code>torch.default_generator Returns the default CPU torch.Generator</code> </dt> 
</dl> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.bernoulli#torch.bernoulli" title="torch.bernoulli"><code>bernoulli</code></a>
</td> <td><p>Draws binary random numbers (0 or 1) from a Bernoulli distribution.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.multinomial#torch.multinomial" title="torch.multinomial"><code>multinomial</code></a>
</td> <td><p>Returns a tensor where each row contains <code>num_samples</code> indices sampled from the multinomial probability distribution located in the corresponding row of tensor <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.normal#torch.normal" title="torch.normal"><code>normal</code></a>
</td> <td><p>Returns a tensor of random numbers drawn from separate normal distributions whose mean and standard deviation are given.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.poisson#torch.poisson" title="torch.poisson"><code>poisson</code></a>
</td> <td><p>Returns a tensor of the same size as <code>input</code> with each element sampled from a Poisson distribution with rate parameter given by the corresponding element in <code>input</code> i.e.,</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rand#torch.rand" title="torch.rand"><code>rand</code></a>
</td> <td><p>Returns a tensor filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rand_like#torch.rand_like" title="torch.rand_like"><code>rand_like</code></a>
</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a uniform distribution on the interval <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">[</mo><mn>0</mn><mo separator="true">,</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">[0, 1)</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.randint#torch.randint" title="torch.randint"><code>randint</code></a>
</td> <td><p>Returns a tensor filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.randint_like#torch.randint_like" title="torch.randint_like"><code>randint_like</code></a>
</td> <td><p>Returns a tensor with the same shape as Tensor <code>input</code> filled with random integers generated uniformly between <code>low</code> (inclusive) and <code>high</code> (exclusive).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.randn#torch.randn" title="torch.randn"><code>randn</code></a>
</td> <td><p>Returns a tensor filled with random numbers from a normal distribution with mean <code>0</code> and variance <code>1</code> (also called the standard normal distribution).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.randn_like#torch.randn_like" title="torch.randn_like"><code>randn_like</code></a>
</td> <td><p>Returns a tensor with the same size as <code>input</code> that is filled with random numbers from a normal distribution with mean 0 and variance 1.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.randperm#torch.randperm" title="torch.randperm"><code>randperm</code></a>
</td> <td><p>Returns a random permutation of integers from <code>0</code> to <code>n - 1</code>.</p></td> </tr>  </table>  <h3 id="inplace-random-sampling">In-place random sampling</h3> <p id="in-place-random-sampling">There are a few more in-place random sampling functions defined on Tensors as well. Click through to refer to their documentation:</p> <ul class="simple"> <li>
<a class="reference internal" href="generated/torch.tensor.bernoulli_#torch.Tensor.bernoulli_" title="torch.Tensor.bernoulli_"><code>torch.Tensor.bernoulli_()</code></a> - in-place version of <a class="reference internal" href="generated/torch.bernoulli#torch.bernoulli" title="torch.bernoulli"><code>torch.bernoulli()</code></a>
</li> <li>
<a class="reference internal" href="generated/torch.tensor.cauchy_#torch.Tensor.cauchy_" title="torch.Tensor.cauchy_"><code>torch.Tensor.cauchy_()</code></a> - numbers drawn from the Cauchy distribution</li> <li>
<a class="reference internal" href="generated/torch.tensor.exponential_#torch.Tensor.exponential_" title="torch.Tensor.exponential_"><code>torch.Tensor.exponential_()</code></a> - numbers drawn from the exponential distribution</li> <li>
<a class="reference internal" href="generated/torch.tensor.geometric_#torch.Tensor.geometric_" title="torch.Tensor.geometric_"><code>torch.Tensor.geometric_()</code></a> - elements drawn from the geometric distribution</li> <li>
<a class="reference internal" href="generated/torch.tensor.log_normal_#torch.Tensor.log_normal_" title="torch.Tensor.log_normal_"><code>torch.Tensor.log_normal_()</code></a> - samples from the log-normal distribution</li> <li>
<a class="reference internal" href="generated/torch.tensor.normal_#torch.Tensor.normal_" title="torch.Tensor.normal_"><code>torch.Tensor.normal_()</code></a> - in-place version of <a class="reference internal" href="generated/torch.normal#torch.normal" title="torch.normal"><code>torch.normal()</code></a>
</li> <li>
<a class="reference internal" href="generated/torch.tensor.random_#torch.Tensor.random_" title="torch.Tensor.random_"><code>torch.Tensor.random_()</code></a> - numbers sampled from the discrete uniform distribution</li> <li>
<a class="reference internal" href="generated/torch.tensor.uniform_#torch.Tensor.uniform_" title="torch.Tensor.uniform_"><code>torch.Tensor.uniform_()</code></a> - numbers sampled from the continuous uniform distribution</li> </ul>   <h3 id="quasi-random-sampling">Quasi-random sampling</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><a class="reference internal" href="generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code>quasirandom.SobolEngine</code></a></p></td> <td><p>The <a class="reference internal" href="generated/torch.quasirandom.sobolengine#torch.quasirandom.SobolEngine" title="torch.quasirandom.SobolEngine"><code>torch.quasirandom.SobolEngine</code></a> is an engine for generating (scrambled) Sobol sequences.</p></td> </tr>  </table>    <h2 id="serialization">Serialization</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.save#torch.save" title="torch.save"><code>save</code></a>
</td> <td><p>Saves an object to a disk file.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.load#torch.load" title="torch.load"><code>load</code></a>
</td> <td><p>Loads an object saved with <a class="reference internal" href="generated/torch.save#torch.save" title="torch.save"><code>torch.save()</code></a> from a file.</p></td> </tr>  </table>   <h2 id="parallelism">Parallelism</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.get_num_threads#torch.get_num_threads" title="torch.get_num_threads"><code>get_num_threads</code></a>
</td> <td><p>Returns the number of threads used for parallelizing CPU operations</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_num_threads#torch.set_num_threads" title="torch.set_num_threads"><code>set_num_threads</code></a>
</td> <td><p>Sets the number of threads used for intraop parallelism on CPU.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.get_num_interop_threads#torch.get_num_interop_threads" title="torch.get_num_interop_threads"><code>get_num_interop_threads</code></a>
</td> <td><p>Returns the number of threads used for inter-op parallelism on CPU (e.g.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_num_interop_threads#torch.set_num_interop_threads" title="torch.set_num_interop_threads"><code>set_num_interop_threads</code></a>
</td> <td><p>Sets the number of threads used for interop parallelism (e.g.</p></td> </tr>  </table>   <h2 id="torch-rst-local-disable-grad">Locally disabling gradient computation</h2> <p id="locally-disabling-gradient-computation">The context managers <a class="reference internal" href="generated/torch.no_grad#torch.no_grad" title="torch.no_grad"><code>torch.no_grad()</code></a>, <a class="reference internal" href="generated/torch.enable_grad#torch.enable_grad" title="torch.enable_grad"><code>torch.enable_grad()</code></a>, and <a class="reference internal" href="generated/torch.set_grad_enabled#torch.set_grad_enabled" title="torch.set_grad_enabled"><code>torch.set_grad_enabled()</code></a> are helpful for locally disabling and enabling gradient computation. See <a class="reference internal" href="autograd#locally-disable-grad"><span class="std std-ref">Locally disabling gradient computation</span></a> for more details on their usage. These context managers are thread local, so they won’t work if you send work to another thread using the <code>threading</code> module, etc.</p> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; is_train = False
&gt;&gt;&gt; with torch.set_grad_enabled(is_train):
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False

&gt;&gt;&gt; torch.set_grad_enabled(True)  # this can also be used as a function
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
True

&gt;&gt;&gt; torch.set_grad_enabled(False)
&gt;&gt;&gt; y = x * 2
&gt;&gt;&gt; y.requires_grad
False
</pre> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.no_grad#torch.no_grad" title="torch.no_grad"><code>no_grad</code></a>
</td> <td><p>Context-manager that disables gradient calculation.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.enable_grad#torch.enable_grad" title="torch.enable_grad"><code>enable_grad</code></a>
</td> <td><p>Context-manager that enables gradient calculation.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_grad_enabled#torch.set_grad_enabled" title="torch.set_grad_enabled"><code>set_grad_enabled</code></a>
</td> <td><p>Context-manager that sets gradient calculation on or off.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_grad_enabled#torch.is_grad_enabled" title="torch.is_grad_enabled"><code>is_grad_enabled</code></a>
</td> <td><p>Returns True if grad mode is currently enabled.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.inference_mode#torch.inference_mode" title="torch.inference_mode"><code>inference_mode</code></a>
</td> <td><p>Context-manager that enables or disables inference mode</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_inference_mode_enabled#torch.is_inference_mode_enabled" title="torch.is_inference_mode_enabled"><code>is_inference_mode_enabled</code></a>
</td> <td><p>Returns True if inference mode is currently enabled.</p></td> </tr>  </table>   <h2 id="math-operations">Math operations</h2>  <h3 id="pointwise-ops">Pointwise Ops</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>abs</code></a>
</td> <td><p>Computes the absolute value of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.absolute#torch.absolute" title="torch.absolute"><code>absolute</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>torch.abs()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>acos</code></a>
</td> <td><p>Computes the inverse cosine of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arccos#torch.arccos" title="torch.arccos"><code>arccos</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>torch.acos()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.acosh#torch.acosh" title="torch.acosh"><code>acosh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arccosh#torch.arccosh" title="torch.arccosh"><code>arccosh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.acosh#torch.acosh" title="torch.acosh"><code>torch.acosh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.add#torch.add" title="torch.add"><code>add</code></a>
</td> <td><p>Adds <code>other</code>, scaled by <code>alpha</code>, to <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.addcdiv#torch.addcdiv" title="torch.addcdiv"><code>addcdiv</code></a>
</td> <td><p>Performs the element-wise division of <code>tensor1</code> by <code>tensor2</code>, multiplies the result by the scalar <code>value</code> and adds it to <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.addcmul#torch.addcmul" title="torch.addcmul"><code>addcmul</code></a>
</td> <td><p>Performs the element-wise multiplication of <code>tensor1</code> by <code>tensor2</code>, multiplies the result by the scalar <code>value</code> and adds it to <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.angle#torch.angle" title="torch.angle"><code>angle</code></a>
</td> <td><p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>asin</code></a>
</td> <td><p>Returns a new tensor with the arcsine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arcsin#torch.arcsin" title="torch.arcsin"><code>arcsin</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>torch.asin()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.asinh#torch.asinh" title="torch.asinh"><code>asinh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arcsinh#torch.arcsinh" title="torch.arcsinh"><code>arcsinh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.asinh#torch.asinh" title="torch.asinh"><code>torch.asinh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>atan</code></a>
</td> <td><p>Returns a new tensor with the arctangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctan#torch.arctan" title="torch.arctan"><code>arctan</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>torch.atan()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atanh#torch.atanh" title="torch.atanh"><code>atanh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctanh#torch.arctanh" title="torch.arctanh"><code>arctanh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atanh#torch.atanh" title="torch.atanh"><code>torch.atanh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atan2#torch.atan2" title="torch.atan2"><code>atan2</code></a>
</td> <td><p>Element-wise arctangent of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span></span></span> with consideration of the quadrant.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctan2#torch.arctan2" title="torch.arctan2"><code>arctan2</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atan2#torch.atan2" title="torch.atan2"><code>torch.atan2()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_not#torch.bitwise_not" title="torch.bitwise_not"><code>bitwise_not</code></a>
</td> <td><p>Computes the bitwise NOT of the given input tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_and#torch.bitwise_and" title="torch.bitwise_and"><code>bitwise_and</code></a>
</td> <td><p>Computes the bitwise AND of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_or#torch.bitwise_or" title="torch.bitwise_or"><code>bitwise_or</code></a>
</td> <td><p>Computes the bitwise OR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_xor#torch.bitwise_xor" title="torch.bitwise_xor"><code>bitwise_xor</code></a>
</td> <td><p>Computes the bitwise XOR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_left_shift#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code>bitwise_left_shift</code></a>
</td> <td><p>Computes the left arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_right_shift#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code>bitwise_right_shift</code></a>
</td> <td><p>Computes the right arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ceil#torch.ceil" title="torch.ceil"><code>ceil</code></a>
</td> <td><p>Returns a new tensor with the ceil of the elements of <code>input</code>, the smallest integer greater than or equal to each element.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.clamp#torch.clamp" title="torch.clamp"><code>clamp</code></a>
</td> <td><p>Clamps all elements in <code>input</code> into the range <code>[</code> <a class="reference internal" href="generated/torch.min#torch.min" title="torch.min"><code>min</code></a>, <a class="reference internal" href="generated/torch.max#torch.max" title="torch.max"><code>max</code></a> <code>]</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.clip#torch.clip" title="torch.clip"><code>clip</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.clamp#torch.clamp" title="torch.clamp"><code>torch.clamp()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.conj_physical#torch.conj_physical" title="torch.conj_physical"><code>conj_physical</code></a>
</td> <td><p>Computes the element-wise conjugate of the given <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.copysign#torch.copysign" title="torch.copysign"><code>copysign</code></a>
</td> <td><p>Create a new floating-point tensor with the magnitude of <code>input</code> and the sign of <code>other</code>, elementwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cos#torch.cos" title="torch.cos"><code>cos</code></a>
</td> <td><p>Returns a new tensor with the cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cosh#torch.cosh" title="torch.cosh"><code>cosh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.deg2rad#torch.deg2rad" title="torch.deg2rad"><code>deg2rad</code></a>
</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in degrees to radians.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>div</code></a>
</td> <td><p>Divides each element of the input <code>input</code> by the corresponding element of <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.divide#torch.divide" title="torch.divide"><code>divide</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>torch.div()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.digamma#torch.digamma" title="torch.digamma"><code>digamma</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.digamma" title="torch.special.digamma"><code>torch.special.digamma()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erf#torch.erf" title="torch.erf"><code>erf</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erf" title="torch.special.erf"><code>torch.special.erf()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erfc#torch.erfc" title="torch.erfc"><code>erfc</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erfc" title="torch.special.erfc"><code>torch.special.erfc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erfinv#torch.erfinv" title="torch.erfinv"><code>erfinv</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erfinv" title="torch.special.erfinv"><code>torch.special.erfinv()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.exp#torch.exp" title="torch.exp"><code>exp</code></a>
</td> <td><p>Returns a new tensor with the exponential of the elements of the input tensor <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.exp2#torch.exp2" title="torch.exp2"><code>exp2</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.exp2" title="torch.special.exp2"><code>torch.special.exp2()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.expm1#torch.expm1" title="torch.expm1"><code>expm1</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.expm1" title="torch.special.expm1"><code>torch.special.expm1()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fake_quantize_per_channel_affine#torch.fake_quantize_per_channel_affine" title="torch.fake_quantize_per_channel_affine"><code>fake_quantize_per_channel_affine</code></a>
</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized per channel using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>, across the channel specified by <code>axis</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fake_quantize_per_tensor_affine#torch.fake_quantize_per_tensor_affine" title="torch.fake_quantize_per_tensor_affine"><code>fake_quantize_per_tensor_affine</code></a>
</td> <td><p>Returns a new tensor with the data in <code>input</code> fake quantized using <code>scale</code>, <code>zero_point</code>, <code>quant_min</code> and <code>quant_max</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fix#torch.fix" title="torch.fix"><code>fix</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.float_power#torch.float_power" title="torch.float_power"><code>float_power</code></a>
</td> <td><p>Raises <code>input</code> to the power of <code>exponent</code>, elementwise, in double precision.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.floor#torch.floor" title="torch.floor"><code>floor</code></a>
</td> <td><p>Returns a new tensor with the floor of the elements of <code>input</code>, the largest integer less than or equal to each element.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.floor_divide#torch.floor_divide" title="torch.floor_divide"><code>floor_divide</code></a>
</td> <td></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmod#torch.fmod" title="torch.fmod"><code>fmod</code></a>
</td> <td><p>Applies C++'s <a class="reference external" href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.frac#torch.frac" title="torch.frac"><code>frac</code></a>
</td> <td><p>Computes the fractional portion of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.frexp#torch.frexp" title="torch.frexp"><code>frexp</code></a>
</td> <td><p>Decomposes <code>input</code> into mantissa and exponent tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mtext>mantissa</mtext><mo>×</mo><msup><mn>2</mn><mtext>exponent</mtext></msup></mrow><annotation encoding="application/x-tex">\text{input} = \text{mantissa} \times 2^{\text{exponent}}</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.gradient#torch.gradient" title="torch.gradient"><code>gradient</code></a>
</td> <td><p>Estimates the gradient of a function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mi>n</mi></msup><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">g : \mathbb{R}^n \rightarrow \mathbb{R}</annotation></semantics></math></span></span></span> in one or more dimensions using the <a class="reference external" href="https://www.ams.org/journals/mcom/1988-51-184/S0025-5718-1988-0935077-0/S0025-5718-1988-0935077-0.pdf">second-order accurate central differences method</a> and either first or second order estimates at the boundaries.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.imag#torch.imag" title="torch.imag"><code>imag</code></a>
</td> <td><p>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ldexp#torch.ldexp" title="torch.ldexp"><code>ldexp</code></a>
</td> <td><p>Multiplies <code>input</code> by 2 ** <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lerp#torch.lerp" title="torch.lerp"><code>lerp</code></a>
</td> <td><p>Does a linear interpolation of two tensors <code>start</code> (given by <code>input</code>) and <code>end</code> based on a scalar or tensor <code>weight</code> and returns the resulting <code>out</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lgamma#torch.lgamma" title="torch.lgamma"><code>lgamma</code></a>
</td> <td><p>Computes the natural logarithm of the absolute value of the gamma function on <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log#torch.log" title="torch.log"><code>log</code></a>
</td> <td><p>Returns a new tensor with the natural logarithm of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log10#torch.log10" title="torch.log10"><code>log10</code></a>
</td> <td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log1p#torch.log1p" title="torch.log1p"><code>log1p</code></a>
</td> <td><p>Returns a new tensor with the natural logarithm of (1 + <code>input</code>).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log2#torch.log2" title="torch.log2"><code>log2</code></a>
</td> <td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp#torch.logaddexp" title="torch.logaddexp"><code>logaddexp</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp2#torch.logaddexp2" title="torch.logaddexp2"><code>logaddexp2</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logical_and#torch.logical_and" title="torch.logical_and"><code>logical_and</code></a>
</td> <td><p>Computes the element-wise logical AND of the given input tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logical_not#torch.logical_not" title="torch.logical_not"><code>logical_not</code></a>
</td> <td><p>Computes the element-wise logical NOT of the given input tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logical_or#torch.logical_or" title="torch.logical_or"><code>logical_or</code></a>
</td> <td><p>Computes the element-wise logical OR of the given input tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logical_xor#torch.logical_xor" title="torch.logical_xor"><code>logical_xor</code></a>
</td> <td><p>Computes the element-wise logical XOR of the given input tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logit#torch.logit" title="torch.logit"><code>logit</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.logit" title="torch.special.logit"><code>torch.special.logit()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hypot#torch.hypot" title="torch.hypot"><code>hypot</code></a>
</td> <td><p>Given the legs of a right triangle, return its hypotenuse.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.i0#torch.i0" title="torch.i0"><code>i0</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.i0" title="torch.special.i0"><code>torch.special.i0()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.igamma#torch.igamma" title="torch.igamma"><code>igamma</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.gammainc" title="torch.special.gammainc"><code>torch.special.gammainc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.igammac#torch.igammac" title="torch.igammac"><code>igammac</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.gammaincc" title="torch.special.gammaincc"><code>torch.special.gammaincc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mul#torch.mul" title="torch.mul"><code>mul</code></a>
</td> <td><p>Multiplies <code>input</code> by <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.multiply#torch.multiply" title="torch.multiply"><code>multiply</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.mul#torch.mul" title="torch.mul"><code>torch.mul()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mvlgamma#torch.mvlgamma" title="torch.mvlgamma"><code>mvlgamma</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.multigammaln" title="torch.special.multigammaln"><code>torch.special.multigammaln()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nan_to_num#torch.nan_to_num" title="torch.nan_to_num"><code>nan_to_num</code></a>
</td> <td><p>Replaces <code>NaN</code>, positive infinity, and negative infinity values in <code>input</code> with the values specified by <code>nan</code>, <code>posinf</code>, and <code>neginf</code>, respectively.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>neg</code></a>
</td> <td><p>Returns a new tensor with the negative of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.negative#torch.negative" title="torch.negative"><code>negative</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>torch.neg()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nextafter#torch.nextafter" title="torch.nextafter"><code>nextafter</code></a>
</td> <td><p>Return the next floating-point value after <code>input</code> towards <code>other</code>, elementwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.polygamma#torch.polygamma" title="torch.polygamma"><code>polygamma</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.polygamma" title="torch.special.polygamma"><code>torch.special.polygamma()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.positive#torch.positive" title="torch.positive"><code>positive</code></a>
</td> <td><p>Returns <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.pow#torch.pow" title="torch.pow"><code>pow</code></a>
</td> <td><p>Takes the power of each element in <code>input</code> with <code>exponent</code> and returns a tensor with the result.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantized_batch_norm#torch.quantized_batch_norm" title="torch.quantized_batch_norm"><code>quantized_batch_norm</code></a>
</td> <td><p>Applies batch normalization on a 4D (NCHW) quantized tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantized_max_pool1d#torch.quantized_max_pool1d" title="torch.quantized_max_pool1d"><code>quantized_max_pool1d</code></a>
</td> <td><p>Applies a 1D max pooling over an input quantized tensor composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantized_max_pool2d#torch.quantized_max_pool2d" title="torch.quantized_max_pool2d"><code>quantized_max_pool2d</code></a>
</td> <td><p>Applies a 2D max pooling over an input quantized tensor composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rad2deg#torch.rad2deg" title="torch.rad2deg"><code>rad2deg</code></a>
</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in radians to degrees.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.real#torch.real" title="torch.real"><code>real</code></a>
</td> <td><p>Returns a new tensor containing real values of the <code>self</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.reciprocal#torch.reciprocal" title="torch.reciprocal"><code>reciprocal</code></a>
</td> <td><p>Returns a new tensor with the reciprocal of the elements of <code>input</code></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.remainder#torch.remainder" title="torch.remainder"><code>remainder</code></a>
</td> <td><p>Computes <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations">Python's modulus operation</a> entrywise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.round#torch.round" title="torch.round"><code>round</code></a>
</td> <td><p>Rounds elements of <code>input</code> to the nearest integer.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rsqrt#torch.rsqrt" title="torch.rsqrt"><code>rsqrt</code></a>
</td> <td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sigmoid#torch.sigmoid" title="torch.sigmoid"><code>sigmoid</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.expit" title="torch.special.expit"><code>torch.special.expit()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sign#torch.sign" title="torch.sign"><code>sign</code></a>
</td> <td><p>Returns a new tensor with the signs of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sgn#torch.sgn" title="torch.sgn"><code>sgn</code></a>
</td> <td><p>This function is an extension of torch.sign() to complex tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.signbit#torch.signbit" title="torch.signbit"><code>signbit</code></a>
</td> <td><p>Tests if each element of <code>input</code> has its sign bit set or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sin#torch.sin" title="torch.sin"><code>sin</code></a>
</td> <td><p>Returns a new tensor with the sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sinc#torch.sinc" title="torch.sinc"><code>sinc</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.sinc" title="torch.special.sinc"><code>torch.special.sinc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sinh#torch.sinh" title="torch.sinh"><code>sinh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.softmax#torch.softmax" title="torch.softmax"><code>softmax</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.nn.functional.softmax#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code>torch.nn.functional.softmax()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sqrt#torch.sqrt" title="torch.sqrt"><code>sqrt</code></a>
</td> <td><p>Returns a new tensor with the square-root of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.square#torch.square" title="torch.square"><code>square</code></a>
</td> <td><p>Returns a new tensor with the square of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sub#torch.sub" title="torch.sub"><code>sub</code></a>
</td> <td><p>Subtracts <code>other</code>, scaled by <code>alpha</code>, from <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.subtract#torch.subtract" title="torch.subtract"><code>subtract</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.sub#torch.sub" title="torch.sub"><code>torch.sub()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tan#torch.tan" title="torch.tan"><code>tan</code></a>
</td> <td><p>Returns a new tensor with the tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tanh#torch.tanh" title="torch.tanh"><code>tanh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.true_divide#torch.true_divide" title="torch.true_divide"><code>true_divide</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>torch.div()</code></a> with <code>rounding_mode=None</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>trunc</code></a>
</td> <td><p>Returns a new tensor with the truncated integer values of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.xlogy#torch.xlogy" title="torch.xlogy"><code>xlogy</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.xlogy" title="torch.special.xlogy"><code>torch.special.xlogy()</code></a>.</p></td> </tr>  </table>   <h3 id="reduction-ops">Reduction Ops</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.argmax#torch.argmax" title="torch.argmax"><code>argmax</code></a>
</td> <td><p>Returns the indices of the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.argmin#torch.argmin" title="torch.argmin"><code>argmin</code></a>
</td> <td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.amax#torch.amax" title="torch.amax"><code>amax</code></a>
</td> <td><p>Returns the maximum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.amin#torch.amin" title="torch.amin"><code>amin</code></a>
</td> <td><p>Returns the minimum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.aminmax#torch.aminmax" title="torch.aminmax"><code>aminmax</code></a>
</td> <td><p>Computes the minimum and maximum values of the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.all#torch.all" title="torch.all"><code>all</code></a>
</td> <td><p>Tests if all elements in <code>input</code> evaluate to <code>True</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.any#torch.any" title="torch.any"><code>any</code></a>
</td> <td><p>Tests if any element in <code>input</code> evaluates to <code>True</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.max#torch.max" title="torch.max"><code>max</code></a>
</td> <td><p>Returns the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.min#torch.min" title="torch.min"><code>min</code></a>
</td> <td><p>Returns the minimum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dist#torch.dist" title="torch.dist"><code>dist</code></a>
</td> <td><p>Returns the p-norm of (<code>input</code> - <code>other</code>)</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logsumexp#torch.logsumexp" title="torch.logsumexp"><code>logsumexp</code></a>
</td> <td><p>Returns the log of summed exponentials of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mean#torch.mean" title="torch.mean"><code>mean</code></a>
</td> <td><p>Returns the mean value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nanmean#torch.nanmean" title="torch.nanmean"><code>nanmean</code></a>
</td> <td><p>Computes the mean of all <code>non-NaN</code> elements along the specified dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.median#torch.median" title="torch.median"><code>median</code></a>
</td> <td><p>Returns the median of the values in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nanmedian#torch.nanmedian" title="torch.nanmedian"><code>nanmedian</code></a>
</td> <td><p>Returns the median of the values in <code>input</code>, ignoring <code>NaN</code> values.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mode#torch.mode" title="torch.mode"><code>mode</code></a>
</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the mode value of each row of the <code>input</code> tensor in the given dimension <code>dim</code>, i.e. a value which appears most often in that row, and <code>indices</code> is the index location of each mode value found.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.norm#torch.norm" title="torch.norm"><code>norm</code></a>
</td> <td><p>Returns the matrix norm or vector norm of a given tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nansum#torch.nansum" title="torch.nansum"><code>nansum</code></a>
</td> <td><p>Returns the sum of all elements, treating Not a Numbers (NaNs) as zero.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.prod#torch.prod" title="torch.prod"><code>prod</code></a>
</td> <td><p>Returns the product of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.quantile#torch.quantile" title="torch.quantile"><code>quantile</code></a>
</td> <td><p>Computes the q-th quantiles of each row of the <code>input</code> tensor along the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nanquantile#torch.nanquantile" title="torch.nanquantile"><code>nanquantile</code></a>
</td> <td><p>This is a variant of <a class="reference internal" href="generated/torch.quantile#torch.quantile" title="torch.quantile"><code>torch.quantile()</code></a> that "ignores" <code>NaN</code> values, computing the quantiles <code>q</code> as if <code>NaN</code> values in <code>input</code> did not exist.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.std#torch.std" title="torch.std"><code>std</code></a>
</td> <td><p>Calculates the standard deviation over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.std_mean#torch.std_mean" title="torch.std_mean"><code>std_mean</code></a>
</td> <td><p>Calculates the standard deviation and mean over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sum#torch.sum" title="torch.sum"><code>sum</code></a>
</td> <td><p>Returns the sum of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.unique#torch.unique" title="torch.unique"><code>unique</code></a>
</td> <td><p>Returns the unique elements of the input tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.unique_consecutive#torch.unique_consecutive" title="torch.unique_consecutive"><code>unique_consecutive</code></a>
</td> <td><p>Eliminates all but the first element from every consecutive group of equivalent elements.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.var#torch.var" title="torch.var"><code>var</code></a>
</td> <td><p>Calculates the variance over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.var_mean#torch.var_mean" title="torch.var_mean"><code>var_mean</code></a>
</td> <td><p>Calculates the variance and mean over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.count_nonzero#torch.count_nonzero" title="torch.count_nonzero"><code>count_nonzero</code></a>
</td> <td><p>Counts the number of non-zero values in the tensor <code>input</code> along the given <code>dim</code>.</p></td> </tr>  </table>   <h3 id="comparison-ops">Comparison Ops</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.allclose#torch.allclose" title="torch.allclose"><code>allclose</code></a>
</td> <td><p>This function checks if <code>input</code> and <code>other</code> satisfy the condition:</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.argsort#torch.argsort" title="torch.argsort"><code>argsort</code></a>
</td> <td><p>Returns the indices that sort a tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.eq#torch.eq" title="torch.eq"><code>eq</code></a>
</td> <td><p>Computes element-wise equality</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.equal#torch.equal" title="torch.equal"><code>equal</code></a>
</td> <td><p><code>True</code> if two tensors have the same size and elements, <code>False</code> otherwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ge#torch.ge" title="torch.ge"><code>ge</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.greater_equal#torch.greater_equal" title="torch.greater_equal"><code>greater_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.ge#torch.ge" title="torch.ge"><code>torch.ge()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.gt#torch.gt" title="torch.gt"><code>gt</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.greater#torch.greater" title="torch.greater"><code>greater</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.gt#torch.gt" title="torch.gt"><code>torch.gt()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isclose#torch.isclose" title="torch.isclose"><code>isclose</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is "close" to the corresponding element of <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isfinite#torch.isfinite" title="torch.isfinite"><code>isfinite</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element is <code>finite</code> or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isin#torch.isin" title="torch.isin"><code>isin</code></a>
</td> <td><p>Tests if each element of <code>elements</code> is in <code>test_elements</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isinf#torch.isinf" title="torch.isinf"><code>isinf</code></a>
</td> <td><p>Tests if each element of <code>input</code> is infinite (positive or negative infinity) or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isposinf#torch.isposinf" title="torch.isposinf"><code>isposinf</code></a>
</td> <td><p>Tests if each element of <code>input</code> is positive infinity or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isneginf#torch.isneginf" title="torch.isneginf"><code>isneginf</code></a>
</td> <td><p>Tests if each element of <code>input</code> is negative infinity or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isnan#torch.isnan" title="torch.isnan"><code>isnan</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is NaN or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isreal#torch.isreal" title="torch.isreal"><code>isreal</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is real-valued or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.kthvalue#torch.kthvalue" title="torch.kthvalue"><code>kthvalue</code></a>
</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the <code>k</code> th smallest element of each row of the <code>input</code> tensor in the given dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.le#torch.le" title="torch.le"><code>le</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.less_equal#torch.less_equal" title="torch.less_equal"><code>less_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.le#torch.le" title="torch.le"><code>torch.le()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lt#torch.lt" title="torch.lt"><code>lt</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.less#torch.less" title="torch.less"><code>less</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.lt#torch.lt" title="torch.lt"><code>torch.lt()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.maximum#torch.maximum" title="torch.maximum"><code>maximum</code></a>
</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.minimum#torch.minimum" title="torch.minimum"><code>minimum</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmax#torch.fmax" title="torch.fmax"><code>fmax</code></a>
</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmin#torch.fmin" title="torch.fmin"><code>fmin</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ne#torch.ne" title="torch.ne"><code>ne</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.not_equal#torch.not_equal" title="torch.not_equal"><code>not_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.ne#torch.ne" title="torch.ne"><code>torch.ne()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sort#torch.sort" title="torch.sort"><code>sort</code></a>
</td> <td><p>Sorts the elements of the <code>input</code> tensor along a given dimension in ascending order by value.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.topk#torch.topk" title="torch.topk"><code>topk</code></a>
</td> <td><p>Returns the <code>k</code> largest elements of the given <code>input</code> tensor along a given dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.msort#torch.msort" title="torch.msort"><code>msort</code></a>
</td> <td><p>Sorts the elements of the <code>input</code> tensor along its first dimension in ascending order by value.</p></td> </tr>  </table>   <h3 id="spectral-ops">Spectral Ops</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.stft#torch.stft" title="torch.stft"><code>stft</code></a>
</td> <td><p>Short-time Fourier transform (STFT).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.istft#torch.istft" title="torch.istft"><code>istft</code></a>
</td> <td><p>Inverse short time Fourier Transform.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bartlett_window#torch.bartlett_window" title="torch.bartlett_window"><code>bartlett_window</code></a>
</td> <td><p>Bartlett window function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.blackman_window#torch.blackman_window" title="torch.blackman_window"><code>blackman_window</code></a>
</td> <td><p>Blackman window function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hamming_window#torch.hamming_window" title="torch.hamming_window"><code>hamming_window</code></a>
</td> <td><p>Hamming window function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hann_window#torch.hann_window" title="torch.hann_window"><code>hann_window</code></a>
</td> <td><p>Hann window function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.kaiser_window#torch.kaiser_window" title="torch.kaiser_window"><code>kaiser_window</code></a>
</td> <td><p>Computes the Kaiser window with window length <code>window_length</code> and shape parameter <code>beta</code>.</p></td> </tr>  </table>   <h3 id="other-operations">Other Operations</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.atleast_1d#torch.atleast_1d" title="torch.atleast_1d"><code>atleast_1d</code></a>
</td> <td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atleast_2d#torch.atleast_2d" title="torch.atleast_2d"><code>atleast_2d</code></a>
</td> <td><p>Returns a 2-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atleast_3d#torch.atleast_3d" title="torch.atleast_3d"><code>atleast_3d</code></a>
</td> <td><p>Returns a 3-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bincount#torch.bincount" title="torch.bincount"><code>bincount</code></a>
</td> <td><p>Count the frequency of each value in an array of non-negative ints.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.block_diag#torch.block_diag" title="torch.block_diag"><code>block_diag</code></a>
</td> <td><p>Create a block diagonal matrix from provided tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.broadcast_tensors#torch.broadcast_tensors" title="torch.broadcast_tensors"><code>broadcast_tensors</code></a>
</td> <td><p>Broadcasts the given tensors according to <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">Broadcasting semantics</span></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.broadcast_to#torch.broadcast_to" title="torch.broadcast_to"><code>broadcast_to</code></a>
</td> <td><p>Broadcasts <code>input</code> to the shape <code>shape</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.broadcast_shapes#torch.broadcast_shapes" title="torch.broadcast_shapes"><code>broadcast_shapes</code></a>
</td> <td><p>Similar to <a class="reference internal" href="generated/torch.broadcast_tensors#torch.broadcast_tensors" title="torch.broadcast_tensors"><code>broadcast_tensors()</code></a> but for shapes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bucketize#torch.bucketize" title="torch.bucketize"><code>bucketize</code></a>
</td> <td><p>Returns the indices of the buckets to which each value in the <code>input</code> belongs, where the boundaries of the buckets are set by <code>boundaries</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cartesian_prod#torch.cartesian_prod" title="torch.cartesian_prod"><code>cartesian_prod</code></a>
</td> <td><p>Do cartesian product of the given sequence of tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cdist#torch.cdist" title="torch.cdist"><code>cdist</code></a>
</td> <td><p>Computes batched the p-norm distance between each pair of the two collections of row vectors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.clone#torch.clone" title="torch.clone"><code>clone</code></a>
</td> <td><p>Returns a copy of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.combinations#torch.combinations" title="torch.combinations"><code>combinations</code></a>
</td> <td><p>Compute combinations of length <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span></span></span> of the given tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.corrcoef#torch.corrcoef" title="torch.corrcoef"><code>corrcoef</code></a>
</td> <td><p>Estimates the Pearson product-moment correlation coefficient matrix of the variables given by the <code>input</code> matrix, where rows are the variables and columns are the observations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cov#torch.cov" title="torch.cov"><code>cov</code></a>
</td> <td><p>Estimates the covariance matrix of the variables given by the <code>input</code> matrix, where rows are the variables and columns are the observations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cross#torch.cross" title="torch.cross"><code>cross</code></a>
</td> <td><p>Returns the cross product of vectors in dimension <code>dim</code> of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cummax#torch.cummax" title="torch.cummax"><code>cummax</code></a>
</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative maximum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cummin#torch.cummin" title="torch.cummin"><code>cummin</code></a>
</td> <td><p>Returns a namedtuple <code>(values, indices)</code> where <code>values</code> is the cumulative minimum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cumprod#torch.cumprod" title="torch.cumprod"><code>cumprod</code></a>
</td> <td><p>Returns the cumulative product of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cumsum#torch.cumsum" title="torch.cumsum"><code>cumsum</code></a>
</td> <td><p>Returns the cumulative sum of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diag#torch.diag" title="torch.diag"><code>diag</code></a>
</td> <td>

<ul class="simple"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diag_embed#torch.diag_embed" title="torch.diag_embed"><code>diag_embed</code></a>
</td> <td><p>Creates a tensor whose diagonals of certain 2D planes (specified by <code>dim1</code> and <code>dim2</code>) are filled by <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diagflat#torch.diagflat" title="torch.diagflat"><code>diagflat</code></a>
</td> <td>

<ul class="simple"> <li>If <code>input</code> is a vector (1-D tensor), then returns a 2-D square tensor</li> </ul> </td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diagonal#torch.diagonal" title="torch.diagonal"><code>diagonal</code></a>
</td> <td><p>Returns a partial view of <code>input</code> with the its diagonal elements with respect to <code>dim1</code> and <code>dim2</code> appended as a dimension at the end of the shape.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.diff#torch.diff" title="torch.diff"><code>diff</code></a>
</td> <td><p>Computes the n-th forward difference along the given dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.einsum#torch.einsum" title="torch.einsum"><code>einsum</code></a>
</td> <td><p>Sums the product of the elements of the input <code>operands</code> along dimensions specified using a notation based on the Einstein summation convention.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.flatten#torch.flatten" title="torch.flatten"><code>flatten</code></a>
</td> <td><p>Flattens <code>input</code> by reshaping it into a one-dimensional tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.flip#torch.flip" title="torch.flip"><code>flip</code></a>
</td> <td><p>Reverse the order of an n-D tensor along given axis in dims.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fliplr#torch.fliplr" title="torch.fliplr"><code>fliplr</code></a>
</td> <td><p>Flip tensor in the left/right direction, returning a new tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.flipud#torch.flipud" title="torch.flipud"><code>flipud</code></a>
</td> <td><p>Flip tensor in the up/down direction, returning a new tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.kron#torch.kron" title="torch.kron"><code>kron</code></a>
</td> <td><p>Computes the Kronecker product, denoted by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span></span></span>, of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rot90#torch.rot90" title="torch.rot90"><code>rot90</code></a>
</td> <td><p>Rotate an n-D tensor by 90 degrees in the plane specified by dims axis.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.gcd#torch.gcd" title="torch.gcd"><code>gcd</code></a>
</td> <td><p>Computes the element-wise greatest common divisor (GCD) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.histc#torch.histc" title="torch.histc"><code>histc</code></a>
</td> <td><p>Computes the histogram of a tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.histogram#torch.histogram" title="torch.histogram"><code>histogram</code></a>
</td> <td><p>Computes a histogram of the values in a tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.histogramdd#torch.histogramdd" title="torch.histogramdd"><code>histogramdd</code></a>
</td> <td><p>Computes a multi-dimensional histogram of the values in a tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.meshgrid#torch.meshgrid" title="torch.meshgrid"><code>meshgrid</code></a>
</td> <td><p>Creates grids of coordinates specified by the 1D inputs in <code>attr</code>:tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lcm#torch.lcm" title="torch.lcm"><code>lcm</code></a>
</td> <td><p>Computes the element-wise least common multiple (LCM) of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logcumsumexp#torch.logcumsumexp" title="torch.logcumsumexp"><code>logcumsumexp</code></a>
</td> <td><p>Returns the logarithm of the cumulative summation of the exponentiation of elements of <code>input</code> in the dimension <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ravel#torch.ravel" title="torch.ravel"><code>ravel</code></a>
</td> <td><p>Return a contiguous flattened tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.renorm#torch.renorm" title="torch.renorm"><code>renorm</code></a>
</td> <td><p>Returns a tensor where each sub-tensor of <code>input</code> along dimension <code>dim</code> is normalized such that the <code>p</code>-norm of the sub-tensor is lower than the value <code>maxnorm</code></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.repeat_interleave#torch.repeat_interleave" title="torch.repeat_interleave"><code>repeat_interleave</code></a>
</td> <td><p>Repeat elements of a tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.roll#torch.roll" title="torch.roll"><code>roll</code></a>
</td> <td><p>Roll the tensor <code>input</code> along the given dimension(s).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.searchsorted#torch.searchsorted" title="torch.searchsorted"><code>searchsorted</code></a>
</td> <td><p>Find the indices from the <em>innermost</em> dimension of <code>sorted_sequence</code> such that, if the corresponding values in <code>values</code> were inserted before the indices, when sorted, the order of the corresponding <em>innermost</em> dimension within <code>sorted_sequence</code> would be preserved.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tensordot#torch.tensordot" title="torch.tensordot"><code>tensordot</code></a>
</td> <td><p>Returns a contraction of a and b over multiple dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.trace#torch.trace" title="torch.trace"><code>trace</code></a>
</td> <td><p>Returns the sum of the elements of the diagonal of the input 2-D matrix.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tril#torch.tril" title="torch.tril"><code>tril</code></a>
</td> <td><p>Returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tril_indices#torch.tril_indices" title="torch.tril_indices"><code>tril_indices</code></a>
</td> <td><p>Returns the indices of the lower triangular part of a <code>row</code>-by- <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.triu#torch.triu" title="torch.triu"><code>triu</code></a>
</td> <td><p>Returns the upper triangular part of a matrix (2-D tensor) or batch of matrices <code>input</code>, the other elements of the result tensor <code>out</code> are set to 0.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.triu_indices#torch.triu_indices" title="torch.triu_indices"><code>triu_indices</code></a>
</td> <td><p>Returns the indices of the upper triangular part of a <code>row</code> by <code>col</code> matrix in a 2-by-N Tensor, where the first row contains row coordinates of all indices and the second row contains column coordinates.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.unflatten#torch.unflatten" title="torch.unflatten"><code>unflatten</code></a>
</td> <td><p>Expands a dimension of the input tensor over multiple dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vander#torch.vander" title="torch.vander"><code>vander</code></a>
</td> <td><p>Generates a Vandermonde matrix.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.view_as_real#torch.view_as_real" title="torch.view_as_real"><code>view_as_real</code></a>
</td> <td><p>Returns a view of <code>input</code> as a real tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.view_as_complex#torch.view_as_complex" title="torch.view_as_complex"><code>view_as_complex</code></a>
</td> <td><p>Returns a view of <code>input</code> as a complex tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.resolve_conj#torch.resolve_conj" title="torch.resolve_conj"><code>resolve_conj</code></a>
</td> <td><p>Returns a new tensor with materialized conjugation if <code>input</code>'s conjugate bit is set to <code>True</code>, else returns <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.resolve_neg#torch.resolve_neg" title="torch.resolve_neg"><code>resolve_neg</code></a>
</td> <td><p>Returns a new tensor with materialized negation if <code>input</code>'s negative bit is set to <code>True</code>, else returns <code>input</code>.</p></td> </tr>  </table>   <h3 id="blas-and-lapack-operations">BLAS and LAPACK Operations</h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.addbmm#torch.addbmm" title="torch.addbmm"><code>addbmm</code></a>
</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>batch1</code> and <code>batch2</code>, with a reduced add step (all matrix multiplications get accumulated along the first dimension).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.addmm#torch.addmm" title="torch.addmm"><code>addmm</code></a>
</td> <td><p>Performs a matrix multiplication of the matrices <code>mat1</code> and <code>mat2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.addmv#torch.addmv" title="torch.addmv"><code>addmv</code></a>
</td> <td><p>Performs a matrix-vector product of the matrix <code>mat</code> and the vector <code>vec</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.addr#torch.addr" title="torch.addr"><code>addr</code></a>
</td> <td><p>Performs the outer-product of vectors <code>vec1</code> and <code>vec2</code> and adds it to the matrix <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.baddbmm#torch.baddbmm" title="torch.baddbmm"><code>baddbmm</code></a>
</td> <td><p>Performs a batch matrix-matrix product of matrices in <code>batch1</code> and <code>batch2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bmm#torch.bmm" title="torch.bmm"><code>bmm</code></a>
</td> <td><p>Performs a batch matrix-matrix product of matrices stored in <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.chain_matmul#torch.chain_matmul" title="torch.chain_matmul"><code>chain_matmul</code></a>
</td> <td><p>Returns the matrix product of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span></span></span> 2-D tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cholesky#torch.cholesky" title="torch.cholesky"><code>cholesky</code></a>
</td> <td><p>Computes the Cholesky decomposition of a symmetric positive-definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> or for batches of symmetric positive-definite matrices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cholesky_inverse#torch.cholesky_inverse" title="torch.cholesky_inverse"><code>cholesky_inverse</code></a>
</td> <td><p>Computes the inverse of a symmetric positive-definite matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> using its Cholesky factor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span></span></span>: returns matrix <code>inv</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cholesky_solve#torch.cholesky_solve" title="torch.cholesky_solve"><code>cholesky_solve</code></a>
</td> <td><p>Solves a linear system of equations with a positive semidefinite matrix to be inverted given its Cholesky factor matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>u</mi></mrow><annotation encoding="application/x-tex">u</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dot#torch.dot" title="torch.dot"><code>dot</code></a>
</td> <td><p>Computes the dot product of two 1D tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.geqrf#torch.geqrf" title="torch.geqrf"><code>geqrf</code></a>
</td> <td><p>This is a low-level function for calling LAPACK's geqrf directly.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ger#torch.ger" title="torch.ger"><code>ger</code></a>
</td> <td><p>Alias of <a class="reference internal" href="generated/torch.outer#torch.outer" title="torch.outer"><code>torch.outer()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.inner#torch.inner" title="torch.inner"><code>inner</code></a>
</td> <td><p>Computes the dot product for 1D tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.inverse#torch.inverse" title="torch.inverse"><code>inverse</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.inv#torch.linalg.inv" title="torch.linalg.inv"><code>torch.linalg.inv()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.det#torch.det" title="torch.det"><code>det</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.det#torch.linalg.det" title="torch.linalg.det"><code>torch.linalg.det()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logdet#torch.logdet" title="torch.logdet"><code>logdet</code></a>
</td> <td><p>Calculates log determinant of a square matrix or batches of square matrices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.slogdet#torch.slogdet" title="torch.slogdet"><code>slogdet</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.slogdet#torch.linalg.slogdet" title="torch.linalg.slogdet"><code>torch.linalg.slogdet()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lu#torch.lu" title="torch.lu"><code>lu</code></a>
</td> <td><p>Computes the LU factorization of a matrix or batches of matrices <code>A</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lu_solve#torch.lu_solve" title="torch.lu_solve"><code>lu_solve</code></a>
</td> <td><p>Returns the LU solve of the linear system <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mi>x</mi><mo>=</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">Ax = b</annotation></semantics></math></span></span></span> using the partially pivoted LU factorization of A from <a class="reference internal" href="generated/torch.linalg.lu_factor#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code>lu_factor()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lu_unpack#torch.lu_unpack" title="torch.lu_unpack"><code>lu_unpack</code></a>
</td> <td><p>Unpacks the LU decomposition returned by <a class="reference internal" href="generated/torch.linalg.lu_factor#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code>lu_factor()</code></a> into the <code>P, L, U</code> matrices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.matmul#torch.matmul" title="torch.matmul"><code>matmul</code></a>
</td> <td><p>Matrix product of two tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.matrix_power#torch.matrix_power" title="torch.matrix_power"><code>matrix_power</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_power#torch.linalg.matrix_power" title="torch.linalg.matrix_power"><code>torch.linalg.matrix_power()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.matrix_exp#torch.matrix_exp" title="torch.matrix_exp"><code>matrix_exp</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.matrix_exp#torch.linalg.matrix_exp" title="torch.linalg.matrix_exp"><code>torch.linalg.matrix_exp()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mm#torch.mm" title="torch.mm"><code>mm</code></a>
</td> <td><p>Performs a matrix multiplication of the matrices <code>input</code> and <code>mat2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mv#torch.mv" title="torch.mv"><code>mv</code></a>
</td> <td><p>Performs a matrix-vector product of the matrix <code>input</code> and the vector <code>vec</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.orgqr#torch.orgqr" title="torch.orgqr"><code>orgqr</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.householder_product#torch.linalg.householder_product" title="torch.linalg.householder_product"><code>torch.linalg.householder_product()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ormqr#torch.ormqr" title="torch.ormqr"><code>ormqr</code></a>
</td> <td><p>Computes the matrix-matrix multiplication of a product of Householder matrices with a general matrix.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.outer#torch.outer" title="torch.outer"><code>outer</code></a>
</td> <td><p>Outer product of <code>input</code> and <code>vec2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.pinverse#torch.pinverse" title="torch.pinverse"><code>pinverse</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.linalg.pinv#torch.linalg.pinv" title="torch.linalg.pinv"><code>torch.linalg.pinv()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.qr#torch.qr" title="torch.qr"><code>qr</code></a>
</td> <td><p>Computes the QR decomposition of a matrix or a batch of matrices <code>input</code>, and returns a namedtuple (Q, R) of tensors such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>=</mo><mi>Q</mi><mi>R</mi></mrow><annotation encoding="application/x-tex">\text{input} = Q R</annotation></semantics></math></span></span></span> with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span></span></span> being an orthogonal matrix or batch of orthogonal matrices and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span></span></span> being an upper triangular matrix or batch of upper triangular matrices.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.svd#torch.svd" title="torch.svd"><code>svd</code></a>
</td> <td><p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.svd_lowrank#torch.svd_lowrank" title="torch.svd_lowrank"><code>svd_lowrank</code></a>
</td> <td><p>Return the singular value decomposition <code>(U, S, V)</code> of a matrix, batches of matrices, or a sparse matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>≈</mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A \approx U diag(S) V^T</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.pca_lowrank#torch.pca_lowrank" title="torch.pca_lowrank"><code>pca_lowrank</code></a>
</td> <td><p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lobpcg#torch.lobpcg" title="torch.lobpcg"><code>lobpcg</code></a>
</td> <td><p>Find the k largest (or smallest) eigenvalues and the corresponding eigenvectors of a symmetric positive definite generalized eigenvalue problem using matrix-free LOBPCG methods.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.trapz#torch.trapz" title="torch.trapz"><code>trapz</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.trapezoid#torch.trapezoid" title="torch.trapezoid"><code>torch.trapezoid()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.trapezoid#torch.trapezoid" title="torch.trapezoid"><code>trapezoid</code></a>
</td> <td><p>Computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cumulative_trapezoid#torch.cumulative_trapezoid" title="torch.cumulative_trapezoid"><code>cumulative_trapezoid</code></a>
</td> <td>

<p>Cumulatively computes the <a class="reference external" href="https://en.wikipedia.org/wiki/Trapezoidal_rule">trapezoidal rule</a> along <code>dim</code>.</p> </td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.triangular_solve#torch.triangular_solve" title="torch.triangular_solve"><code>triangular_solve</code></a>
</td> <td><p>Solves a system of equations with a square upper or lower triangular invertible matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> and multiple right-hand sides <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>b</mi></mrow><annotation encoding="application/x-tex">b</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vdot#torch.vdot" title="torch.vdot"><code>vdot</code></a>
</td> <td><p>Computes the dot product of two 1D vectors along a dimension.</p></td> </tr>  </table>   <h3 id="foreach-operations">Foreach Operations</h3> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This API is in beta and subject to future changes. Forward-mode AD is not supported.</p> </div> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_abs#torch._foreach_abs" title="torch._foreach_abs"><code>_foreach_abs</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>torch.abs()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_abs_#torch._foreach_abs_" title="torch._foreach_abs_"><code>_foreach_abs_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>torch.abs()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_acos#torch._foreach_acos" title="torch._foreach_acos"><code>_foreach_acos</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>torch.acos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_acos_#torch._foreach_acos_" title="torch._foreach_acos_"><code>_foreach_acos_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>torch.acos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_asin#torch._foreach_asin" title="torch._foreach_asin"><code>_foreach_asin</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>torch.asin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_asin_#torch._foreach_asin_" title="torch._foreach_asin_"><code>_foreach_asin_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>torch.asin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_atan#torch._foreach_atan" title="torch._foreach_atan"><code>_foreach_atan</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>torch.atan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_atan_#torch._foreach_atan_" title="torch._foreach_atan_"><code>_foreach_atan_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>torch.atan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_ceil#torch._foreach_ceil" title="torch._foreach_ceil"><code>_foreach_ceil</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.ceil#torch.ceil" title="torch.ceil"><code>torch.ceil()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_ceil_#torch._foreach_ceil_" title="torch._foreach_ceil_"><code>_foreach_ceil_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.ceil#torch.ceil" title="torch.ceil"><code>torch.ceil()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_cos#torch._foreach_cos" title="torch._foreach_cos"><code>_foreach_cos</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.cos#torch.cos" title="torch.cos"><code>torch.cos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_cos_#torch._foreach_cos_" title="torch._foreach_cos_"><code>_foreach_cos_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.cos#torch.cos" title="torch.cos"><code>torch.cos()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_cosh#torch._foreach_cosh" title="torch._foreach_cosh"><code>_foreach_cosh</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.cosh#torch.cosh" title="torch.cosh"><code>torch.cosh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_cosh_#torch._foreach_cosh_" title="torch._foreach_cosh_"><code>_foreach_cosh_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.cosh#torch.cosh" title="torch.cosh"><code>torch.cosh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_erf#torch._foreach_erf" title="torch._foreach_erf"><code>_foreach_erf</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.erf#torch.erf" title="torch.erf"><code>torch.erf()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_erf_#torch._foreach_erf_" title="torch._foreach_erf_"><code>_foreach_erf_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.erf#torch.erf" title="torch.erf"><code>torch.erf()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_erfc#torch._foreach_erfc" title="torch._foreach_erfc"><code>_foreach_erfc</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.erfc#torch.erfc" title="torch.erfc"><code>torch.erfc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_erfc_#torch._foreach_erfc_" title="torch._foreach_erfc_"><code>_foreach_erfc_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.erfc#torch.erfc" title="torch.erfc"><code>torch.erfc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_exp#torch._foreach_exp" title="torch._foreach_exp"><code>_foreach_exp</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.exp#torch.exp" title="torch.exp"><code>torch.exp()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_exp_#torch._foreach_exp_" title="torch._foreach_exp_"><code>_foreach_exp_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.exp#torch.exp" title="torch.exp"><code>torch.exp()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_expm1#torch._foreach_expm1" title="torch._foreach_expm1"><code>_foreach_expm1</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.expm1#torch.expm1" title="torch.expm1"><code>torch.expm1()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_expm1_#torch._foreach_expm1_" title="torch._foreach_expm1_"><code>_foreach_expm1_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.expm1#torch.expm1" title="torch.expm1"><code>torch.expm1()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_floor#torch._foreach_floor" title="torch._foreach_floor"><code>_foreach_floor</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.floor#torch.floor" title="torch.floor"><code>torch.floor()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_floor_#torch._foreach_floor_" title="torch._foreach_floor_"><code>_foreach_floor_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.floor#torch.floor" title="torch.floor"><code>torch.floor()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log#torch._foreach_log" title="torch._foreach_log"><code>_foreach_log</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log#torch.log" title="torch.log"><code>torch.log()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log_#torch._foreach_log_" title="torch._foreach_log_"><code>_foreach_log_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log#torch.log" title="torch.log"><code>torch.log()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log10#torch._foreach_log10" title="torch._foreach_log10"><code>_foreach_log10</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log10#torch.log10" title="torch.log10"><code>torch.log10()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log10_#torch._foreach_log10_" title="torch._foreach_log10_"><code>_foreach_log10_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log10#torch.log10" title="torch.log10"><code>torch.log10()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log1p#torch._foreach_log1p" title="torch._foreach_log1p"><code>_foreach_log1p</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log1p#torch.log1p" title="torch.log1p"><code>torch.log1p()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log1p_#torch._foreach_log1p_" title="torch._foreach_log1p_"><code>_foreach_log1p_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log1p#torch.log1p" title="torch.log1p"><code>torch.log1p()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log2#torch._foreach_log2" title="torch._foreach_log2"><code>_foreach_log2</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log2#torch.log2" title="torch.log2"><code>torch.log2()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_log2_#torch._foreach_log2_" title="torch._foreach_log2_"><code>_foreach_log2_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.log2#torch.log2" title="torch.log2"><code>torch.log2()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_neg#torch._foreach_neg" title="torch._foreach_neg"><code>_foreach_neg</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>torch.neg()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_neg_#torch._foreach_neg_" title="torch._foreach_neg_"><code>_foreach_neg_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>torch.neg()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_tan#torch._foreach_tan" title="torch._foreach_tan"><code>_foreach_tan</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.tan#torch.tan" title="torch.tan"><code>torch.tan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_tan_#torch._foreach_tan_" title="torch._foreach_tan_"><code>_foreach_tan_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.tan#torch.tan" title="torch.tan"><code>torch.tan()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sin#torch._foreach_sin" title="torch._foreach_sin"><code>_foreach_sin</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sin#torch.sin" title="torch.sin"><code>torch.sin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sin_#torch._foreach_sin_" title="torch._foreach_sin_"><code>_foreach_sin_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sin#torch.sin" title="torch.sin"><code>torch.sin()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sinh#torch._foreach_sinh" title="torch._foreach_sinh"><code>_foreach_sinh</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sinh#torch.sinh" title="torch.sinh"><code>torch.sinh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sinh_#torch._foreach_sinh_" title="torch._foreach_sinh_"><code>_foreach_sinh_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sinh#torch.sinh" title="torch.sinh"><code>torch.sinh()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_round#torch._foreach_round" title="torch._foreach_round"><code>_foreach_round</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.round#torch.round" title="torch.round"><code>torch.round()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_round_#torch._foreach_round_" title="torch._foreach_round_"><code>_foreach_round_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.round#torch.round" title="torch.round"><code>torch.round()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sqrt#torch._foreach_sqrt" title="torch._foreach_sqrt"><code>_foreach_sqrt</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sqrt#torch.sqrt" title="torch.sqrt"><code>torch.sqrt()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sqrt_#torch._foreach_sqrt_" title="torch._foreach_sqrt_"><code>_foreach_sqrt_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sqrt#torch.sqrt" title="torch.sqrt"><code>torch.sqrt()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_lgamma#torch._foreach_lgamma" title="torch._foreach_lgamma"><code>_foreach_lgamma</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.lgamma#torch.lgamma" title="torch.lgamma"><code>torch.lgamma()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_lgamma_#torch._foreach_lgamma_" title="torch._foreach_lgamma_"><code>_foreach_lgamma_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.lgamma#torch.lgamma" title="torch.lgamma"><code>torch.lgamma()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_frac#torch._foreach_frac" title="torch._foreach_frac"><code>_foreach_frac</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.frac#torch.frac" title="torch.frac"><code>torch.frac()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_frac_#torch._foreach_frac_" title="torch._foreach_frac_"><code>_foreach_frac_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.frac#torch.frac" title="torch.frac"><code>torch.frac()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_reciprocal#torch._foreach_reciprocal" title="torch._foreach_reciprocal"><code>_foreach_reciprocal</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.reciprocal#torch.reciprocal" title="torch.reciprocal"><code>torch.reciprocal()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_reciprocal_#torch._foreach_reciprocal_" title="torch._foreach_reciprocal_"><code>_foreach_reciprocal_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.reciprocal#torch.reciprocal" title="torch.reciprocal"><code>torch.reciprocal()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sigmoid#torch._foreach_sigmoid" title="torch._foreach_sigmoid"><code>_foreach_sigmoid</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sigmoid#torch.sigmoid" title="torch.sigmoid"><code>torch.sigmoid()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_sigmoid_#torch._foreach_sigmoid_" title="torch._foreach_sigmoid_"><code>_foreach_sigmoid_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.sigmoid#torch.sigmoid" title="torch.sigmoid"><code>torch.sigmoid()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_trunc#torch._foreach_trunc" title="torch._foreach_trunc"><code>_foreach_trunc</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_trunc_#torch._foreach_trunc_" title="torch._foreach_trunc_"><code>_foreach_trunc_</code></a>
</td> <td><p>Apply <a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a> to each Tensor of the input list.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._foreach_zero_#torch._foreach_zero_" title="torch._foreach_zero_"><code>_foreach_zero_</code></a>
</td> <td><p>Apply <code>torch.zero()</code> to each Tensor of the input list.</p></td> </tr>  </table>    <h2 id="utilities">Utilities</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.compiled_with_cxx11_abi#torch.compiled_with_cxx11_abi" title="torch.compiled_with_cxx11_abi"><code>compiled_with_cxx11_abi</code></a>
</td> <td><p>Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.result_type#torch.result_type" title="torch.result_type"><code>result_type</code></a>
</td> <td><p>Returns the <a class="reference internal" href="tensor_attributes#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a> that would result from performing an arithmetic operation on the provided input tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.can_cast#torch.can_cast" title="torch.can_cast"><code>can_cast</code></a>
</td> <td><p>Determines if a type conversion is allowed under PyTorch casting rules described in the type promotion <a class="reference internal" href="tensor_attributes#type-promotion-doc"><span class="std std-ref">documentation</span></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.promote_types#torch.promote_types" title="torch.promote_types"><code>promote_types</code></a>
</td> <td><p>Returns the <a class="reference internal" href="tensor_attributes#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a> with the smallest size and scalar kind that is not smaller nor of lower kind than either <code>type1</code> or <code>type2</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.use_deterministic_algorithms#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code>use_deterministic_algorithms</code></a>
</td> <td><p>Sets whether PyTorch operations must use "deterministic" algorithms.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.are_deterministic_algorithms_enabled#torch.are_deterministic_algorithms_enabled" title="torch.are_deterministic_algorithms_enabled"><code>are_deterministic_algorithms_enabled</code></a>
</td> <td><p>Returns True if the global deterministic flag is turned on.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_deterministic_algorithms_warn_only_enabled#torch.is_deterministic_algorithms_warn_only_enabled" title="torch.is_deterministic_algorithms_warn_only_enabled"><code>is_deterministic_algorithms_warn_only_enabled</code></a>
</td> <td><p>Returns True if the global deterministic flag is set to warn only.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_deterministic_debug_mode#torch.set_deterministic_debug_mode" title="torch.set_deterministic_debug_mode"><code>set_deterministic_debug_mode</code></a>
</td> <td><p>Sets the debug mode for deterministic operations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.get_deterministic_debug_mode#torch.get_deterministic_debug_mode" title="torch.get_deterministic_debug_mode"><code>get_deterministic_debug_mode</code></a>
</td> <td><p>Returns the current value of the debug mode for deterministic operations.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_float32_matmul_precision#torch.set_float32_matmul_precision" title="torch.set_float32_matmul_precision"><code>set_float32_matmul_precision</code></a>
</td> <td><p>Sets the internal precision of float32 matrix multiplications.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.get_float32_matmul_precision#torch.get_float32_matmul_precision" title="torch.get_float32_matmul_precision"><code>get_float32_matmul_precision</code></a>
</td> <td><p>Returns the current value of float32 matrix multiplication precision.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.set_warn_always#torch.set_warn_always" title="torch.set_warn_always"><code>set_warn_always</code></a>
</td> <td><p>When this flag is False (default) then some PyTorch warnings may only appear once per process.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.is_warn_always_enabled#torch.is_warn_always_enabled" title="torch.is_warn_always_enabled"><code>is_warn_always_enabled</code></a>
</td> <td><p>Returns True if the global warn_always flag is turned on.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vmap#torch.vmap" title="torch.vmap"><code>vmap</code></a>
</td> <td><p>vmap is the vectorizing map; <code>vmap(func)</code> returns a new function that maps <code>func</code> over some dimension of the inputs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch._assert#torch._assert" title="torch._assert"><code>_assert</code></a>
</td> <td><p>A wrapper around Python's assert which is symbolically traceable.</p></td> </tr>  </table>   <h2 id="symbolic-numbers">Symbolic Numbers</h2> <dl class="py class"> <dt class="sig sig-object py" id="torch.SymInt">
<code>class torch.SymInt(node)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#SymInt"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Like an int (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.SymFloat">
<code>class torch.SymFloat(node)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#SymFloat"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Like an float (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.SymBool">
<code>class torch.SymBool(node)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch.html#SymBool"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Like an bool (including magic methods), but redirects all operations on the wrapped node. This is used in particular to symbolically record operations in the symbolic shape workflow.</p> <p>Unlike regular bools, regular boolean operators will force extra guards instead of symbolically evaluate. Use the bitwise operators instead to handle this.</p> </dd>
</dl> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.sym_float#torch.sym_float" title="torch.sym_float"><code>sym_float</code></a>
</td> <td><p>SymInt-aware utility for float casting.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sym_int#torch.sym_int" title="torch.sym_int"><code>sym_int</code></a>
</td> <td><p>SymInt-aware utility for int casting.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sym_max#torch.sym_max" title="torch.sym_max"><code>sym_max</code></a>
</td> <td><p>SymInt-aware utility for max().</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sym_min#torch.sym_min" title="torch.sym_min"><code>sym_min</code></a>
</td> <td><p>SymInt-aware utility for max().</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sym_not#torch.sym_not" title="torch.sym_not"><code>sym_not</code></a>
</td> <td><p>SymInt-aware utility for logical negation.</p></td> </tr>  </table>   <h2 id="export-path">Export Path</h2>  <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This feature is a prototype and may have compatibility breaking changes in the future.</p> <p>export generated/exportdb/index</p> </div>   <h2 id="optimizations">Optimizations</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.compile#torch.compile" title="torch.compile"><code>compile</code></a>
</td> <td><p>Optimizes given model/function using TorchDynamo and specified backend.</p></td> </tr>  </table> <p><a class="reference external" href="https://pytorch.org/docs/main/compile/index.html">torch.compile documentation</a></p>   <h2 id="operator-tags">Operator Tags</h2> <dl class="py class" id="module-torch.utils.viz"> <dt class="sig sig-object py" id="torch.Tag">
<code>class torch.Tag</code> </dt> <dd>
<p>Members:</p> <p>core</p> <p>data_dependent_output</p> <p>dynamic_output_shape</p> <p>generated</p> <p>inplace_view</p> <p>nondeterministic_bitwise</p> <p>nondeterministic_seeded</p> <p>pointwise</p> <p>view_copy</p> <dl class="py property"> <dt class="sig sig-object py" id="torch.Tag.name">
<code>property name</code> </dt> 
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.html</a>
  </p>
</div>
