<h1 id="torch-profiler">torch.profiler</h1>  <h2 id="overview">Overview</h2> <p id="module-torch.profiler">PyTorch Profiler is a tool that allows the collection of performance metrics during training and inference. Profiler’s context manager API can be used to better understand what model operators are the most expensive, examine their input shapes and stack traces, study device kernel activity and visualize the execution trace.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>An earlier version of the API in <a class="reference internal" href="autograd#module-torch.autograd" title="torch.autograd"><code>torch.autograd</code></a> module is considered legacy and will be deprecated.</p> </div>   <h2 id="api-reference">API Reference</h2> <dl class="py class"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile">
<code>class torch.profiler._KinetoProfile(*, activities=None, record_shapes=False, profile_memory=False, with_stack=False, with_flops=False, with_modules=False, experimental_config=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Low-level profiler wrap the autograd profile</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>activities</strong> (<em>iterable</em>) – list of activity groups (CPU, CUDA) to use in profiling, supported values: <code>torch.profiler.ProfilerActivity.CPU</code>, <code>torch.profiler.ProfilerActivity.CUDA</code>. Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.</li> <li>
<strong>record_shapes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – save information about operator’s input shapes.</li> <li>
<strong>profile_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – track tensor memory allocation/deallocation.</li> <li>
<strong>with_stack</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – record source information (file and line number) for the ops.</li> <li>
<strong>with_flops</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – use formula to estimate the FLOPS of specific operators (matrix multiplication and 2D convolution).</li> <li>
<strong>with_modules</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.</li> <li>
<strong>experimental_config</strong> (<em>_ExperimentalConfig</em>) – A set of experimental options used by profiler libraries like Kineto. Note, backward compatibility is not guaranteed.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This API is experimental and subject to change in the future.</p> <p>Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.</p> </div> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.add_metadata">
<code>add_metadata(key, value)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.add_metadata"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds a user defined metadata with a string key and a string value into the trace file</p>  </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.add_metadata_json">
<code>add_metadata_json(key, value)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.add_metadata_json"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Adds a user defined metadata with a string key and a valid json value into the trace file</p>  </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.events">
<code>events()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.events"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the list of unaggregated profiler events, to be used in the trace callback or after the profiling is finished</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.export_chrome_trace">
<code>export_chrome_trace(path)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.export_chrome_trace"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Exports the collected trace in Chrome JSON format.</p>  </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.export_memory_timeline">
<code>export_memory_timeline(path, device=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.export_memory_timeline"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Extract the memory information from the memory profile collected tree for a given device, and export a timeline plot consisting of [times, [sizes by category]], where times are timestamps and sizes are memory usage for each category. The memory timeline plot will be saved a JSON (by default) or gzipped JSON.</p> <p>Input: (path of file, device) Output: File written as JSON or gzipped JSON</p>  </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.export_stacks">
<code>export_stacks(path, metric='self_cpu_time_total')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.export_stacks"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Save stack traces in a file in a format suitable for visualization.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – save stacks file to this location;</li> <li>
<strong>metric</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – metric to use: “self_cpu_time_total” or “self_cuda_time_total”</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Example of using FlameGraph tool:</p> <ul class="simple"> <li>git clone <a class="reference external" href="https://github.com/brendangregg/FlameGraph">https://github.com/brendangregg/FlameGraph</a>
</li> <li>cd FlameGraph</li> <li>./flamegraph.pl –title “CPU time” –countname “us.” profiler.stacks &gt; perf_viz.svg</li> </ul> </div> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler._KinetoProfile.key_averages">
<code>key_averages(group_by_input_shape=False, group_by_stack_n=0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#_KinetoProfile.key_averages"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Averages events, grouping them by operator name and (optionally) input shapes and stack.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>To use shape/stack functionality make sure to set record_shapes/with_stack when creating profiler context manager.</p> </div>  </dd>
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.profiler.profile">
<code>class torch.profiler.profile(*, activities=None, schedule=None, on_trace_ready=None, record_shapes=False, profile_memory=False, with_stack=False, with_flops=False, with_modules=False, experimental_config=None, use_cuda=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#profile"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Profiler context manager.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>activities</strong> (<em>iterable</em>) – list of activity groups (CPU, CUDA) to use in profiling, supported values: <code>torch.profiler.ProfilerActivity.CPU</code>, <code>torch.profiler.ProfilerActivity.CUDA</code>. Default value: ProfilerActivity.CPU and (when available) ProfilerActivity.CUDA.</li> <li>
<strong>schedule</strong> (<em>Callable</em>) – callable that takes step (int) as a single parameter and returns <code>ProfilerAction</code> value that specifies the profiler action to perform at each step.</li> <li>
<strong>on_trace_ready</strong> (<em>Callable</em>) – callable that is called at each step when <code>schedule</code> returns <code>ProfilerAction.RECORD_AND_SAVE</code> during the profiling.</li> <li>
<strong>record_shapes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – save information about operator’s input shapes.</li> <li>
<strong>profile_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – track tensor memory allocation/deallocation.</li> <li>
<strong>with_stack</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – record source information (file and line number) for the ops.</li> <li>
<strong>with_flops</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – use formula to estimate the FLOPs (floating point operations) of specific operators (matrix multiplication and 2D convolution).</li> <li>
<strong>with_modules</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – record module hierarchy (including function names) corresponding to the callstack of the op. e.g. If module A’s forward call’s module B’s forward which contains an aten::add op, then aten::add’s module hierarchy is A.B Note that this support exist, at the moment, only for TorchScript models and not eager mode models.</li> <li>
<strong>experimental_config</strong> (<em>_ExperimentalConfig</em>) – A set of experimental options used for Kineto library features. Note, backward compatibility is not guaranteed.</li> <li>
<p><strong>use_cuda</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – </p>
<div class="deprecated"> <p><span class="versionmodified deprecated">Deprecated since version 1.8.1: </span>use <code>activities</code> instead.</p> </div> </li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Use <a class="reference internal" href="#torch.profiler.schedule" title="torch.profiler.schedule"><code>schedule()</code></a> to generate the callable schedule. Non-default schedules are useful when profiling long training jobs and allow the user to obtain multiple traces at the different iterations of the training process. The default schedule simply records all the events continuously for the duration of the context manager.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Use <a class="reference internal" href="#torch.profiler.tensorboard_trace_handler" title="torch.profiler.tensorboard_trace_handler"><code>tensorboard_trace_handler()</code></a> to generate result files for TensorBoard:</p> <p><code>on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name)</code></p> <p>After profiling, result files can be found in the specified directory. Use the command:</p> <p><code>tensorboard --logdir dir_name</code></p> <p>to see the results in TensorBoard. For more information, see <a class="reference external" href="https://github.com/pytorch/kineto/tree/master/tb_plugin">PyTorch Profiler TensorBoard Plugin</a></p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Enabling shape and stack tracing results in additional overhead. When record_shapes=True is specified, profiler will temporarily hold references to the tensors; that may further prevent certain optimizations that depend on the reference count and introduce extra tensor copies.</p> </div> <p>Examples:</p> <pre data-language="python">with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ]
) as p:
    code_to_profile()
print(p.key_averages().table(
    sort_by="self_cuda_time_total", row_limit=-1))
</pre> <p>Using the profiler’s <code>schedule</code>, <code>on_trace_ready</code> and <code>step</code> functions:</p> <pre data-language="python"># Non-default profiler schedule allows user to turn profiler on and off
# on different iterations of the training loop;
# trace_handler is called every time a new trace becomes available
def trace_handler(prof):
    print(prof.key_averages().table(
        sort_by="self_cuda_time_total", row_limit=-1))
    # prof.export_chrome_trace("/tmp/test_trace_" + str(prof.step_num) + ".json")

with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],

    # In this example with wait=1, warmup=1, active=2, repeat=1,
    # profiler will skip the first step/iteration,
    # start warming up on the second, record
    # the third and the forth iterations,
    # after which the trace will become available
    # and on_trace_ready (when set) is called;
    # the cycle repeats starting with the next step

    schedule=torch.profiler.schedule(
        wait=1,
        warmup=1,
        active=2,
        repeat=1),
    on_trace_ready=trace_handler
    # on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')
    # used when outputting for tensorboard
    ) as p:
        for iter in range(N):
            code_iteration_to_profile(iter)
            # send a signal to the profiler that the next iteration has started
            p.step()
</pre> <dl class="py method"> <dt class="sig sig-object py" id="torch.profiler.profile.step">
<code>step()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#profile.step"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Signals the profiler that the next profiling step has started.</p> </dd>
</dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.profiler.ProfilerAction">
<code>class torch.profiler.ProfilerAction(value)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#ProfilerAction"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Profiler actions that can be taken at the specified intervals</p> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.profiler.ProfilerActivity">
<code>class torch.profiler.ProfilerActivity</code> </dt> <dd>
<p>Members:</p> <p>CPU</p> <p>XPU</p> <p>MTIA</p> <p>CUDA</p> <dl class="py property"> <dt class="sig sig-object py" id="torch.profiler.ProfilerActivity.name">
<code>property name</code> </dt> 
</dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.schedule">
<code>torch.profiler.schedule(*, wait, warmup, active, repeat=0, skip_first=0)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#schedule"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns a callable that can be used as profiler <code>schedule</code> argument. The profiler will skip the first <code>skip_first</code> steps, then wait for <code>wait</code> steps, then do the warmup for the next <code>warmup</code> steps, then do the active recording for the next <code>active</code> steps and then repeat the cycle starting with <code>wait</code> steps. The optional number of cycles is specified with the <code>repeat</code> parameter, the zero value means that the cycles will continue until the profiling is finished.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a></p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.tensorboard_trace_handler">
<code>torch.profiler.tensorboard_trace_handler(dir_name, worker_name=None, use_gzip=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/profiler.html#tensorboard_trace_handler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Outputs tracing files to directory of <code>dir_name</code>, then that directory can be directly delivered to tensorboard as logdir. <code>worker_name</code> should be unique for each worker in distributed scenario, it will be set to ‘[hostname]_[pid]’ by default.</p>  </dd>
</dl>   <h2 id="intel-instrumentation-and-tracing-technology-apis">Intel Instrumentation and Tracing Technology APIs</h2> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.itt.is_available">
<code>torch.profiler.itt.is_available()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/itt.html#is_available"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Check if ITT feature is available or not</p> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.itt.mark">
<code>torch.profiler.itt.mark(msg)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/itt.html#mark"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Describe an instantaneous event that occurred at some point.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>msg</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – ASCII message to associate with the event.</p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.itt.range_push">
<code>torch.profiler.itt.range_push(msg)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/itt.html#range_push"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Pushes a range onto a stack of nested range span. Returns zero-based depth of the range that is started.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>msg</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – ASCII message to associate with range</p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.profiler.itt.range_pop">
<code>torch.profiler.itt.range_pop()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/profiler/itt.html#range_pop"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Pops a range off of a stack of nested range spans. Returns the zero-based depth of the range that is ended.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/profiler.html" class="_attribution-link">https://pytorch.org/docs/2.1/profiler.html</a>
  </p>
</div>
