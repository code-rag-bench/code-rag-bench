<h1 id="torch-linalg-pinv">torch.linalg.pinv</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.linalg.pinv">
<code>torch.linalg.pinv(A, *, atol=None, rtol=None, hermitian=False, out=None) → Tensor</code> </dt> <dd>
<p>Computes the pseudoinverse (Moore-Penrose inverse) of a matrix.</p> <p>The pseudoinverse may be <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Existence_and_uniqueness">defined algebraically</a> but it is more computationally convenient to understand it <a class="reference external" href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Singular_value_decomposition_(SVD)">through the SVD</a></p> <p>Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if <code>A</code> is a batch of matrices then the output has the same batch dimensions.</p> <p>If <code>hermitian</code><code>= True</code>, <code>A</code> is assumed to be Hermitian if complex or symmetric if real, but this is not checked internally. Instead, just the lower triangular part of the matrix is used in the computations.</p> <p>The singular values (or the norm of the eigenvalues when <code>hermitian</code><code>= True</code>) that are below <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mtext>atol</mtext><mo separator="true">,</mo><msub><mi>σ</mi><mn>1</mn></msub><mo>⋅</mo><mtext>rtol</mtext><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\max(\text{atol}, \sigma_1 \cdot \text{rtol})</annotation></semantics></math></span></span></span> threshold are treated as zero and discarded in the computation, where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">\sigma_1</annotation></semantics></math></span></span></span> is the largest singular value (or eigenvalue).</p> <p>If <code>rtol</code> is not specified and <code>A</code> is a matrix of dimensions <code>(m, n)</code>, the relative tolerance is set to be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>rtol</mtext><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mi>ε</mi></mrow><annotation encoding="application/x-tex">\text{rtol} = \max(m, n) \varepsilon</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ε</mi></mrow><annotation encoding="application/x-tex">\varepsilon</annotation></semantics></math></span></span></span> is the epsilon value for the dtype of <code>A</code> (see <a class="reference internal" href="../type_info#torch.torch.finfo" title="torch.torch.finfo"><code>finfo</code></a>). If <code>rtol</code> is not specified and <code>atol</code> is specified to be larger than zero then <code>rtol</code> is set to zero.</p> <p>If <code>atol</code> or <code>rtol</code> is a <a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>, its shape must be broadcastable to that of the singular values of <code>A</code> as returned by <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function uses <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> if <code>hermitian</code><code>= False</code> and <a class="reference internal" href="torch.linalg.eigh#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> if <code>hermitian</code><code>= True</code>. For CUDA inputs, this function synchronizes that device with the CPU.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Consider using <a class="reference internal" href="torch.linalg.lstsq#torch.linalg.lstsq" title="torch.linalg.lstsq"><code>torch.linalg.lstsq()</code></a> if possible for multiplying a matrix on the left by the pseudoinverse, as:</p> <pre data-language="python">torch.linalg.lstsq(A, B).solution == A.pinv() @ B
</pre> <p>It is always preferred to use <a class="reference internal" href="torch.linalg.lstsq#torch.linalg.lstsq" title="torch.linalg.lstsq"><code>lstsq()</code></a> when possible, as it is faster and more numerically stable than computing the pseudoinverse explicitly.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function has NumPy compatible variant <code>linalg.pinv(A, rcond, hermitian=False)</code>. However, use of the positional argument <code>rcond</code> is deprecated in favor of <code>rtol</code>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function uses internally <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> (or <a class="reference internal" href="torch.linalg.eigh#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> when <code>hermitian</code><code>= True</code>), so its derivative has the same problems as those of these functions. See the warnings in <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> and <a class="reference internal" href="torch.linalg.eigh#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> for more details.</p> </div> <div class="admonition seealso"> <p class="admonition-title">See also</p> <p><a class="reference internal" href="torch.linalg.inv#torch.linalg.inv" title="torch.linalg.inv"><code>torch.linalg.inv()</code></a> computes the inverse of a square matrix.</p> <p><a class="reference internal" href="torch.linalg.lstsq#torch.linalg.lstsq" title="torch.linalg.lstsq"><code>torch.linalg.lstsq()</code></a> computes <code>A</code><code>.pinv() @ </code><code>B</code> with a numerically stable algorithm.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensor of shape <code>(*, m, n)</code> where <code>*</code> is zero or more batch dimensions.</li> <li>
<strong>rcond</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – [NumPy Compat]. Alias for <code>rtol</code>. Default: <code>None</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – the absolute tolerance value. When <code>None</code> it’s considered to be zero. Default: <code>None</code>.</li> <li>
<strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – the relative tolerance value. See above for the value it takes when <code>None</code>. Default: <code>None</code>.</li> <li>
<strong>hermitian</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – indicates whether <code>A</code> is Hermitian if complex or symmetric if real. Default: <code>False</code>.</li> <li>
<strong>out</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – output tensor. Ignored if <code>None</code>. Default: <code>None</code>.</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(3, 5)
&gt;&gt;&gt; A
tensor([[ 0.5495,  0.0979, -1.4092, -0.1128,  0.4132],
        [-1.1143, -0.3662,  0.3042,  1.6374, -0.9294],
        [-0.3269, -0.5745, -0.0382, -0.5922, -0.6759]])
&gt;&gt;&gt; torch.linalg.pinv(A)
tensor([[ 0.0600, -0.1933, -0.2090],
        [-0.0903, -0.0817, -0.4752],
        [-0.7124, -0.1631, -0.2272],
        [ 0.1356,  0.3933, -0.5023],
        [-0.0308, -0.1725, -0.5216]])

&gt;&gt;&gt; A = torch.randn(2, 6, 3)
&gt;&gt;&gt; Apinv = torch.linalg.pinv(A)
&gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))
tensor(8.5633e-07)

&gt;&gt;&gt; A = torch.randn(3, 3, dtype=torch.complex64)
&gt;&gt;&gt; A = A + A.T.conj()  # creates a Hermitian matrix
&gt;&gt;&gt; Apinv = torch.linalg.pinv(A, hermitian=True)
&gt;&gt;&gt; torch.dist(Apinv @ A, torch.eye(3))
tensor(1.0830e-06)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.linalg.pinv.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.linalg.pinv.html</a>
  </p>
</div>
