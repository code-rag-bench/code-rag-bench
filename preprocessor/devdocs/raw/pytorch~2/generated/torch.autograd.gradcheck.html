<h1 id="torch-autograd-gradcheck">torch.autograd.gradcheck</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.gradcheck">
<code>torch.autograd.gradcheck(func, inputs, *, eps=1e-06, atol=1e-05, rtol=0.001, raise_exception=True, check_sparse_nnz=None, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_batched_forward_grad=False, check_forward_ad=False, check_backward_ad=True, fast_mode=False, masked=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/gradcheck.html#gradcheck"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Check gradients computed via small finite differences against analytical gradients wrt tensors in <code>inputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p> <p>The check between numerical and analytical gradients uses <a class="reference internal" href="torch.allclose#torch.allclose" title="torch.allclose"><code>allclose()</code></a>.</p> <p>For most of the complex functions we consider for optimization purposes, no notion of Jacobian exists. Instead, gradcheck verifies if the numerical and analytical values of the Wirtinger and Conjugate Wirtinger derivatives are consistent. Because the gradient computation is done under the assumption that the overall function has a real-valued output, we treat functions with complex output in a special way. For these functions, gradcheck is applied to two real-valued functions corresponding to taking the real components of the complex outputs for the first, and taking the imaginary components of the complex outputs for the second. For more details, check out <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html#complex-autograd-doc"><span class="std std-ref">Autograd for Complex Numbers</span></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The default values are designed for <code>input</code> of double precision. This check will likely fail if <code>input</code> is of less precision, e.g., <code>FloatTensor</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Gradcheck may fail when evaluated on non-differentiable points because the numerically computed gradients via finite differencing may differ those computed analytically (not necessarily because either is incorrect). For more context, see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html#non-differentiable-func-grad"><span class="std std-ref">Gradients for non-differentiable functions</span></a>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If any checked tensor in <code>input</code> has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from <code>torch.expand()</code>), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – perturbation for finite differences</li> <li>
<strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – absolute tolerance</li> <li>
<strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – relative tolerance</li> <li>
<strong>raise_exception</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks.</li> <li>
<strong>check_sparse_nnz</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, gradcheck allows for SparseTensor input, and for any SparseTensor inputs, gradcheck will perform its check at <code>nnz</code> positions only. The <code>check_sparse_nnz</code> argument is deprecated, use the <code>masked</code> argument instead. If <code>check_sparse_nnz != masked</code>, an exception is raised.</li> <li>
<strong>nondet_tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance.</li> <li>
<strong>check_undefined_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, check if undefined output grads are supported and treated as zeros, for <code>Tensor</code> outputs.</li> <li>
<strong>check_batched_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, check if we can compute batched gradients using prototype vmap support. Defaults to False.</li> <li>
<strong>check_batched_forward_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, checks if we can compute batched forward gradients using forward ad and prototype vmap support. Defaults to <code>False</code>.</li> <li>
<strong>check_forward_ad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, check that the gradients computed with forward mode AD match the numerical ones. Defaults to <code>False</code>.</li> <li>
<strong>check_backward_ad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>False</code>, do not perform any checks that rely on backward mode AD to be implemented. Defaults to <code>True</code>.</li> <li>
<strong>fast_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – Fast mode for gradcheck and gradgradcheck is currently only implemented for R to R functions. If none of the inputs and outputs are complex a faster implementation of gradcheck that no longer computes the entire jacobian is run; otherwise, we fall back to the slow implementation.</li> <li>
<strong>masked</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, the gradients of unspecified elements of sparse tensors are ignored. Defaults to <code>False</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p><code>True</code> if all differences satisfy allclose condition</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.gradcheck.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.gradcheck.html</a>
  </p>
</div>
