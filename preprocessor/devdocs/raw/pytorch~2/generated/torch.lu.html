<h1 id="torch-lu">torch.lu</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.lu">
<code>torch.lu(*args, **kwargs)</code> </dt> <dd>
<p>Computes the LU factorization of a matrix or batches of matrices <code>A</code>. Returns a tuple containing the LU factorization and pivots of <code>A</code>. Pivoting is done if <code>pivot</code> is set to <code>True</code>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> is deprecated in favor of <a class="reference internal" href="torch.linalg.lu_factor#torch.linalg.lu_factor" title="torch.linalg.lu_factor"><code>torch.linalg.lu_factor()</code></a> and <a class="reference internal" href="torch.linalg.lu_factor_ex#torch.linalg.lu_factor_ex" title="torch.linalg.lu_factor_ex"><code>torch.linalg.lu_factor_ex()</code></a>. <a class="reference internal" href="#torch.lu" title="torch.lu"><code>torch.lu()</code></a> will be removed in a future PyTorch release. <code>LU, pivots, info = torch.lu(A, compute_pivots)</code> should be replaced with</p> <pre data-language="python">LU, pivots = torch.linalg.lu_factor(A, compute_pivots)
</pre> <p><code>LU, pivots, info = torch.lu(A, compute_pivots, get_infos=True)</code> should be replaced with</p> <pre data-language="python">LU, pivots, info = torch.linalg.lu_factor_ex(A, compute_pivots)
</pre> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <ul class="simple"> <li>The returned permutation matrix for every matrix in the batch is represented by a 1-indexed vector of size <code>min(A.shape[-2], A.shape[-1])</code>. <code>pivots[i] == j</code> represents that in the <code>i</code>-th step of the algorithm, the <code>i</code>-th row was permuted with the <code>j-1</code>-th row.</li> <li>LU factorization with <code>pivot</code> = <code>False</code> is not available for CPU, and attempting to do so will throw an error. However, LU factorization with <code>pivot</code> = <code>False</code> is available for CUDA.</li> <li>This function does not check if the factorization was successful or not if <code>get_infos</code> is <code>True</code> since the status of the factorization is present in the third element of the return tuple.</li> <li>In the case of batches of square matrices with size less or equal to 32 on a CUDA device, the LU factorization is repeated for singular matrices due to the bug in the MAGMA library (see magma issue 13).</li> <li>
<code>L</code>, <code>U</code>, and <code>P</code> can be derived using <a class="reference internal" href="torch.lu_unpack#torch.lu_unpack" title="torch.lu_unpack"><code>torch.lu_unpack()</code></a>.</li> </ul> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The gradients of this function will only be finite when <code>A</code> is full rank. This is because the LU decomposition is just differentiable at full rank matrices. Furthermore, if <code>A</code> is close to not being full rank, the gradient will be numerically unstable as it depends on the computation of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>L</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">L^{-1}</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>U</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">U^{-1}</annotation></semantics></math></span></span></span>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the tensor to factor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span></span>
</li> <li>
<strong>pivot</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – controls whether pivoting is done. Default: <code>True</code>
</li> <li>
<strong>get_infos</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if set to <code>True</code>, returns an info IntTensor. Default: <code>False</code>
</li> <li>
<strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – optional output tuple. If <code>get_infos</code> is <code>True</code>, then the elements in the tuple are Tensor, IntTensor, and IntTensor. If <code>get_infos</code> is <code>False</code>, then the elements in the tuple are Tensor, IntTensor. Default: <code>None</code>
</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<p>A tuple of tensors containing</p>  <ul class="simple"> <li>
<strong>factorization</strong> (<em>Tensor</em>): the factorization of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span></span>
</li> <li>
<strong>pivots</strong> (<em>IntTensor</em>): the pivots of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mtext>min</mtext><mo stretchy="false">(</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, \text{min}(m, n))</annotation></semantics></math></span></span></span>. <code>pivots</code> stores all the intermediate transpositions of rows. The final permutation <code>perm</code> could be reconstructed by applying <code>swap(perm[i], perm[pivots[i] - 1])</code> for <code>i = 0, ..., pivots.size(-1) - 1</code>, where <code>perm</code> is initially the identity permutation of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi></mrow><annotation encoding="application/x-tex">m</annotation></semantics></math></span></span></span> elements (essentially this is what <a class="reference internal" href="torch.lu_unpack#torch.lu_unpack" title="torch.lu_unpack"><code>torch.lu_unpack()</code></a> is doing).</li> <li>
<strong>infos</strong> (<em>IntTensor</em>, <em>optional</em>): if <code>get_infos</code> is <code>True</code>, this is a tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*)</annotation></semantics></math></span></span></span> where non-zero values indicate whether factorization for the matrix or each minibatch has succeeded or failed</li> </ul>  </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>(<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>, IntTensor, IntTensor (optional))</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(2, 3, 3)
&gt;&gt;&gt; A_LU, pivots = torch.lu(A)
&gt;&gt;&gt; A_LU
tensor([[[ 1.3506,  2.5558, -0.0816],
         [ 0.1684,  1.1551,  0.1940],
         [ 0.1193,  0.6189, -0.5497]],

        [[ 0.4526,  1.2526, -0.3285],
         [-0.7988,  0.7175, -0.9701],
         [ 0.2634, -0.9255, -0.3459]]])
&gt;&gt;&gt; pivots
tensor([[ 3,  3,  3],
        [ 3,  3,  3]], dtype=torch.int32)
&gt;&gt;&gt; A_LU, pivots, info = torch.lu(A, get_infos=True)
&gt;&gt;&gt; if info.nonzero().size(0) == 0:
...     print('LU factorization succeeded for all samples!')
LU factorization succeeded for all samples!
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.lu.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.lu.html</a>
  </p>
</div>
