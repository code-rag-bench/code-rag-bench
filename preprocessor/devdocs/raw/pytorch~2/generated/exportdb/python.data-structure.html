<h1 id="python-data-structure">python.data-structure</h1>  <h2 id="dictionary">dictionary</h2> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Tags: <a class="reference internal" href="#"><span class="doc">python.data-structure</span></a></p> <p>Support Level: SUPPORTED</p> </div> <p>Original source code:</p> <pre data-language="python">import torch



def dictionary(x, y):
    """
    Dictionary structures are inlined and flattened along tracing.
    """
    elements = {}
    elements["x2"] = x * x
    y = y * elements["x2"]
    return {"y": y}
</pre> <p>Result:</p> <pre data-language="python">ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, arg0_1: f32[3, 2], arg1_1: i64[]):
            #
            mul: f32[3, 2] = torch.ops.aten.mul.Tensor(arg0_1, arg0_1);  arg0_1 = None
            mul_1: f32[3, 2] = torch.ops.aten.mul.Tensor(arg1_1, mul);  arg1_1 = mul = None
            return (mul_1,)

Graph Signature: ExportGraphSignature(parameters=[], buffers=[], user_inputs=['arg0_1', 'arg1_1'], user_outputs=['mul_1'], inputs_to_parameters={}, inputs_to_buffers={}, buffers_to_mutate={}, backward_signature=None, assertion_dep_token=None)
Symbol to range: {}
</pre>   <h2 id="fn-with-kwargs">fn_with_kwargs</h2> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Tags: <a class="reference internal" href="#"><span class="doc">python.data-structure</span></a></p> <p>Support Level: SUPPORTED</p> </div> <p>Original source code:</p> <pre data-language="python">import torch



def fn_with_kwargs(pos0, tuple0, *myargs, mykw0, **mykwargs):
    """
    Keyword arguments are not supported at the moment.
    """
    out = pos0
    for arg in tuple0:
        out = out * arg
    for arg in myargs:
        out = out * arg
    out = out * mykw0
    out = out * mykwargs["input0"] * mykwargs["input1"]
    return out
</pre> <p>Result:</p> <pre data-language="python">ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, arg0_1: f32[4], arg1_1: f32[4], arg2_1: f32[4], arg3_1: f32[4], arg4_1: f32[4], arg5_1: f32[4], arg6_1: f32[4], arg7_1: f32[4]):
            #
            mul: f32[4] = torch.ops.aten.mul.Tensor(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
            mul_1: f32[4] = torch.ops.aten.mul.Tensor(mul, arg2_1);  mul = arg2_1 = None
            mul_2: f32[4] = torch.ops.aten.mul.Tensor(mul_1, arg3_1);  mul_1 = arg3_1 = None
            mul_3: f32[4] = torch.ops.aten.mul.Tensor(mul_2, arg4_1);  mul_2 = arg4_1 = None
            mul_4: f32[4] = torch.ops.aten.mul.Tensor(mul_3, arg5_1);  mul_3 = arg5_1 = None
            mul_5: f32[4] = torch.ops.aten.mul.Tensor(mul_4, arg6_1);  mul_4 = arg6_1 = None
            mul_6: f32[4] = torch.ops.aten.mul.Tensor(mul_5, arg7_1);  mul_5 = arg7_1 = None
            return (mul_6,)

Graph Signature: ExportGraphSignature(parameters=[], buffers=[], user_inputs=['arg0_1', 'arg1_1', 'arg2_1', 'arg3_1', 'arg4_1', 'arg5_1', 'arg6_1', 'arg7_1'], user_outputs=['mul_6'], inputs_to_parameters={}, inputs_to_buffers={}, buffers_to_mutate={}, backward_signature=None, assertion_dep_token=None)
Symbol to range: {}
</pre>   <h2 id="list-contains">list_contains</h2> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Tags: <a class="reference internal" href="torch.dynamic-shape"><span class="doc">torch.dynamic-shape</span></a>, <a class="reference internal" href="python.assert"><span class="doc">python.assert</span></a>, <a class="reference internal" href="#"><span class="doc">python.data-structure</span></a></p> <p>Support Level: SUPPORTED</p> </div> <p>Original source code:</p> <pre data-language="python">import torch



def list_contains(x):
    """
    List containment relation can be checked on a dynamic shape or constants.
    """
    assert x.size(-1) in [6, 2]
    assert x.size(0) not in [4, 5, 6]
    assert "monkey" not in ["cow", "pig"]
    return x + x
</pre> <p>Result:</p> <pre data-language="python">ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, arg0_1: f32[3, 2]):
            #
            add: f32[3, 2] = torch.ops.aten.add.Tensor(arg0_1, arg0_1);  arg0_1 = None
            return (add,)

Graph Signature: ExportGraphSignature(parameters=[], buffers=[], user_inputs=['arg0_1'], user_outputs=['add'], inputs_to_parameters={}, inputs_to_buffers={}, buffers_to_mutate={}, backward_signature=None, assertion_dep_token=None)
Symbol to range: {}
</pre>   <h2 id="list-unpack">list_unpack</h2> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Tags: <a class="reference internal" href="#"><span class="doc">python.data-structure</span></a>, <a class="reference internal" href="python.control-flow"><span class="doc">python.control-flow</span></a></p> <p>Support Level: SUPPORTED</p> </div> <p>Original source code:</p> <pre data-language="python">from typing import List

import torch



def list_unpack(args: List[torch.Tensor]):
    """
    Lists are treated as static construct, therefore unpacking should be
    erased after tracing.
    """
    x, *y = args
    return x + y[0]
</pre> <p>Result:</p> <pre data-language="python">ExportedProgram:
    class GraphModule(torch.nn.Module):
        def forward(self, arg0_1: f32[3, 2], arg1_1: i64[], arg2_1: i64[]):
            #
            add: f32[3, 2] = torch.ops.aten.add.Tensor(arg0_1, arg1_1);  arg0_1 = arg1_1 = None
            return (add,)

Graph Signature: ExportGraphSignature(parameters=[], buffers=[], user_inputs=['arg0_1', 'arg1_1', 'arg2_1'], user_outputs=['add'], inputs_to_parameters={}, inputs_to_buffers={}, buffers_to_mutate={}, backward_signature=None, assertion_dep_token=None)
Symbol to range: {}
</pre><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/exportdb/python.data-structure.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/exportdb/python.data-structure.html</a>
  </p>
</div>
