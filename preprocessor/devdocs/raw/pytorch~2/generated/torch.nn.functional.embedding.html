<h1 id="torch-nn-functional-embedding">torch.nn.functional.embedding</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.functional.embedding">
<code>torch.nn.functional.embedding(input, weight, padding_idx=None, max_norm=None, norm_type=2.0, scale_grad_by_freq=False, sparse=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/functional.html#embedding"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p> <p>This module is often used to retrieve word embeddings using indices. The input to the module is a list of indices, and the embedding matrix, and the output is the corresponding word embeddings.</p> <p>See <a class="reference internal" href="torch.nn.embedding#torch.nn.Embedding" title="torch.nn.Embedding"><code>torch.nn.Embedding</code></a> for more details.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Note that the analytical gradients of this function with respect to entries in <code>weight</code> at the row specified by <code>padding_idx</code> are expected to differ from the numerical ones.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Note that <code>:class:`torch.nn.Embedding</code> differs from this function in that it initializes the row of <code>weight</code> specified by <code>padding_idx</code> to all zeros on construction.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<em>LongTensor</em>) – Tensor containing indices into the embedding matrix</li> <li>
<strong>weight</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – The embedding matrix with number of rows equal to the maximum possible index + 1, and number of columns equal to the embedding size</li> <li>
<strong>padding_idx</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – If specified, the entries at <code>padding_idx</code> do not contribute to the gradient; therefore, the embedding vector at <code>padding_idx</code> is not updated during training, i.e. it remains as a fixed “pad”.</li> <li>
<strong>max_norm</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – If given, each embedding vector with norm larger than <code>max_norm</code> is renormalized to have norm <code>max_norm</code>. Note: this will modify <code>weight</code> in-place.</li> <li>
<strong>norm_type</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – The p of the p-norm to compute for the <code>max_norm</code> option. Default <code>2</code>.</li> <li>
<strong>scale_grad_by_freq</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If given, this will scale gradients by the inverse of frequency of the words in the mini-batch. Default <code>False</code>.</li> <li>
<strong>sparse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, gradient w.r.t. <code>weight</code> will be a sparse tensor. See Notes under <a class="reference internal" href="torch.nn.embedding#torch.nn.Embedding" title="torch.nn.Embedding"><code>torch.nn.Embedding</code></a> for more details regarding sparse gradients.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: LongTensor of arbitrary shape containing the indices to extract</li> <li>Weight: Embedding matrix of floating point type with shape <code>(V, embedding_dim)</code>, where V = maximum index + 1 and embedding_dim = the embedding size</li> <li>Output: <code>(*, embedding_dim)</code>, where <code>*</code> is the input shape</li> </ul> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; # a batch of 2 samples of 4 indices each
&gt;&gt;&gt; input = torch.tensor([[1, 2, 4, 5], [4, 3, 2, 9]])
&gt;&gt;&gt; # an embedding matrix containing 10 tensors of size 3
&gt;&gt;&gt; embedding_matrix = torch.rand(10, 3)
&gt;&gt;&gt; F.embedding(input, embedding_matrix)
tensor([[[ 0.8490,  0.9625,  0.6753],
         [ 0.9666,  0.7761,  0.6108],
         [ 0.6246,  0.9751,  0.3618],
         [ 0.4161,  0.2419,  0.7383]],

        [[ 0.6246,  0.9751,  0.3618],
         [ 0.0237,  0.7794,  0.0528],
         [ 0.9666,  0.7761,  0.6108],
         [ 0.3385,  0.8612,  0.1867]]])

&gt;&gt;&gt; # example with padding_idx
&gt;&gt;&gt; weights = torch.rand(10, 3)
&gt;&gt;&gt; weights[0, :].zero_()
&gt;&gt;&gt; embedding_matrix = weights
&gt;&gt;&gt; input = torch.tensor([[0, 2, 0, 5]])
&gt;&gt;&gt; F.embedding(input, embedding_matrix, padding_idx=0)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.5609,  0.5384,  0.8720],
         [ 0.0000,  0.0000,  0.0000],
         [ 0.6262,  0.2438,  0.7471]]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.functional.embedding.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.functional.embedding.html</a>
  </p>
</div>
