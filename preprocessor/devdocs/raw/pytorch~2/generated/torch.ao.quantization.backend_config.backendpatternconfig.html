<h1 id="backendpatternconfig">BackendPatternConfig</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig">
<code>class torch.ao.quantization.backend_config.BackendPatternConfig(pattern=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Config object that specifies quantization behavior for a given operator pattern. For a detailed example usage, see <a class="reference internal" href="torch.ao.quantization.backend_config.backendconfig#torch.ao.quantization.backend_config.BackendConfig" title="torch.ao.quantization.backend_config.BackendConfig"><code>BackendConfig</code></a>.</p>  <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.add_dtype_config">
<code>add_dtype_config(dtype_config)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.add_dtype_config"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Add a set of supported data types passed as arguments to quantize ops in the reference model spec.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.from_dict">
<code>classmethod from_dict(backend_pattern_config_dict)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.from_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Create a <code>BackendPatternConfig</code> from a dictionary with the following items:</p>  <p>“pattern”: the pattern being configured “observation_type”: the <a class="reference internal" href="torch.ao.quantization.backend_config.observationtype#torch.ao.quantization.backend_config.ObservationType" title="torch.ao.quantization.backend_config.ObservationType"><code>ObservationType</code></a> that specifies how observers should be inserted for this pattern “dtype_configs”: a list of dictionaries that represents <a class="reference internal" href="torch.ao.quantization.backend_config.dtypeconfig#torch.ao.quantization.backend_config.DTypeConfig" title="torch.ao.quantization.backend_config.DTypeConfig"><code>DTypeConfig</code></a> s “root_module”: a <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> that represents the root for this pattern “qat_module”: a <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> that represents the QAT implementation for this pattern “reference_quantized_module”: a <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> that represents the reference quantized implementation for this pattern’s root module. “fused_module”: a <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> that represents the fused implementation for this pattern “fuser_method”: a function that specifies how to fuse the pattern for this pattern “pattern_complex_format”: the pattern specified in the reversed nested tuple format (deprecated)</p>  <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_dtype_configs">
<code>set_dtype_configs(dtype_configs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_dtype_configs"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the supported data types passed as arguments to quantize ops in the reference model spec, overriding all previously registered data types.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_fused_module">
<code>set_fused_module(fused_module)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_fused_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the module that represents the fused implementation for this pattern.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_fuser_method">
<code>set_fuser_method(fuser_method)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_fuser_method"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the function that specifies how to fuse this BackendPatternConfig’s pattern.</p> <p>The first argument of this function should be <code>is_qat</code>, and the rest of the arguments should be the items in the tuple pattern. The return value of this function should be the resulting fused module.</p> <p>For example, the fuser method for the pattern <code>(torch.nn.Linear, torch.nn.ReLU)</code> can be:</p>  <dl class="simple"> <dt>def fuse_linear_relu(is_qat, linear, relu):</dt>
<dd>
<p>return torch.ao.nn.intrinsic.LinearReLU(linear, relu)</p> </dd> </dl>  <p>For a more complicated example, see <a class="reference external" href="https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6">https://gist.github.com/jerryzh168/8bea7180a8ba3c279f2c9b050f2a69a6</a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_observation_type">
<code>set_observation_type(observation_type)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_observation_type"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set how observers should be inserted in the graph for this pattern.</p> <p>Observation type here refers to how observers (or quant-dequant ops) will be placed in the graph. This is used to produce the desired reference patterns understood by the backend. Weighted ops such as linear and conv require different observers (or quantization parameters passed to quantize ops in the reference model) for the input and the output.</p> <p>There are two observation types:</p>  <p><code>OUTPUT_USE_DIFFERENT_OBSERVER_AS_INPUT</code> (default): the output observer instance will be different from the input. This is the most common observation type.</p> <p><code>OUTPUT_SHARE_OBSERVER_WITH_INPUT</code>: the output observer instance will be the same as the input. This is useful for operators like <code>cat</code>.</p>  <p>Note: This will be renamed in the near future, since we will soon insert QuantDeQuantStubs with observers (and fake quantizes) attached instead of observers themselves.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_pattern">
<code>set_pattern(pattern)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_pattern"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the pattern to configure.</p> <p>The pattern can be a float module, functional operator, pytorch operator, or a tuple combination of the above. Tuple patterns are treated as sequential patterns, and currently only tuples of 2 or 3 elements are supported.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_qat_module">
<code>set_qat_module(qat_module)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_qat_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the module that represents the QAT implementation for this pattern.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_reference_quantized_module">
<code>set_reference_quantized_module(reference_quantized_module)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_reference_quantized_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the module that represents the reference quantized implementation for this pattern’s root module.</p> <p>For more detail, see <a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module" title="torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module"><code>set_root_module()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.set_root_module">
<code>set_root_module(root_module)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.set_root_module"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Set the module that represents the root for this pattern.</p> <p>When we construct the reference quantized model during the convert phase, the root modules (e.g. torch.nn.Linear for torch.ao.nn.intrinsic.LinearReLU) will be swapped to the corresponding reference quantized modules (e.g. torch.ao.nn.reference.quantized.Linear). This allows custom backends to specify custom reference quantized module implementations to match the numerics of their lowered operators. Since this is a one-to-one mapping, both the root module and the reference quantized module must be specified in the same BackendPatternConfig in order for the conversion to take place.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig" title="torch.ao.quantization.backend_config.backend_config.BackendPatternConfig">BackendPatternConfig</a></p> </dd> </dl> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.ao.quantization.backend_config.BackendPatternConfig.to_dict">
<code>to_dict()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/quantization/backend_config/backend_config.html#BackendPatternConfig.to_dict"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Convert this <code>BackendPatternConfig</code> to a dictionary with the items described in <a class="reference internal" href="#torch.ao.quantization.backend_config.BackendPatternConfig.from_dict" title="torch.ao.quantization.backend_config.BackendPatternConfig.from_dict"><code>from_dict()</code></a>.</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a>[<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>, <a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a>]</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.quantization.backend_config.BackendPatternConfig.html</a>
  </p>
</div>
