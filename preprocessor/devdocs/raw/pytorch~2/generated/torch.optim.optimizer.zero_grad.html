<h1 id="torch-optim-optimizer-zero-grad">torch.optim.Optimizer.zero_grad</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.Optimizer.zero_grad">
<code>Optimizer.zero_grad(set_to_none=True)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/optimizer.html#Optimizer.zero_grad"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Resets the gradients of all optimized <a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> s.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) â€“ instead of setting to zero, set the grads to None. This will in general have lower memory footprint, and can modestly improve performance. However, it changes certain behaviors. For example: 1. When the user tries to access a gradient and perform manual ops on it, a None attribute or a Tensor full of 0s will behave differently. 2. If the user requests <code>zero_grad(set_to_none=True)</code> followed by a backward pass, <code>.grad</code>s are guaranteed to be None for params that did not receive a gradient. 3. <code>torch.optim</code> optimizers have a different behavior if the gradient is 0 or None (in one case it does the step with a gradient of 0 and in the other it skips the step altogether).</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.zero_grad.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.optim.Optimizer.zero_grad.html</a>
  </p>
</div>
