<h1 id="torch-sparse-compressed-tensor">torch.sparse_compressed_tensor</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.sparse_compressed_tensor">
<code>torch.sparse_compressed_tensor(compressed_indices, plain_indices, values, size=None, *, dtype=None, layout=None, device=None, requires_grad=False, check_invariants=None) → Tensor</code> </dt> <dd>
<p>Constructs a <a class="reference internal" href="../sparse#sparse-compressed-docs"><span class="std std-ref">sparse tensor in Compressed Sparse format - CSR, CSC, BSR, or BSC -</span></a> with specified values at the given <code>compressed_indices</code> and <code>plain_indices</code>. Sparse matrix multiplication operations in Compressed Sparse format are typically faster than that for sparse tensors in COO format. Make you have a look at <a class="reference internal" href="../sparse#sparse-compressed-docs"><span class="std std-ref">the note on the data type of the indices</span></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the <code>device</code> argument is not specified the device of the given <code>values</code> and indices tensor(s) must match. If, however, the argument is specified the input Tensors will be converted to the given device and in turn determine the device of the constructed sparse tensor.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>compressed_indices</strong> (<em>array_like</em>) – (B+1)-dimensional array of size <code>(*batchsize, compressed_dim_size + 1)</code>. The last element of each batch is the number of non-zero elements or blocks. This tensor encodes the index in <code>values</code> and <code>plain_indices</code> depending on where the given compressed dimension (row or column) starts. Each successive number in the tensor subtracted by the number before it denotes the number of elements or blocks in a given compressed dimension.</li> <li>
<strong>plain_indices</strong> (<em>array_like</em>) – Plain dimension (column or row) co-ordinates of each element or block in values. (B+1)-dimensional tensor with the same length as values.</li> <li>
<strong>values</strong> (<em>array_list</em>) – Initial values for the tensor. Can be a list, tuple, NumPy <code>ndarray</code>, scalar, and other types. that represents a (1+K)-dimensional (for CSR and CSC layouts) or (1+2+K)-dimensional tensor (for BSR and BSC layouts) where <code>K</code> is the number of dense dimensions.</li> <li>
<strong>size</strong> (list, tuple, <code>torch.Size</code>, optional) – Size of the sparse tensor: <code>(*batchsize, nrows * blocksize[0], ncols *
blocksize[1], *densesize)</code> where <code>blocksize[0] ==
blocksize[1] == 1</code> for CSR and CSC formats. If not provided, the size will be inferred as the minimum size big enough to hold all non-zero elements or blocks.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<ul class="simple"> <li>
<strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>, optional) – the desired data type of returned tensor. Default: if None, infers data type from <code>values</code>.</li> <li>
<strong>layout</strong> (<a class="reference internal" href="../tensor_attributes#torch.layout" title="torch.layout"><code>torch.layout</code></a>, required) – the desired layout of returned tensor: <code>torch.sparse_csr</code>, <code>torch.sparse_csc</code>, <code>torch.sparse_bsr</code>, or <code>torch.sparse_bsc</code>.</li> <li>
<strong>device</strong> (<a class="reference internal" href="../tensor_attributes#torch.device" title="torch.device"><code>torch.device</code></a>, optional) – the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (see <a class="reference internal" href="torch.set_default_tensor_type#torch.set_default_tensor_type" title="torch.set_default_tensor_type"><code>torch.set_default_tensor_type()</code></a>). <a class="reference internal" href="../tensor_attributes#torch.device" title="torch.device"><code>device</code></a> will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.</li> <li>
<strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If autograd should record operations on the returned tensor. Default: <code>False</code>.</li> <li>
<strong>check_invariants</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If sparse tensor invariants are checked. Default: as returned by <a class="reference internal" href="torch.sparse.check_sparse_tensor_invariants#torch.sparse.check_sparse_tensor_invariants.is_enabled" title="torch.sparse.check_sparse_tensor_invariants.is_enabled"><code>torch.sparse.check_sparse_tensor_invariants.is_enabled()</code></a>, initially False.</li> </ul> </dd> </dl> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; compressed_indices = [0, 2, 4]
&gt;&gt;&gt; plain_indices = [0, 1, 0, 1]
&gt;&gt;&gt; values = [1, 2, 3, 4]
&gt;&gt;&gt; torch.sparse_compressed_tensor(torch.tensor(compressed_indices, dtype=torch.int64),
...                                torch.tensor(plain_indices, dtype=torch.int64),
...                                torch.tensor(values), dtype=torch.double, layout=torch.sparse_csr)
tensor(crow_indices=tensor([0, 2, 4]),
       col_indices=tensor([0, 1, 0, 1]),
       values=tensor([1., 2., 3., 4.]), size=(2, 2), nnz=4,
       dtype=torch.float64, layout=torch.sparse_csr)
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.sparse_compressed_tensor.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.sparse_compressed_tensor.html</a>
  </p>
</div>
