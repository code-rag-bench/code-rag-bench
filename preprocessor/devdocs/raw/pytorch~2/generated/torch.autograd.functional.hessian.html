<h1 id="torch-autograd-functional-hessian">torch.autograd.functional.hessian</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.functional.hessian">
<code>torch.autograd.functional.hessian(func, inputs, create_graph=False, strict=False, vectorize=False, outer_jacobian_strategy='reverse-mode')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/functional.html#hessian"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that computes the Hessian of a given scalar function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor with a single element.</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>
<strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the Hessian will be computed in a differentiable manner. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the hessian for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> <li>
<strong>vectorize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – This feature is experimental. Please consider using <a class="reference internal" href="torch.func.hessian#torch.func.hessian" title="torch.func.hessian"><code>torch.func.hessian()</code></a> instead if you are looking for something less experimental and more performant. When computing the hessian, usually we invoke <code>autograd.grad</code> once per row of the hessian. If this flag is <code>True</code>, we use the vmap prototype feature as the backend to vectorize calls to <code>autograd.grad</code> so we only invoke it once instead of once per row. This should lead to performance improvements in many use cases, however, due to this feature being incomplete, there may be performance cliffs. Please use <code>torch._C._debug_only_display_vmap_fallback_warnings(True)</code> to show any performance warnings and file us issues if warnings exist for your use case. Defaults to <code>False</code>.</li> <li>
<strong>outer_jacobian_strategy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – The Hessian is computed by computing the Jacobian of a Jacobian. The inner Jacobian is always computed in reverse-mode AD. Setting strategy to <code>"forward-mode"</code> or <code>"reverse-mode"</code> determines whether the outer Jacobian will be computed with forward or reverse mode AD. Currently, computing the outer Jacobian in <code>"forward-mode"</code> requires <code>vectorized=True</code>. Defaults to <code>"reverse-mode"</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>if there is a single input, this will be a single Tensor containing the Hessian for the input. If it is a tuple, then the Hessian will be a tuple of tuples where <code>Hessian[i][j]</code> will contain the Hessian of the <code>i</code>th input and <code>j</code>th input with size the sum of the size of the <code>i</code>th input plus the size of the <code>j</code>th input. <code>Hessian[i][j]</code> will have the same dtype and device as the corresponding <code>i</code>th input.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>Hessian (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a> or a tuple of <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a> of Tensors)</p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; def pow_reducer(x):
...     return x.pow(3).sum()
&gt;&gt;&gt; inputs = torch.rand(2, 2)
&gt;&gt;&gt; hessian(pow_reducer, inputs)
tensor([[[[5.2265, 0.0000],
          [0.0000, 0.0000]],
         [[0.0000, 4.8221],
          [0.0000, 0.0000]]],
        [[[0.0000, 0.0000],
          [1.9456, 0.0000]],
         [[0.0000, 0.0000],
          [0.0000, 3.2550]]]])
</pre> <pre data-language="python">&gt;&gt;&gt; hessian(pow_reducer, inputs, create_graph=True)
tensor([[[[5.2265, 0.0000],
          [0.0000, 0.0000]],
         [[0.0000, 4.8221],
          [0.0000, 0.0000]]],
        [[[0.0000, 0.0000],
          [1.9456, 0.0000]],
         [[0.0000, 0.0000],
          [0.0000, 3.2550]]]], grad_fn=&lt;ViewBackward&gt;)
</pre> <pre data-language="python">&gt;&gt;&gt; def pow_adder_reducer(x, y):
...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()
&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))
&gt;&gt;&gt; hessian(pow_adder_reducer, inputs)
((tensor([[4., 0.],
          [0., 4.]]),
  tensor([[0., 0.],
          [0., 0.]])),
 (tensor([[0., 0.],
          [0., 0.]]),
  tensor([[6., 0.],
          [0., 6.]])))
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.functional.hessian.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.functional.hessian.html</a>
  </p>
</div>
