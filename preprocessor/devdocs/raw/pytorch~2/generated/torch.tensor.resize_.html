<h1 id="torch-tensor-resize">torch.Tensor.resize_</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.resize_">
<code>Tensor.resize_(*sizes, memory_format=torch.contiguous_format) → Tensor</code> </dt> <dd>
<p>Resizes <code>self</code> tensor to the specified size. If the number of elements is larger than the current storage size, then the underlying storage is resized to fit the new number of elements. If the number of elements is smaller, the underlying storage is not changed. Existing elements are preserved but any new memory is uninitialized.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This is a low-level method. The storage is reinterpreted as C-contiguous, ignoring the current strides (unless the target size equals the current size, in which case the tensor is left unchanged). For most purposes, you will instead want to use <a class="reference internal" href="torch.tensor.view#torch.Tensor.view" title="torch.Tensor.view"><code>view()</code></a>, which checks for contiguity, or <a class="reference internal" href="torch.tensor.reshape#torch.Tensor.reshape" title="torch.Tensor.reshape"><code>reshape()</code></a>, which copies data if needed. To change the size in-place with custom strides, see <a class="reference internal" href="torch.tensor.set_#torch.Tensor.set_" title="torch.Tensor.set_"><code>set_()</code></a>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If <a class="reference internal" href="torch.use_deterministic_algorithms#torch.use_deterministic_algorithms" title="torch.use_deterministic_algorithms"><code>torch.use_deterministic_algorithms()</code></a> is set to <code>True</code>, new elements are initialized to prevent nondeterministic behavior from using the result as an input to an operation. Floating point and complex values are set to NaN, and integer values are set to the maximum value.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>sizes</strong> (<em>torch.Size</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>...</em>) – the desired size</li> <li>
<strong>memory_format</strong> (<a class="reference internal" href="../tensor_attributes#torch.memory_format" title="torch.memory_format"><code>torch.memory_format</code></a>, optional) – the desired memory format of Tensor. Default: <code>torch.contiguous_format</code>. Note that memory format of <code>self</code> is going to be unaffected if <code>self.size()</code> matches <code>sizes</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.tensor([[1, 2], [3, 4], [5, 6]])
&gt;&gt;&gt; x.resize_(2, 2)
tensor([[ 1,  2],
        [ 3,  4]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.resize_.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.resize_.html</a>
  </p>
</div>
