<h1 id="linear">linear</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.ao.nn.quantized.functional.linear">
<code>class torch.ao.nn.quantized.functional.linear(input, weight, bias=None, scale=None, zero_point=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/ao/nn/quantized/functional.html#linear"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Applies a linear transformation to the incoming quantized data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span></span></span>. See <a class="reference internal" href="torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear" title="torch.ao.nn.quantized.Linear"><code>Linear</code></a></p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Current implementation packs weights on every call, which has penalty on performance. If you want to avoid the overhead, use <a class="reference internal" href="torch.ao.nn.quantized.linear#torch.ao.nn.quantized.Linear" title="torch.ao.nn.quantized.Linear"><code>Linear</code></a>.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Quantized input of type <code>torch.quint8</code>
</li> <li>
<strong>weight</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Quantized weight of type <code>torch.qint8</code>
</li> <li>
<strong>bias</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – None or fp32 bias of type <code>torch.float</code>
</li> <li>
<strong>scale</strong> (<em>double</em>) – output scale. If None, derived from the input scale</li> <li>
<strong>zero_point</strong> (<em>python:long</em>) – output zero point. If None, derived from the input zero_point</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>Input: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *, in\_features)</annotation></semantics></math></span></span></span> where <code>*</code> means any number of additional dimensions</li> <li>Weight: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo separator="true">,</mo><mi>i</mi><mi>n</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(out\_features, in\_features)</annotation></semantics></math></span></span></span>
</li> <li>Bias: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(out\_features)</annotation></semantics></math></span></span></span>
</li> <li>Output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mo>∗</mo><mo separator="true">,</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi mathvariant="normal">_</mi><mi>f</mi><mi>e</mi><mi>a</mi><mi>t</mi><mi>u</mi><mi>r</mi><mi>e</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, *, out\_features)</annotation></semantics></math></span></span></span>
</li> </ul> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantized.functional.linear.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.ao.nn.quantized.functional.linear.html</a>
  </p>
</div>
