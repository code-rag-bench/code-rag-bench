<h1 id="cycliclr">CyclicLR</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.optim.lr_scheduler.CyclicLR">
<code>class torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr, step_size_up=2000, step_size_down=None, mode='triangular', gamma=1.0, scale_fn=None, scale_mode='cycle', cycle_momentum=True, base_momentum=0.8, max_momentum=0.9, last_epoch=-1, verbose=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/lr_scheduler.html#CyclicLR"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sets the learning rate of each parameter group according to cyclical learning rate policy (CLR). The policy cycles the learning rate between two boundaries with a constant frequency, as detailed in the paper <a class="reference external" href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rates for Training Neural Networks</a>. The distance between the two boundaries can be scaled on a per-iteration or per-cycle basis.</p> <p>Cyclical learning rate policy changes the learning rate after every batch. <code>step</code> should be called after a batch has been used for training.</p> <p>This class has three built-in policies, as put forth in the paper:</p> <ul class="simple"> <li>“triangular”: A basic triangular cycle without amplitude scaling.</li> <li>“triangular2”: A basic triangular cycle that scales initial amplitude by half each cycle.</li> <li>“exp_range”: A cycle that scales initial amplitude by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mtext>gamma</mtext><mtext>cycle iterations</mtext></msup></mrow><annotation encoding="application/x-tex">\text{gamma}^{\text{cycle iterations}}</annotation></semantics></math></span></span></span> at each cycle iteration.</li> </ul> <p>This implementation was adapted from the github repo: <a class="reference external" href="https://github.com/bckenstler/CLR">bckenstler/CLR</a></p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>optimizer</strong> (<a class="reference internal" href="../optim#torch.optim.Optimizer" title="torch.optim.Optimizer">Optimizer</a>) – Wrapped optimizer.</li> <li>
<strong>base_lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a>) – Initial learning rate which is the lower boundary in the cycle for each parameter group.</li> <li>
<strong>max_lr</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a>) – Upper learning rate boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_lr - base_lr). The lr at any cycle is the sum of base_lr and some scaling of the amplitude; therefore max_lr may not actually be reached depending on scaling function.</li> <li>
<strong>step_size_up</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of training iterations in the increasing half of a cycle. Default: 2000</li> <li>
<strong>step_size_down</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Number of training iterations in the decreasing half of a cycle. If step_size_down is None, it is set to step_size_up. Default: None</li> <li>
<strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – One of {triangular, triangular2, exp_range}. Values correspond to policies detailed above. If scale_fn is not None, this argument is ignored. Default: ‘triangular’</li> <li>
<strong>gamma</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – Constant in ‘exp_range’ scaling function: gamma**(cycle iterations) Default: 1.0</li> <li>
<strong>scale_fn</strong> (<em>function</em>) – Custom scaling policy defined by a single argument lambda function, where 0 &lt;= scale_fn(x) &lt;= 1 for all x &gt;= 0. If specified, then ‘mode’ is ignored. Default: None</li> <li>
<strong>scale_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – {‘cycle’, ‘iterations’}. Defines whether scale_fn is evaluated on cycle number or cycle iterations (training iterations since start of cycle). Default: ‘cycle’</li> <li>
<strong>cycle_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, momentum is cycled inversely to learning rate between ‘base_momentum’ and ‘max_momentum’. Default: True</li> <li>
<strong>base_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a>) – Lower momentum boundaries in the cycle for each parameter group. Note that momentum is cycled inversely to learning rate; at the peak of a cycle, momentum is ‘base_momentum’ and learning rate is ‘max_lr’. Default: 0.8</li> <li>
<strong>max_momentum</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.12)">list</a>) – Upper momentum boundaries in the cycle for each parameter group. Functionally, it defines the cycle amplitude (max_momentum - base_momentum). The momentum at any cycle is the difference of max_momentum and some scaling of the amplitude; therefore base_momentum may not actually be reached depending on scaling function. Note that momentum is cycled inversely to learning rate; at the start of a cycle, momentum is ‘max_momentum’ and learning rate is ‘base_lr’ Default: 0.9</li> <li>
<strong>last_epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The index of the last batch. This parameter is used when resuming a training job. Since <code>step()</code> should be invoked after each batch instead of after each epoch, this number represents the total number of <em>batches</em> computed, not the total number of epochs computed. When last_epoch=-1, the schedule is started from the beginning. Default: -1</li> <li>
<strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</li> </ul> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
&gt;&gt;&gt; scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.01, max_lr=0.1)
&gt;&gt;&gt; data_loader = torch.utils.data.DataLoader(...)
&gt;&gt;&gt; for epoch in range(10):
&gt;&gt;&gt;     for batch in data_loader:
&gt;&gt;&gt;         train_batch(...)
&gt;&gt;&gt;         scheduler.step()
</pre> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.lr_scheduler.CyclicLR.get_last_lr">
<code>get_last_lr()</code> </dt> <dd>
<p>Return last computed learning rate by current scheduler.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.lr_scheduler.CyclicLR.get_lr">
<code>get_lr()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/optim/lr_scheduler.html#CyclicLR.get_lr"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Calculates the learning rate at batch index. This function treats <code>self.last_epoch</code> as the last batch index.</p> <p>If <code>self.cycle_momentum</code> is <code>True</code>, this function has a side effect of updating the optimizer’s momentum.</p> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.optim.lr_scheduler.CyclicLR.print_lr">
<code>print_lr(is_verbose, group, lr, epoch=None)</code> </dt> <dd>
<p>Display the current learning rate.</p> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.CyclicLR.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.optim.lr_scheduler.CyclicLR.html</a>
  </p>
</div>
