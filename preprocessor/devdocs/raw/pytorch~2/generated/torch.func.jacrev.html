<h1 id="torch-func-jacrev">torch.func.jacrev</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.jacrev">
<code>torch.func.jacrev(func, argnums=0, *, has_aux=False, chunk_size=None, _preallocate_and_copy=False)</code> </dt> <dd>
<p>Computes the Jacobian of <code>func</code> with respect to the arg(s) at index <code>argnum</code> using reverse mode autodiff</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Using <code>chunk_size=1</code> is equivalent to computing the jacobian row-by-row with a for-loop i.e. the constraints of <a class="reference internal" href="torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> are not applicable.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors</li> <li>
<strong>argnums</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>]</em>) – Optional, integer or tuple of integers, saying which arguments to get the Jacobian with respect to. Default: 0.</li> <li>
<strong>has_aux</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Flag indicating that <code>func</code> returns a <code>(output, aux)</code> tuple where the first element is the output of the function to be differentiated and the second element is auxiliary objects that will not be differentiated. Default: False.</li> <li>
<strong>chunk_size</strong> (<em>None</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – If None (default), use the maximum chunk size (equivalent to doing a single vmap over vjp to compute the jacobian). If 1, then compute the jacobian row-by-row with a for-loop. If not None, then compute the jacobian <code>chunk_size</code> rows at a time (equivalent to doing multiple vmap over vjp). If you run into memory issues computing the jacobian, please try to specify a non-None chunk_size.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Returns a function that takes in the same inputs as <code>func</code> and returns the Jacobian of <code>func</code> with respect to the arg(s) at <code>argnums</code>. If <code>has_aux is True</code>, then the returned function instead returns a <code>(jacobian, aux)</code> tuple where <code>jacobian</code> is the Jacobian and <code>aux</code> is auxiliary objects returned by <code>func</code>.</p> </dd> </dl> <p>A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; jacobian = jacrev(torch.sin)(x)
&gt;&gt;&gt; expected = torch.diag(torch.cos(x))
&gt;&gt;&gt; assert torch.allclose(jacobian, expected)
</pre> <p>If you would like to compute the output of the function as well as the jacobian of the function, use the <code>has_aux</code> flag to return the output as an auxiliary object:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt;
&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;   return x.sin()
&gt;&gt;&gt;
&gt;&gt;&gt; def g(x):
&gt;&gt;&gt;   result = f(x)
&gt;&gt;&gt;   return result, result
&gt;&gt;&gt;
&gt;&gt;&gt; jacobian_f, f_x = jacrev(g, has_aux=True)(x)
&gt;&gt;&gt; assert torch.allclose(f_x, f(x))
</pre> <p><a class="reference internal" href="#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a> can be composed with vmap to produce batched Jacobians:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev, vmap
&gt;&gt;&gt; x = torch.randn(64, 5)
&gt;&gt;&gt; jacobian = vmap(jacrev(torch.sin))(x)
&gt;&gt;&gt; assert jacobian.shape == (64, 5, 5)
</pre> <p>Additionally, <a class="reference internal" href="#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a> can be composed with itself to produce Hessians</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev
&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;   return x.sin().sum()
&gt;&gt;&gt;
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; hessian = jacrev(jacrev(f))(x)
&gt;&gt;&gt; assert torch.allclose(hessian, torch.diag(-x.sin()))
</pre> <p>By default, <a class="reference internal" href="#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a> computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using <code>argnums</code>:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev
&gt;&gt;&gt; def f(x, y):
&gt;&gt;&gt;   return x + y ** 2
&gt;&gt;&gt;
&gt;&gt;&gt; x, y = torch.randn(5), torch.randn(5)
&gt;&gt;&gt; jacobian = jacrev(f, argnums=1)(x, y)
&gt;&gt;&gt; expected = torch.diag(2 * y)
&gt;&gt;&gt; assert torch.allclose(jacobian, expected)
</pre> <p>Additionally, passing a tuple to <code>argnums</code> will compute the Jacobian with respect to multiple arguments</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacrev
&gt;&gt;&gt; def f(x, y):
&gt;&gt;&gt;   return x + y ** 2
&gt;&gt;&gt;
&gt;&gt;&gt; x, y = torch.randn(5), torch.randn(5)
&gt;&gt;&gt; jacobian = jacrev(f, argnums=(0, 1))(x, y)
&gt;&gt;&gt; expectedX = torch.diag(torch.ones_like(x))
&gt;&gt;&gt; expectedY = torch.diag(2 * y)
&gt;&gt;&gt; assert torch.allclose(jacobian[0], expectedX)
&gt;&gt;&gt; assert torch.allclose(jacobian[1], expectedY)
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Using PyTorch <code>torch.no_grad</code> together with <code>jacrev</code>. Case 1: Using <code>torch.no_grad</code> inside a function:</p> <pre data-language="python">&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;     with torch.no_grad():
&gt;&gt;&gt;         c = x ** 2
&gt;&gt;&gt;     return x - c
</pre> <p>In this case, <code>jacrev(f)(x)</code> will respect the inner <code>torch.no_grad</code>.</p> <p>Case 2: Using <code>jacrev</code> inside <code>torch.no_grad</code> context manager:</p> <pre data-language="python">&gt;&gt;&gt; with torch.no_grad():
&gt;&gt;&gt;     jacrev(f)(x)
</pre> <p>In this case, <code>jacrev</code> will respect the inner <code>torch.no_grad</code>, but not the outer one. This is because <code>jacrev</code> is a “function transform”: its result should not depend on the result of a context manager outside of <code>f</code>.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.jacrev.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.jacrev.html</a>
  </p>
</div>
