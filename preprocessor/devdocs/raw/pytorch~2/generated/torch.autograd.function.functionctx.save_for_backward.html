<h1 id="torch-autograd-function-functionctx-save-for-backward">torch.autograd.function.FunctionCtx.save_for_backward</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.autograd.function.FunctionCtx.save_for_backward">
<code>FunctionCtx.save_for_backward(*tensors)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/function.html#FunctionCtx.save_for_backward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Saves given tensors for a future call to <code>backward()</code>.</p> <p><code>save_for_backward</code> should be called at most once, only from inside the <code>forward()</code> method, and only with tensors.</p> <p>All tensors intended to be used in the backward pass should be saved with <code>save_for_backward</code> (as opposed to directly on <code>ctx</code>) to prevent incorrect gradients and memory leaks, and enable the application of saved tensor hooks. See <a class="reference internal" href="../autograd#torch.autograd.graph.saved_tensors_hooks" title="torch.autograd.graph.saved_tensors_hooks"><code>torch.autograd.graph.saved_tensors_hooks</code></a>.</p> <p>Note that if intermediary tensors, tensors that are neither inputs nor outputs of <code>forward()</code>, are saved for backward, your custom Function may not support double backward. Custom Functions that do not support double backward should decorate their <code>backward()</code> method with <code>@once_differentiable</code> so that performing double backward raises an error. If you’d like to support double backward, you can either recompute intermediaries based on the inputs during backward or return the intermediaries as the outputs of the custom Function. See the <a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a> for more details.</p> <p>In <code>backward()</code>, saved tensors can be accessed through the <code>saved_tensors</code> attribute. Before returning them to the user, a check is made to ensure they weren’t used in any in-place operation that modified their content.</p> <p>Arguments can also be <code>None</code>. This is a no-op.</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/extending.html#extending-autograd"><span class="std std-ref">Extending torch.autograd</span></a> for more details on how to use this method.</p> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; class Func(Function):
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, x: torch.Tensor, y: torch.Tensor, z: int):
&gt;&gt;&gt;         w = x * z
&gt;&gt;&gt;         out = x * y + y * z + w * y
&gt;&gt;&gt;         ctx.save_for_backward(x, y, w, out)
&gt;&gt;&gt;         ctx.z = z  # z is not a tensor
&gt;&gt;&gt;         return out
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     @once_differentiable
&gt;&gt;&gt;     def backward(ctx, grad_out):
&gt;&gt;&gt;         x, y, w, out = ctx.saved_tensors
&gt;&gt;&gt;         z = ctx.z
&gt;&gt;&gt;         gx = grad_out * (y + y * z)
&gt;&gt;&gt;         gy = grad_out * (x + z + w)
&gt;&gt;&gt;         gz = None
&gt;&gt;&gt;         return gx, gy, gz
&gt;&gt;&gt;
&gt;&gt;&gt; a = torch.tensor(1., requires_grad=True, dtype=torch.double)
&gt;&gt;&gt; b = torch.tensor(2., requires_grad=True, dtype=torch.double)
&gt;&gt;&gt; c = 4
&gt;&gt;&gt; d = Func.apply(a, b, c)
</pre> </dd> </dl>  </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.function.FunctionCtx.save_for_backward.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.function.FunctionCtx.save_for_backward.html</a>
  </p>
</div>
