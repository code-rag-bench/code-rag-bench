<h1 id="torch-func-functional-call">torch.func.functional_call</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.functional_call">
<code>torch.func.functional_call(module, parameter_and_buffer_dicts, args, kwargs=None, *, tie_weights=True, strict=False)</code> </dt> <dd>
<p>Performs a functional call on the module by replacing the module parameters and buffers with the provided ones.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the module has active parametrizations, passing a value in the <code>parameter_and_buffer_dicts</code> argument with the name set to the regular parameter name will completely disable the parametrization. If you want to apply the parametrization function to the value passed please set the key as <code>{submodule_name}.parametrizations.{parameter_name}.original</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the module performs in-place operations on parameters/buffers, these will be reflected in the <code>parameter_and_buffer_dicts</code> input.</p>  <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = {'foo': torch.zeros(())}
&gt;&gt;&gt; mod = Foo()  # does self.foo = self.foo + 1
&gt;&gt;&gt; print(mod.foo)  # tensor(0.)
&gt;&gt;&gt; functional_call(mod, a, torch.ones(()))
&gt;&gt;&gt; print(mod.foo)  # tensor(0.)
&gt;&gt;&gt; print(a['foo'])  # tensor(1.)
</pre>  </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the module has tied weights, whether or not functional_call respects the tying is determined by the tie_weights flag.</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = {'foo': torch.zeros(())}
&gt;&gt;&gt; mod = Foo()  # has both self.foo and self.foo_tied which are tied. Returns x + self.foo + self.foo_tied
&gt;&gt;&gt; print(mod.foo)  # tensor(1.)
&gt;&gt;&gt; mod(torch.zeros(()))  # tensor(2.)
&gt;&gt;&gt; functional_call(mod, a, torch.zeros(()))  # tensor(0.) since it will change self.foo_tied too
&gt;&gt;&gt; functional_call(mod, a, torch.zeros(()), tie_weights=False)  # tensor(1.)--self.foo_tied is not updated
&gt;&gt;&gt; new_a = {'foo', torch.zeros(()), 'foo_tied': torch.zeros(())}
&gt;&gt;&gt; functional_call(mod, new_a, torch.zeros()) # tensor(0.)
</pre> </div> <p>An example of passing mutliple dictionaries</p> <pre data-language="python">a = ({'weight': torch.ones(1, 1)}, {'buffer': torch.zeros(1)})  # two separate dictionaries
mod = nn.Bar(1, 1)  # return self.weight @ x + self.buffer
print(mod.weight)  # tensor(...)
print(mod.buffer)  # tensor(...)
x = torch.randn((1, 1))
print(x)
functional_call(mod, a, x)  # same as x
print(mod.weight)  # same as before functional_call
</pre> <p>And here is an example of applying the grad transform over the parameters of a model.</p> <pre data-language="python">import torch
import torch.nn as nn
from torch.func import functional_call, grad

x = torch.randn(4, 3)
t = torch.randn(4, 3)
model = nn.Linear(3, 3)

def compute_loss(params, x, t):
    y = functional_call(model, params, x)
    return nn.functional.mse_loss(y, t)

grad_weights = grad(compute_loss)(dict(model.named_parameters()), x, t)
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If the user does not need grad tracking outside of grad transforms, they can detach all of the parameters for better performance and memory usage</p> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; detached_params = {k: v.detach() for k, v in model.named_parameters()}
&gt;&gt;&gt; grad_weights = grad(compute_loss)(detached_params, x, t)
&gt;&gt;&gt; grad_weights.grad_fn  # None--it's not tracking gradients outside of grad
</pre> <p>This means that the user cannot call <code>grad_weight.backward()</code>. However, if they don’t need autograd tracking outside of the transforms, this will result in less memory usage and faster speeds.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>module</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">torch.nn.Module</a>) – the module to call</li> <li>
<strong>parameters_and_buffer_dicts</strong> (<em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>] or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Dict</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the parameters that will be used in the module call. If given a tuple of dictionaries, they must have distinct keys so that all dictionaries can be used together</li> <li>
<strong>args</strong> (<em>Any</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>) – arguments to be passed to the module call. If not a tuple, considered a single argument.</li> <li>
<strong>kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">dict</a>) – keyword arguments to be passed to the module call</li> <li>
<strong>tie_weights</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If True, then parameters and buffers tied in the original model will be treated as tied in the reparamaterized version. Therefore, if True and different values are passed for the tied paramaters and buffers, it will error. If False, it will not respect the originally tied parameters and buffers unless the values passed for both weights are the same. Default: True.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If True, then the parameters and buffers passed in must match the parameters and buffers in the original module. Therefore, if True and there are any missing or unexpected keys, it will error. Default: False.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>the result of calling <code>module</code>.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>Any</p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.functional_call.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.functional_call.html</a>
  </p>
</div>
