<h1 id="torch-nn-functional-ctc-loss">torch.nn.functional.ctc_loss</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.nn.functional.ctc_loss">
<code>torch.nn.functional.ctc_loss(log_probs, targets, input_lengths, target_lengths, blank=0, reduction='mean', zero_infinity=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/functional.html#ctc_loss"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>The Connectionist Temporal Classification loss.</p> <p>See <a class="reference internal" href="torch.nn.ctcloss#torch.nn.CTCLoss" title="torch.nn.CTCLoss"><code>CTCLoss</code></a> for details.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some circumstances when given tensors on a CUDA device and using CuDNN, this operator may select a nondeterministic algorithm to increase performance. If this is undesirable, you can try to make the operation deterministic (potentially at a performance cost) by setting <code>torch.backends.cudnn.deterministic = True</code>. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This operation may produce nondeterministic gradients when given tensors on a CUDA device. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html"><span class="doc">Reproducibility</span></a> for more information.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>log_probs</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, N, C)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>C</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, C)</annotation></semantics></math></span></span></span> where <code>C = number of characters in alphabet including blank</code>, <code>T = input length</code>, and <code>N = batch size</code>. The logarithmized probabilities of the outputs (e.g. obtained with <a class="reference internal" href="torch.nn.functional.log_softmax#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>torch.nn.functional.log_softmax()</code></a>).</li> <li>
<strong>targets</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span></span> or <code>(sum(target_lengths))</code>. Targets cannot be blank. In the second form, the targets are assumed to be concatenated.</li> <li>
<strong>input_lengths</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">()</annotation></semantics></math></span></span></span>. Lengths of the inputs (must each be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">\leq T</annotation></semantics></math></span></span></span>)</li> <li>
<strong>target_lengths</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">()</annotation></semantics></math></span></span></span>. Lengths of the targets</li> <li>
<strong>blank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Blank label. Default <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0</mn></mrow><annotation encoding="application/x-tex">0</annotation></semantics></math></span></span></span>.</li> <li>
<strong>reduction</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – Specifies the reduction to apply to the output: <code>'none'</code> | <code>'mean'</code> | <code>'sum'</code>. <code>'none'</code>: no reduction will be applied, <code>'mean'</code>: the output losses will be divided by the target lengths and then the mean over the batch is taken, <code>'sum'</code>: the output will be summed. Default: <code>'mean'</code>
</li> <li>
<strong>zero_infinity</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – Whether to zero infinite losses and the associated gradients. Default: <code>False</code> Infinite losses mainly occur when the inputs are too short to be aligned to the targets.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; log_probs = torch.randn(50, 16, 20).log_softmax(2).detach().requires_grad_()
&gt;&gt;&gt; targets = torch.randint(1, 20, (16, 30), dtype=torch.long)
&gt;&gt;&gt; input_lengths = torch.full((16,), 50, dtype=torch.long)
&gt;&gt;&gt; target_lengths = torch.randint(10, 30, (16,), dtype=torch.long)
&gt;&gt;&gt; loss = F.ctc_loss(log_probs, targets, input_lengths, target_lengths)
&gt;&gt;&gt; loss.backward()
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.functional.ctc_loss.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.functional.ctc_loss.html</a>
  </p>
</div>
