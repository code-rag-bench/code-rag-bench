<h1 id="torch-autograd-functional-jacobian">torch.autograd.functional.jacobian</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.functional.jacobian">
<code>torch.autograd.functional.jacobian(func, inputs, create_graph=False, strict=False, vectorize=False, strategy='reverse-mode')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/functional.html#jacobian"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that computes the Jacobian of a given function.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>
<strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the Jacobian will be computed in a differentiable manner. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the jacobian for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> <li>
<strong>vectorize</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – This feature is experimental. Please consider using <a class="reference internal" href="torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>torch.func.jacrev()</code></a> or <a class="reference internal" href="torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>torch.func.jacfwd()</code></a> instead if you are looking for something less experimental and more performant. When computing the jacobian, usually we invoke <code>autograd.grad</code> once per row of the jacobian. If this flag is <code>True</code>, we perform only a single <code>autograd.grad</code> call with <code>batched_grad=True</code> which uses the vmap prototype feature. Though this should lead to performance improvements in many cases, because this feature is still experimental, there may be performance cliffs. See <a class="reference internal" href="torch.autograd.grad#torch.autograd.grad" title="torch.autograd.grad"><code>torch.autograd.grad()</code></a>’s <code>batched_grad</code> parameter for more information.</li> <li>
<strong>strategy</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – Set to <code>"forward-mode"</code> or <code>"reverse-mode"</code> to determine whether the Jacobian will be computed with forward or reverse mode AD. Currently, <code>"forward-mode"</code> requires <code>vectorized=True</code>. Defaults to <code>"reverse-mode"</code>. If <code>func</code> has more outputs than inputs, <code>"forward-mode"</code> tends to be more performant. Otherwise, prefer to use <code>"reverse-mode"</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>if there is a single input and output, this will be a single Tensor containing the Jacobian for the linearized inputs and output. If one of the two is a tuple, then the Jacobian will be a tuple of Tensors. If both of them are tuples, then the Jacobian will be a tuple of tuple of Tensors where <code>Jacobian[i][j]</code> will contain the Jacobian of the <code>i</code>th output and <code>j</code>th input and will have as size the concatenation of the sizes of the corresponding output and the corresponding input and will have same dtype and device as the corresponding input. If strategy is <code>forward-mode</code>, the dtype will be that of the output; otherwise, the input.</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>Jacobian (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a> or nested tuple of Tensors)</p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; def exp_reducer(x):
...     return x.exp().sum(dim=1)
&gt;&gt;&gt; inputs = torch.rand(2, 2)
&gt;&gt;&gt; jacobian(exp_reducer, inputs)
tensor([[[1.4917, 2.4352],
         [0.0000, 0.0000]],
        [[0.0000, 0.0000],
         [2.4369, 2.3799]]])
</pre> <pre data-language="python">&gt;&gt;&gt; jacobian(exp_reducer, inputs, create_graph=True)
tensor([[[1.4917, 2.4352],
         [0.0000, 0.0000]],
        [[0.0000, 0.0000],
         [2.4369, 2.3799]]], grad_fn=&lt;ViewBackward&gt;)
</pre> <pre data-language="python">&gt;&gt;&gt; def exp_adder(x, y):
...     return 2 * x.exp() + 3 * y
&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))
&gt;&gt;&gt; jacobian(exp_adder, inputs)
(tensor([[2.8052, 0.0000],
        [0.0000, 3.3963]]),
 tensor([[3., 0.],
         [0., 3.]]))
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.functional.jacobian.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.functional.jacobian.html</a>
  </p>
</div>
