<h1 id="torch-svd">torch.svd</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.svd">
<code>torch.svd(input, some=True, compute_uv=True, *, out=None)</code> </dt> <dd>
<p>Computes the singular value decomposition of either a matrix or batch of matrices <code>input</code>. The singular value decomposition is represented as a namedtuple <code>(U, S, V)</code>, such that <code>input</code> <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mi>U</mi><mtext>diag</mtext><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">= U \text{diag}(S) V^{\text{H}}</annotation></semantics></math></span></span></span>. where <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">V^{\text{H}}</annotation></semantics></math></span></span></span> is the transpose of <code>V</code> for real inputs, and the conjugate transpose of <code>V</code> for complex inputs. If <code>input</code> is a batch of matrices, then <code>U</code>, <code>S</code>, and <code>V</code> are also batched with the same batch dimensions as <code>input</code>.</p> <p>If <code>some</code> is <code>True</code> (default), the method returns the reduced singular value decomposition. In this case, if the last two dimensions of <code>input</code> are <code>m</code> and <code>n</code>, then the returned <code>U</code> and <code>V</code> matrices will contain only <code>min(n, m)</code> orthonormal columns.</p> <p>If <code>compute_uv</code> is <code>False</code>, the returned <code>U</code> and <code>V</code> will be zero-filled matrices of shape <code>(m, m)</code> and <code>(n, n)</code> respectively, and the same device as <code>input</code>. The argument <code>some</code> has no effect when <code>compute_uv</code> is <code>False</code>.</p> <p>Supports <code>input</code> of float, double, cfloat and cdouble data types. The dtypes of <code>U</code> and <code>V</code> are the same as <code>input</code>’s. <code>S</code> will always be real-valued, even if <code>input</code> is complex.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> is deprecated in favor of <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> and will be removed in a future PyTorch release.</p> <p><code>U, S, V = torch.svd(A, some=some, compute_uv=True)</code> (default) should be replaced with</p> <pre data-language="python">U, S, Vh = torch.linalg.svd(A, full_matrices=not some)
V = Vh.mH
</pre> <p><code>_, S, _ = torch.svd(A, some=some, compute_uv=False)</code> should be replaced with</p> <pre data-language="python">S = torch.linalg.svdvals(A)
</pre> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Differences with <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>:</p> <ul class="simple"> <li>
<code>some</code> is the opposite of <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a>’s <code>full_matrices</code>. Note that default value for both is <code>True</code>, so the default behavior is effectively the opposite.</li> <li>
<a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> returns <code>V</code>, whereas <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> returns <code>Vh</code>, that is, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>V</mi><mtext>H</mtext></msup></mrow><annotation encoding="application/x-tex">V^{\text{H}}</annotation></semantics></math></span></span></span>.</li> <li>If <code>compute_uv</code> is <code>False</code>, <a class="reference internal" href="#torch.svd" title="torch.svd"><code>torch.svd()</code></a> returns zero-filled tensors for <code>U</code> and <code>Vh</code>, whereas <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> returns empty tensors.</li> </ul> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The singular values are returned in descending order. If <code>input</code> is a batch of matrices, then the singular values of each matrix in the batch are returned in descending order.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>S</code> tensor can only be used to compute gradients if <code>compute_uv</code> is <code>True</code>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>some</code> is <code>False</code>, the gradients on <code>U[…, :, min(m, n):]</code> and <code>V[…, :, min(m, n):]</code> will be ignored in the backward pass, as those vectors can be arbitrary bases of the corresponding subspaces.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The implementation of <a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> on CPU uses LAPACK’s routine <code>?gesdd</code> (a divide-and-conquer algorithm) instead of <code>?gesvd</code> for speed. Analogously, on GPU, it uses cuSOLVER’s routines <code>gesvdj</code> and <code>gesvdjBatched</code> on CUDA 10.1.243 and later, and MAGMA’s routine <code>gesdd</code> on earlier versions of CUDA.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The returned <code>U</code> will not be contiguous. The matrix (or batch of matrices) will be represented as a column-major matrix (i.e. Fortran-contiguous).</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The gradients with respect to <code>U</code> and <code>V</code> will only be finite when the input does not have zero nor repeated singular values.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If the distance between any two singular values is close to zero, the gradients with respect to <code>U</code> and <code>V</code> will be numerically unstable, as they depends on <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow></msub><msubsup><mi>σ</mi><mi>i</mi><mn>2</mn></msubsup><mo>−</mo><msubsup><mi>σ</mi><mi>j</mi><mn>2</mn></msubsup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\min_{i \neq j} \sigma_i^2 - \sigma_j^2}</annotation></semantics></math></span></span></span>. The same happens when the matrix has small singular values, as these gradients also depend on <code>S⁻¹</code>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>For complex-valued <code>input</code> the singular value decomposition is not unique, as <code>U</code> and <code>V</code> may be multiplied by an arbitrary phase factor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>ϕ</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{i \phi}</annotation></semantics></math></span></span></span> on every column. The same happens when <code>input</code> has repeated singular values, where one may multiply the columns of the spanning subspace in <code>U</code> and <code>V</code> by a rotation matrix and <a class="reference external" href="#">the resulting vectors will span the same subspace</a>. Different platforms, like NumPy, or inputs on different device types, may produce different <code>U</code> and <code>V</code> tensors.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>input</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <code>(*, m, n)</code> where <code>*</code> is zero or more batch dimensions consisting of <code>(m, n)</code> matrices.</li> <li>
<strong>some</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – controls whether to compute the reduced or full decomposition, and consequently, the shape of returned <code>U</code> and <code>V</code>. Default: <code>True</code>.</li> <li>
<strong>compute_uv</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – controls whether to compute <code>U</code> and <code>V</code>. Default: <code>True</code>.</li> </ul> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – the output tuple of tensors</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; a = torch.randn(5, 3)
&gt;&gt;&gt; a
tensor([[ 0.2364, -0.7752,  0.6372],
        [ 1.7201,  0.7394, -0.0504],
        [-0.3371, -1.0584,  0.5296],
        [ 0.3550, -0.4022,  1.5569],
        [ 0.2445, -0.0158,  1.1414]])
&gt;&gt;&gt; u, s, v = torch.svd(a)
&gt;&gt;&gt; u
tensor([[ 0.4027,  0.0287,  0.5434],
        [-0.1946,  0.8833,  0.3679],
        [ 0.4296, -0.2890,  0.5261],
        [ 0.6604,  0.2717, -0.2618],
        [ 0.4234,  0.2481, -0.4733]])
&gt;&gt;&gt; s
tensor([2.3289, 2.0315, 0.7806])
&gt;&gt;&gt; v
tensor([[-0.0199,  0.8766,  0.4809],
        [-0.5080,  0.4054, -0.7600],
        [ 0.8611,  0.2594, -0.4373]])
&gt;&gt;&gt; torch.dist(a, torch.mm(torch.mm(u, torch.diag(s)), v.t()))
tensor(8.6531e-07)
&gt;&gt;&gt; a_big = torch.randn(7, 5, 3)
&gt;&gt;&gt; u, s, v = torch.svd(a_big)
&gt;&gt;&gt; torch.dist(a_big, torch.matmul(torch.matmul(u, torch.diag_embed(s)), v.mT))
tensor(2.6503e-06)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.svd.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.svd.html</a>
  </p>
</div>
