<h1 id="torch-linalg-eig">torch.linalg.eig</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.linalg.eig">
<code>torch.linalg.eig(A, *, out=None)</code> </dt> <dd>
<p>Computes the eigenvalue decomposition of a square matrix if it exists.</p> <p>Letting <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">K</mi></mrow><annotation encoding="application/x-tex">\mathbb{K}</annotation></semantics></math></span></span></span> be <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">\mathbb{R}</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="double-struck">C</mi></mrow><annotation encoding="application/x-tex">\mathbb{C}</annotation></semantics></math></span></span></span>, the <strong>eigenvalue decomposition</strong> of a square matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>∈</mo><msup><mi mathvariant="double-struck">K</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup></mrow><annotation encoding="application/x-tex">A \in \mathbb{K}^{n \times n}</annotation></semantics></math></span></span></span> (if it exists) is defined as</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>A</mi><mo>=</mo><mi>V</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">Λ</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mpadded width="0px"><mrow><mspace width="2em"></mspace><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">C</mi><mrow><mi>n</mi><mo>×</mo><mi>n</mi></mrow></msup><mo separator="true">,</mo><mi mathvariant="normal">Λ</mi><mo>∈</mo><msup><mi mathvariant="double-struck">C</mi><mi>n</mi></msup></mrow></mpadded></mrow><annotation encoding="application/x-tex">A = V \operatorname{diag}(\Lambda) V^{-1}\mathrlap{\qquad V \in \mathbb{C}^{n \times n}, \Lambda \in \mathbb{C}^n}</annotation></semantics></math></span></span></span>
</div>
<p>This decomposition exists if and only if <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> is <a class="reference external" href="https://en.wikipedia.org/wiki/Diagonalizable_matrix#Definition">diagonalizable</a>. This is the case when all its eigenvalues are different.</p> <p>Supports input of float, double, cfloat and cdouble dtypes. Also supports batches of matrices, and if <code>A</code> is a batch of matrices then the output has the same batch dimensions.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The eigenvalues and eigenvectors of a real matrix may be complex.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When inputs are on a CUDA device, this function synchronizes that device with the CPU.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This function assumes that <code>A</code> is <a class="reference external" href="https://en.wikipedia.org/wiki/Diagonalizable_matrix#Definition">diagonalizable</a> (for example, when all the eigenvalues are different). If it is not diagonalizable, the returned eigenvalues will be correct but <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo mathvariant="normal">≠</mo><mi>V</mi><mi mathvariant="normal">diag</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi mathvariant="normal">Λ</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">A \neq V \operatorname{diag}(\Lambda)V^{-1}</annotation></semantics></math></span></span></span>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The returned eigenvectors are normalized to have norm <code>1</code>. Even then, the eigenvectors of a matrix are not unique, nor are they continuous with respect to <code>A</code>. Due to this lack of uniqueness, different hardware and software may compute different eigenvectors.</p> <p>This non-uniqueness is caused by the fact that multiplying an eigenvector by by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>e</mi><mrow><mi>i</mi><mi>ϕ</mi></mrow></msup><mo separator="true">,</mo><mi>ϕ</mi><mo>∈</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">e^{i \phi}, \phi \in \mathbb{R}</annotation></semantics></math></span></span></span> produces another set of valid eigenvectors of the matrix. For this reason, the loss function shall not depend on the phase of the eigenvectors, as this quantity is not well-defined. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, this function synchronizes that device with the CPU when computing the gradients. This is checked when computing the gradients of this function. As such, when inputs are on a CUDA device, the computation of the gradients of this function synchronizes that device with the CPU.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Gradients computed using the <code>eigenvectors</code> tensor will only be finite when <code>A</code> has distinct eigenvalues. Furthermore, if the distance between any two eigenvalues is close to zero, the gradient will be numerically unstable, as it depends on the eigenvalues <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>λ</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\lambda_i</annotation></semantics></math></span></span></span> through the computation of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mrow><msub><mrow><mi>min</mi><mo>⁡</mo></mrow><mrow><mi>i</mi><mo mathvariant="normal">≠</mo><mi>j</mi></mrow></msub><msub><mi>λ</mi><mi>i</mi></msub><mo>−</mo><msub><mi>λ</mi><mi>j</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\min_{i \neq j} \lambda_i - \lambda_j}</annotation></semantics></math></span></span></span>.</p> </div> <div class="admonition seealso"> <p class="admonition-title">See also</p> <p><a class="reference internal" href="torch.linalg.eigvals#torch.linalg.eigvals" title="torch.linalg.eigvals"><code>torch.linalg.eigvals()</code></a> computes only the eigenvalues. Unlike <a class="reference internal" href="#torch.linalg.eig" title="torch.linalg.eig"><code>torch.linalg.eig()</code></a>, the gradients of <a class="reference internal" href="torch.linalg.eigvals#torch.linalg.eigvals" title="torch.linalg.eigvals"><code>eigvals()</code></a> are always numerically stable.</p> <p><a class="reference internal" href="torch.linalg.eigh#torch.linalg.eigh" title="torch.linalg.eigh"><code>torch.linalg.eigh()</code></a> for a (faster) function that computes the eigenvalue decomposition for Hermitian and symmetric matrices.</p> <p><a class="reference internal" href="torch.linalg.svd#torch.linalg.svd" title="torch.linalg.svd"><code>torch.linalg.svd()</code></a> for a function that computes another type of spectral decomposition that works on matrices of any shape.</p> <p><a class="reference internal" href="torch.linalg.qr#torch.linalg.qr" title="torch.linalg.qr"><code>torch.linalg.qr()</code></a> for another (much faster) decomposition that works on matrices of any shape.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensor of shape <code>(*, n, n)</code> where <code>*</code> is zero or more batch dimensions consisting of diagonalizable matrices.</p> </dd> <dt class="field-even">Keyword Arguments</dt> <dd class="field-even">
<p><strong>out</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em>, </em><em>optional</em>) – output tuple of two tensors. Ignored if <code>None</code>. Default: <code>None</code>.</p> </dd> <dt class="field-odd">Returns</dt> <dd class="field-odd">

<p>A named tuple <code>(eigenvalues, eigenvectors)</code> which corresponds to <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Λ</mi></mrow><annotation encoding="application/x-tex">\Lambda</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span></span></span> above.</p> <p><code>eigenvalues</code> and <code>eigenvectors</code> will always be complex-valued, even when <code>A</code> is real. The eigenvectors will be given by the columns of <code>eigenvectors</code>.</p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; A = torch.randn(2, 2, dtype=torch.complex128)
&gt;&gt;&gt; A
tensor([[ 0.9828+0.3889j, -0.4617+0.3010j],
        [ 0.1662-0.7435j, -0.6139+0.0562j]], dtype=torch.complex128)
&gt;&gt;&gt; L, V = torch.linalg.eig(A)
&gt;&gt;&gt; L
tensor([ 1.1226+0.5738j, -0.7537-0.1286j], dtype=torch.complex128)
&gt;&gt;&gt; V
tensor([[ 0.9218+0.0000j,  0.1882-0.2220j],
        [-0.0270-0.3867j,  0.9567+0.0000j]], dtype=torch.complex128)
&gt;&gt;&gt; torch.dist(V @ torch.diag(L) @ torch.linalg.inv(V), A)
tensor(7.7119e-16, dtype=torch.float64)

&gt;&gt;&gt; A = torch.randn(3, 2, 2, dtype=torch.float64)
&gt;&gt;&gt; L, V = torch.linalg.eig(A)
&gt;&gt;&gt; torch.dist(V @ torch.diag_embed(L) @ torch.linalg.inv(V), A)
tensor(3.2841e-16, dtype=torch.float64)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.linalg.eig.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.linalg.eig.html</a>
  </p>
</div>
