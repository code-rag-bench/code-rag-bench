<h1 id="torch-autograd-backward">torch.autograd.backward</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.backward">
<code>torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd.html#backward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Computes the sum of gradients of given tensors with respect to graph leaves.</p> <p>The graph is differentiated using the chain rule. If any of <code>tensors</code> are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed, in this case the function additionally requires specifying <code>grad_tensors</code>. It should be a sequence of matching length, that contains the “vector” in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors (<code>None</code> is an acceptable value for all tensors that don’t need gradient tensors).</p> <p>This function accumulates gradients in the leaves - you might need to zero <code>.grad</code> attributes or set them to <code>None</code> before calling it. See <a class="reference internal" href="../autograd#default-grad-layouts"><span class="std std-ref">Default gradient layouts</span></a> for details on the memory layout of accumulated gradients.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Using this method with <code>create_graph=True</code> will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using <code>autograd.grad</code> when creating the graph to avoid this. If you have to use this function, make sure to reset the <code>.grad</code> fields of your parameters to <code>None</code> after use to break the cycle and avoid the leak.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If you run any forward ops, create <code>grad_tensors</code>, and/or call <code>backward</code> in a user-specified CUDA stream context, see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#bwd-cuda-stream-semantics"><span class="std std-ref">Stream semantics of backward passes</span></a>.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When <code>inputs</code> are provided and a given input is not a leaf, the current implementation will call its grad_fn (even though it is not strictly needed to get this gradients). It is an implementation detail on which the user should not rely. See <a class="reference external" href="https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780">https://github.com/pytorch/pytorch/pull/60521#issuecomment-867061780</a> for more details.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>tensors</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>] or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – Tensors of which the derivative will be computed.</li> <li>
<strong>grad_tensors</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><em>None</em><em>] or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The “vector” in the Jacobian-vector product, usually gradients w.r.t. each element of corresponding tensors. None values can be specified for scalar Tensors or ones that don’t require grad. If a None value would be acceptable for all grad_tensors, then this argument is optional.</li> <li>
<strong>retain_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>False</code>, the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to <code>True</code> is not needed and often can be worked around in a much more efficient way. Defaults to the value of <code>create_graph</code>.</li> <li>
<strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to <code>False</code>.</li> <li>
<strong>inputs</strong> (<em>Sequence</em><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>] or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – Inputs w.r.t. which the gradient be will accumulated into <code>.grad</code>. All other Tensors will be ignored. If not provided, the gradient is accumulated into all the leaf Tensors that were used to compute the attr::tensors.</li> </ul> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.backward.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.backward.html</a>
  </p>
</div>
