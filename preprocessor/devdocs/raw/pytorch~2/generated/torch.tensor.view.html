<h1 id="torch-tensor-view">torch.Tensor.view</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.view">
<code>Tensor.view(*shape) → Tensor</code> </dt> <dd>
<p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <a class="reference internal" href="torch.tensor.shape#torch.Tensor.shape" title="torch.Tensor.shape"><code>shape</code></a>.</p> <p>The returned tensor shares the same data and must have the same number of elements, but may have a different size. For a tensor to be viewed, the new view size must be compatible with its original size and stride, i.e., each new view dimension must either be a subspace of an original dimension, or only span across original dimensions <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo separator="true">,</mo><mi>d</mi><mo>+</mo><mn>1</mn><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo>+</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">d, d+1, \dots, d+k</annotation></semantics></math></span></span></span> that satisfy the following contiguity-like condition that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∀</mi><mi>i</mi><mo>=</mo><mi>d</mi><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mi>d</mi><mo>+</mo><mi>k</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">\forall i = d, \dots, d+k-1</annotation></semantics></math></span></span></span>,</p> <div class="math"> <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>stride</mtext><mo stretchy="false">[</mo><mi>i</mi><mo stretchy="false">]</mo><mo>=</mo><mtext>stride</mtext><mo stretchy="false">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo><mo>×</mo><mtext>size</mtext><mo stretchy="false">[</mo><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{stride}[i] = \text{stride}[i+1] \times \text{size}[i+1]</annotation></semantics></math></span></span></span>
</div>
<p>Otherwise, it will not be possible to view <code>self</code> tensor as <a class="reference internal" href="torch.tensor.shape#torch.Tensor.shape" title="torch.Tensor.shape"><code>shape</code></a> without copying it (e.g., via <a class="reference internal" href="torch.tensor.contiguous#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code>contiguous()</code></a>). When it is unclear whether a <a class="reference internal" href="#torch.Tensor.view" title="torch.Tensor.view"><code>view()</code></a> can be performed, it is advisable to use <a class="reference internal" href="torch.reshape#torch.reshape" title="torch.reshape"><code>reshape()</code></a>, which returns a view if the shapes are compatible, and copies (equivalent to calling <a class="reference internal" href="torch.tensor.contiguous#torch.Tensor.contiguous" title="torch.Tensor.contiguous"><code>contiguous()</code></a>) otherwise.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>shape</strong> (<em>torch.Size</em><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>...</em>) – the desired size</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; x.size()
torch.Size([4, 4])
&gt;&gt;&gt; y = x.view(16)
&gt;&gt;&gt; y.size()
torch.Size([16])
&gt;&gt;&gt; z = x.view(-1, 8)  # the size -1 is inferred from other dimensions
&gt;&gt;&gt; z.size()
torch.Size([2, 8])

&gt;&gt;&gt; a = torch.randn(1, 2, 3, 4)
&gt;&gt;&gt; a.size()
torch.Size([1, 2, 3, 4])
&gt;&gt;&gt; b = a.transpose(1, 2)  # Swaps 2nd and 3rd dimension
&gt;&gt;&gt; b.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; c = a.view(1, 3, 2, 4)  # Does not change tensor layout in memory
&gt;&gt;&gt; c.size()
torch.Size([1, 3, 2, 4])
&gt;&gt;&gt; torch.equal(b, c)
False
</pre> <dl class="py method"> <dt class="sig sig-object py"> <span class="sig-name descname">view</span><span class="sig-paren">(</span><em class="sig-param"><span class="n">dtype</span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">→</span> <span class="sig-return-typehint"><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></span></span>
</dt> 
</dl> <p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>dtype</code></a>.</p> <p>If the element size of <a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>dtype</code></a> is different than that of <code>self.dtype</code>, then the size of the last dimension of the output will be scaled proportionally. For instance, if <a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>dtype</code></a> element size is twice that of <code>self.dtype</code>, then each pair of elements in the last dimension of <code>self</code> will be combined, and the size of the last dimension of the output will be half that of <code>self</code>. If <a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>dtype</code></a> element size is half that of <code>self.dtype</code>, then each element in the last dimension of <code>self</code> will be split in two, and the size of the last dimension of the output will be double that of <code>self</code>. For this to be possible, the following conditions must be true:</p>  <ul class="simple"> <li>
<code>self.dim()</code> must be greater than 0.</li> <li>
<code>self.stride(-1)</code> must be 1.</li> </ul>  <p>Additionally, if the element size of <a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>dtype</code></a> is greater than that of <code>self.dtype</code>, the following conditions must be true as well:</p>  <ul class="simple"> <li>
<code>self.size(-1)</code> must be divisible by the ratio between the element sizes of the dtypes.</li> <li>
<code>self.storage_offset()</code> must be divisible by the ratio between the element sizes of the dtypes.</li> <li>The strides of all dimensions, except the last dimension, must be divisible by the ratio between the element sizes of the dtypes.</li> </ul>  <p>If any of the above conditions are not met, an error is thrown.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This overload is not supported by TorchScript, and using it in a Torchscript program will cause undefined behavior.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>dtype</strong> (<a class="reference internal" href="../tensor_attributes#torch.dtype" title="torch.dtype"><code>torch.dtype</code></a>) – the desired dtype</p> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; x = torch.randn(4, 4)
&gt;&gt;&gt; x
tensor([[ 0.9482, -0.0310,  1.4999, -0.5316],
        [-0.1520,  0.7472,  0.5617, -0.8649],
        [-2.4724, -0.0334, -0.2976, -0.8499],
        [-0.2109,  1.9913, -0.9607, -0.6123]])
&gt;&gt;&gt; x.dtype
torch.float32

&gt;&gt;&gt; y = x.view(torch.int32)
&gt;&gt;&gt; y
tensor([[ 1064483442, -1124191867,  1069546515, -1089989247],
        [-1105482831,  1061112040,  1057999968, -1084397505],
        [-1071760287, -1123489973, -1097310419, -1084649136],
        [-1101533110,  1073668768, -1082790149, -1088634448]],
    dtype=torch.int32)
&gt;&gt;&gt; y[0, 0] = 1000000000
&gt;&gt;&gt; x
tensor([[ 0.0047, -0.0310,  1.4999, -0.5316],
        [-0.1520,  0.7472,  0.5617, -0.8649],
        [-2.4724, -0.0334, -0.2976, -0.8499],
        [-0.2109,  1.9913, -0.9607, -0.6123]])

&gt;&gt;&gt; x.view(torch.cfloat)
tensor([[ 0.0047-0.0310j,  1.4999-0.5316j],
        [-0.1520+0.7472j,  0.5617-0.8649j],
        [-2.4724-0.0334j, -0.2976-0.8499j],
        [-0.2109+1.9913j, -0.9607-0.6123j]])
&gt;&gt;&gt; x.view(torch.cfloat).size()
torch.Size([4, 2])

&gt;&gt;&gt; x.view(torch.uint8)
tensor([[  0, 202, 154,  59, 182, 243, 253, 188, 185, 252, 191,  63, 240,  22,
           8, 191],
        [227, 165,  27, 190, 128,  72,  63,  63, 146, 203,  15,  63,  22, 106,
          93, 191],
        [205,  59,  30, 192, 112, 206,   8, 189,   7,  95, 152, 190,  12, 147,
          89, 191],
        [ 43, 246,  87, 190, 235, 226, 254,  63, 111, 240, 117, 191, 177, 191,
          28, 191]], dtype=torch.uint8)
&gt;&gt;&gt; x.view(torch.uint8).size()
torch.Size([4, 16])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.view.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.view.html</a>
  </p>
</div>
