<h1 id="torch-func-jacfwd">torch.func.jacfwd</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.func.jacfwd">
<code>torch.func.jacfwd(func, argnums=0, has_aux=False, *, randomness='error')</code> </dt> <dd>
<p>Computes the Jacobian of <code>func</code> with respect to the arg(s) at index <code>argnum</code> using forward-mode autodiff</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – A Python function that takes one or more arguments, one of which must be a Tensor, and returns one or more Tensors</li> <li>
<strong>argnums</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em> or </em><em>Tuple</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>]</em>) – Optional, integer or tuple of integers, saying which arguments to get the Jacobian with respect to. Default: 0.</li> <li>
<strong>has_aux</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Flag indicating that <code>func</code> returns a <code>(output, aux)</code> tuple where the first element is the output of the function to be differentiated and the second element is auxiliary objects that will not be differentiated. Default: False.</li> <li>
<strong>randomness</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a>) – Flag indicating what type of randomness to use. See <a class="reference internal" href="torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> for more detail. Allowed: “different”, “same”, “error”. Default: “error”</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Returns a function that takes in the same inputs as <code>func</code> and returns the Jacobian of <code>func</code> with respect to the arg(s) at <code>argnums</code>. If <code>has_aux is True</code>, then the returned function instead returns a <code>(jacobian, aux)</code> tuple where <code>jacobian</code> is the Jacobian and <code>aux</code> is auxiliary objects returned by <code>func</code>.</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>You may see this API error out with “forward-mode AD not implemented for operator X”. If so, please file a bug report and we will prioritize it. An alternative is to use <a class="reference internal" href="torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a>, which has better operator coverage.</p> </div> <p>A basic usage with a pointwise, unary operation will give a diagonal array as the Jacobian</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; jacobian = jacfwd(torch.sin)(x)
&gt;&gt;&gt; expected = torch.diag(torch.cos(x))
&gt;&gt;&gt; assert torch.allclose(jacobian, expected)
</pre> <p><a class="reference internal" href="#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd()</code></a> can be composed with vmap to produce batched Jacobians:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd, vmap
&gt;&gt;&gt; x = torch.randn(64, 5)
&gt;&gt;&gt; jacobian = vmap(jacfwd(torch.sin))(x)
&gt;&gt;&gt; assert jacobian.shape == (64, 5, 5)
</pre> <p>If you would like to compute the output of the function as well as the jacobian of the function, use the <code>has_aux</code> flag to return the output as an auxiliary object:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt;
&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;   return x.sin()
&gt;&gt;&gt;
&gt;&gt;&gt; def g(x):
&gt;&gt;&gt;   result = f(x)
&gt;&gt;&gt;   return result, result
&gt;&gt;&gt;
&gt;&gt;&gt; jacobian_f, f_x = jacfwd(g, has_aux=True)(x)
&gt;&gt;&gt; assert torch.allclose(f_x, f(x))
</pre> <p>Additionally, <a class="reference internal" href="torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a> can be composed with itself or <a class="reference internal" href="torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a> to produce Hessians</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd, jacrev
&gt;&gt;&gt; def f(x):
&gt;&gt;&gt;   return x.sin().sum()
&gt;&gt;&gt;
&gt;&gt;&gt; x = torch.randn(5)
&gt;&gt;&gt; hessian = jacfwd(jacrev(f))(x)
&gt;&gt;&gt; assert torch.allclose(hessian, torch.diag(-x.sin()))
</pre> <p>By default, <a class="reference internal" href="#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd()</code></a> computes the Jacobian with respect to the first input. However, it can compute the Jacboian with respect to a different argument by using <code>argnums</code>:</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd
&gt;&gt;&gt; def f(x, y):
&gt;&gt;&gt;   return x + y ** 2
&gt;&gt;&gt;
&gt;&gt;&gt; x, y = torch.randn(5), torch.randn(5)
&gt;&gt;&gt; jacobian = jacfwd(f, argnums=1)(x, y)
&gt;&gt;&gt; expected = torch.diag(2 * y)
&gt;&gt;&gt; assert torch.allclose(jacobian, expected)
</pre> <p>Additionally, passing a tuple to <code>argnums</code> will compute the Jacobian with respect to multiple arguments</p> <pre data-language="python">&gt;&gt;&gt; from torch.func import jacfwd
&gt;&gt;&gt; def f(x, y):
&gt;&gt;&gt;   return x + y ** 2
&gt;&gt;&gt;
&gt;&gt;&gt; x, y = torch.randn(5), torch.randn(5)
&gt;&gt;&gt; jacobian = jacfwd(f, argnums=(0, 1))(x, y)
&gt;&gt;&gt; expectedX = torch.diag(torch.ones_like(x))
&gt;&gt;&gt; expectedY = torch.diag(2 * y)
&gt;&gt;&gt; assert torch.allclose(jacobian[0], expectedX)
&gt;&gt;&gt; assert torch.allclose(jacobian[1], expectedY)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.func.jacfwd.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.func.jacfwd.html</a>
  </p>
</div>
