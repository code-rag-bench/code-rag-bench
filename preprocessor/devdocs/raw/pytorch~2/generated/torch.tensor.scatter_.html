<h1 id="torch-tensor-scatter">torch.Tensor.scatter_</h1> <dl class="py method"> <dt class="sig sig-object py" id="torch.Tensor.scatter_">
<code>Tensor.scatter_(dim, index, src, reduce=None) → Tensor</code> </dt> <dd>
<p>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor. For each value in <code>src</code>, its output index is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p> <p>For a 3-D tensor, <code>self</code> is updated as:</p> <pre data-language="python">self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2
</pre> <p>This is the reverse operation of the manner described in <a class="reference internal" href="torch.tensor.gather#torch.Tensor.gather" title="torch.Tensor.gather"><code>gather()</code></a>.</p> <p><code>self</code>, <code>index</code> and <code>src</code> (if it is a Tensor) should all have the same number of dimensions. It is also required that <code>index.size(d) &lt;= src.size(d)</code> for all dimensions <code>d</code>, and that <code>index.size(d) &lt;= self.size(d)</code> for all dimensions <code>d != dim</code>. Note that <code>index</code> and <code>src</code> do not broadcast.</p> <p>Moreover, as for <a class="reference internal" href="torch.tensor.gather#torch.Tensor.gather" title="torch.Tensor.gather"><code>gather()</code></a>, the values of <code>index</code> must be between <code>0</code> and <code>self.size(dim) - 1</code> inclusive.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>When indices are not unique, the behavior is non-deterministic (one of the values from <code>src</code> will be picked arbitrarily) and the gradient will be incorrect (it will be propagated to all locations in the source that correspond to the same index)!</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The backward pass is implemented only for <code>src.shape == index.shape</code>.</p> </div> <p>Additionally accepts an optional <code>reduce</code> argument that allows specification of an optional reduction operation, which is applied to all values in the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code>. For each value in <code>src</code>, the reduction operation is applied to an index in <code>self</code> which is specified by its index in <code>src</code> for <code>dimension != dim</code> and by the corresponding value in <code>index</code> for <code>dimension = dim</code>.</p> <p>Given a 3-D tensor and reduction using the multiplication operation, <code>self</code> is updated as:</p> <pre data-language="python">self[index[i][j][k]][j][k] *= src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] *= src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] *= src[i][j][k]  # if dim == 2
</pre> <p>Reducing with the addition operation is the same as using <a class="reference internal" href="torch.tensor.scatter_add_#torch.Tensor.scatter_add_" title="torch.Tensor.scatter_add_"><code>scatter_add_()</code></a>.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The reduce argument with Tensor <code>src</code> is deprecated and will be removed in a future PyTorch release. Please use <a class="reference internal" href="torch.tensor.scatter_reduce_#torch.Tensor.scatter_reduce_" title="torch.Tensor.scatter_reduce_"><code>scatter_reduce_()</code></a> instead for more reduction options.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dim</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the axis along which to index</li> <li>
<strong>index</strong> (<em>LongTensor</em>) – the indices of elements to scatter, can be either empty or of the same dimensionality as <code>src</code>. When empty, the operation returns <code>self</code> unchanged.</li> <li>
<strong>src</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the source element(s) to scatter.</li> <li>
<strong>reduce</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – reduction operation to apply, can be either <code>'add'</code> or <code>'multiply'</code>.</li> </ul> </dd> </dl> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; src = torch.arange(1, 11).reshape((2, 5))
&gt;&gt;&gt; src
tensor([[ 1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10]])
&gt;&gt;&gt; index = torch.tensor([[0, 1, 2, 0]])
&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)
tensor([[1, 0, 0, 4, 0],
        [0, 2, 0, 0, 0],
        [0, 0, 3, 0, 0]])
&gt;&gt;&gt; index = torch.tensor([[0, 1, 2], [0, 1, 4]])
&gt;&gt;&gt; torch.zeros(3, 5, dtype=src.dtype).scatter_(1, index, src)
tensor([[1, 2, 3, 0, 0],
        [6, 7, 0, 0, 8],
        [0, 0, 0, 0, 0]])

&gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),
...            1.23, reduce='multiply')
tensor([[2.0000, 2.0000, 2.4600, 2.0000],
        [2.0000, 2.0000, 2.0000, 2.4600]])
&gt;&gt;&gt; torch.full((2, 4), 2.).scatter_(1, torch.tensor([[2], [3]]),
...            1.23, reduce='add')
tensor([[2.0000, 2.0000, 3.2300, 2.0000],
        [2.0000, 2.0000, 2.0000, 3.2300]])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.Tensor.scatter_.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.Tensor.scatter_.html</a>
  </p>
</div>
