<h1 id="no-grad">no_grad</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.no_grad">
<code>class torch.no_grad(orig_func=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/grad_mode.html#no_grad"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Context-manager that disables gradient calculation.</p> <p>Disabling gradient calculation is useful for inference, when you are sure that you will not call <a class="reference internal" href="torch.tensor.backward#torch.Tensor.backward" title="torch.Tensor.backward"><code>Tensor.backward()</code></a>. It will reduce memory consumption for computations that would otherwise have <code>requires_grad=True</code>.</p> <p>In this mode, the result of every computation will have <code>requires_grad=False</code>, even when the inputs have <code>requires_grad=True</code>. There is an exception! All factory functions, or functions that create a new Tensor and take a requires_grad kwarg, will NOT be affected by this mode.</p> <p>This context manager is thread local; it will not affect computation in other threads.</p> <p>Also functions as a decorator.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>No-grad is one of several mechanisms that can enable or disable gradients locally see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/autograd.html#locally-disable-grad-doc"><span class="std std-ref">Locally disabling gradient computation</span></a> for more information on how they compare.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This API does not apply to <a class="reference internal" href="../autograd#forward-mode-ad"><span class="std std-ref">forward-mode AD</span></a>. If you want to disable forward AD for a computation, you can unpack your dual tensors.</p> </div> <dl> <dt>Example::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; x = torch.tensor([1.], requires_grad=True)
&gt;&gt;&gt; with torch.no_grad():
...     y = x * 2
&gt;&gt;&gt; y.requires_grad
False
&gt;&gt;&gt; @torch.no_grad()
... def doubler(x):
...     return x * 2
&gt;&gt;&gt; z = doubler(x)
&gt;&gt;&gt; z.requires_grad
False
&gt;&gt;&gt; @torch.no_grad
... def tripler(x):
...     return x * 3
&gt;&gt;&gt; z = tripler(x)
&gt;&gt;&gt; z.requires_grad
False
&gt;&gt;&gt; # factory function exception
&gt;&gt;&gt; with torch.no_grad():
...     a = torch.nn.Parameter(torch.rand(10))
&gt;&gt;&gt; a.requires_grad
True
</pre> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.no_grad.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.no_grad.html</a>
  </p>
</div>
