<h1 id="torch-cuda-make-graphed-callables">torch.cuda.make_graphed_callables</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.cuda.make_graphed_callables">
<code>torch.cuda.make_graphed_callables(callables, sample_args, num_warmup_iters=3, allow_unused_input=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/cuda/graphs.html#make_graphed_callables"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Accepts callables (functions or <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>nn.Module</code></a>s) and returns graphed versions.</p> <p>Each graphed callable’s forward pass runs its source callable’s forward CUDA work as a CUDA graph inside a single autograd node.</p> <p>The graphed callable’s forward pass also appends a backward node to the autograd graph. During backward, this node runs the callable’s backward work as a CUDA graph.</p> <p>Therefore, each graphed callable should be a drop-in replacement for its source callable in an autograd-enabled training loop.</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#partial-network-capture"><span class="std std-ref">Partial-network capture</span></a> for detailed use and constraints.</p> <p>If you pass a tuple of several callables, their captures will use the same memory pool. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#graph-memory-management"><span class="std std-ref">Graph memory management</span></a> for when this is appropriate.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>callables</strong> (<a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module">torch.nn.Module</a><em> or </em><em>Python function</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>these</em>) – Callable or callables to graph. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#graph-memory-management"><span class="std std-ref">Graph memory management</span></a> for when passing a tuple of callables is appropriate. If you pass a tuple of callables, their order in the tuple must be the same order they’ll run in the live workload.</li> <li>
<strong>sample_args</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em>, or </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>tuples</em><em> of </em><em>Tensors</em>) – Samples args for each callable. If a single callable was passed, <code>sample_args</code> must be a single tuple of argument Tensors. If a tuple of callables was passed, <code>sample_args</code> must be tuple of tuples of argument Tensors.</li> <li>
<strong>num_warmup_iters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – The number of warmup iterations. Currently, <code>DataDistributedParallel</code> needs 11 iterations for warm up. Default: <code>3</code>.</li> <li>
<strong>allow_unused_input</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If False, specifying inputs that were not used when computing outputs (and therefore their grad is always zero) is an error. Defaults to False.</li> </ul> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>requires_grad</code> state of each Tensor in <code>sample_args</code> must match the state that’s expected for the corresponding real input in the training loop.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>This API is in beta and may change in future releases.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>sample_args</code> for each callable must contain only Tensors. Other types are not allowed.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Returned callables do not support higher order differentiation (e.g., double backward).</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>In any <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>Module</code></a> passed to <a class="reference internal" href="#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>, only parameters may be trainable. Buffers must have <code>requires_grad=False</code>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>After you pass a <a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a> through <a class="reference internal" href="#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a>, you may not add or remove any of that Module’s parameters or buffers.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><a class="reference internal" href="torch.nn.module#torch.nn.Module" title="torch.nn.Module"><code>torch.nn.Module</code></a>s passed to <a class="reference internal" href="#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> must not have module hooks registered on them at the time they are passed. However, registering hooks on modules <em>after</em> passing them through <a class="reference internal" href="#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> is allowed.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>When running a graphed callable, you must pass its arguments in the same order and format they appeared in that callable’s <code>sample_args</code>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The automatic mixed precision is supported in <a class="reference internal" href="#torch.cuda.make_graphed_callables" title="torch.cuda.make_graphed_callables"><code>make_graphed_callables()</code></a> only with disabled caching. The context manager <code>torch.cuda.amp.autocast()</code> must have <code>cache_enabled=False</code>.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.cuda.make_graphed_callables.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.cuda.make_graphed_callables.html</a>
  </p>
</div>
