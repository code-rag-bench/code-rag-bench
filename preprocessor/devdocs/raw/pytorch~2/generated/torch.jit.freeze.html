<h1 id="torch-jit-freeze">torch.jit.freeze</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.jit.freeze">
<code>torch.jit.freeze(mod, preserved_attrs=None, optimize_numerics=True)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/jit/_freeze.html#freeze"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Freezing a <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a> will clone it and attempt to inline the cloned module’s submodules, parameters, and attributes as constants in the TorchScript IR Graph. By default, <code>forward</code> will be preserved, as well as attributes &amp; methods specified in <code>preserved_attrs</code>. Additionally, any attribute that is modified within a preserved method will be preserved.</p> <p>Freezing currently only accepts ScriptModules that are in eval mode.</p> <p>Freezing applies generic optimization that will speed up your model regardless of machine. To further optimize using server-specific settings, run <code>optimize_for_inference</code> after freezing.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>mod</strong> (<a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a>) – a module to be frozen</li> <li>
<strong>preserved_attrs</strong> (<em>Optional</em><em>[</em><em>List</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>]</em><em>]</em>) – a list of attributes to preserve in addition to the forward method. Attributes modified in preserved methods will also be preserved.</li> <li>
<strong>optimize_numerics</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, a set of optimization passes will be run that does not strictly preserve numerics. Full details of optimization can be found at <code>torch.jit.run_frozen_optimizations</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>Frozen <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a>.</p> </dd> </dl> <p>Example (Freezing a simple module with a Parameter):</p> <pre data-language="python">    def forward(self, input):
        output = self.weight.mm(input)
        output = self.linear(output)
        return output

scripted_module = torch.jit.script(MyModule(2, 3).eval())
frozen_module = torch.jit.freeze(scripted_module)
# parameters have been removed and inlined into the Graph as constants
assert len(list(frozen_module.named_parameters())) == 0
# See the compiled graph as Python code
print(frozen_module.code)
</pre> <p>Example (Freezing a module with preserved attributes)</p> <pre data-language="python">    def forward(self, input):
        self.modified_tensor += 1
        return input + self.modified_tensor

scripted_module = torch.jit.script(MyModule2().eval())
frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=["version"])
# we've manually preserved `version`, so it still exists on the frozen module and can be modified
assert frozen_module.version == 1
frozen_module.version = 2
# `modified_tensor` is detected as being mutated in the forward, so freezing preserves
# it to retain model semantics
assert frozen_module(torch.tensor(1)) == torch.tensor(12)
# now that we've run it once, the next result will be incremented by one
assert frozen_module(torch.tensor(1)) == torch.tensor(13)
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Freezing submodule attributes is also supported: frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[“submodule.version”])</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>If you’re not sure why an attribute is not being inlined as a constant, you can run <code>dump_alias_db</code> on frozen_module.forward.graph to see if freezing has detected the attribute is being modified.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Because freezing makes weights constants and removes module hierarchy, <code>to</code> and other nn.Module methods to manipulate device or dtype no longer work. As a workaround, You can remap devices by specifying <code>map_location</code> in <code>torch.jit.load</code>, however device-specific logic may have been baked into the model.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.jit.freeze.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.jit.freeze.html</a>
  </p>
</div>
