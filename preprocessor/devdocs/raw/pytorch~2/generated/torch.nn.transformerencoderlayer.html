<h1 id="transformerencoderlayer">TransformerEncoderLayer</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.TransformerEncoderLayer">
<code>class torch.nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=&lt;function relu&gt;, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper “Attention Is All You Need”. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application.</p> <p>TransformerEncoderLayer can handle either traditional torch.tensor inputs, or Nested Tensor inputs. Derived classes are expected to similarly accept both input formats. (Not all combinations of inputs are currently supported by TransformerEncoderLayer while Nested Tensor is in prototype state.)</p> <p>If you are implementing a custom layer, you may derive it either from the Module or TransformerEncoderLayer class. If your custom layer supports both torch.Tensors and Nested Tensors inputs, make its implementation a derived class of TransformerEncoderLayer. If your custom Layer supports only torch.Tensor inputs, derive its implementation from Module.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of expected features in the input (required).</li> <li>
<strong>nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of heads in the multiheadattention models (required).</li> <li>
<strong>dim_feedforward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the dimension of the feedforward network model (default=2048).</li> <li>
<strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the dropout value (default=0.1).</li> <li>
<strong>activation</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a><em>[</em><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>]</em>) – the activation function of the intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu</li> <li>
<strong>layer_norm_eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the eps value in layer normalization components (default=1e-5).</li> <li>
<strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature).</li> <li>
<strong>norm_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – if <code>True</code>, layer norm is done prior to attention and feedforward operations, respectively. Otherwise it’s done after. Default: <code>False</code> (after).</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If set to <code>False</code>, <code>Linear</code> and <code>LayerNorm</code> layers will not learn an additive bias. Default: <code>True</code>.</li> </ul> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
&gt;&gt;&gt; src = torch.rand(10, 32, 512)
&gt;&gt;&gt; out = encoder_layer(src)
</pre> </dd> <dt>
<code>Alternatively, when batch_first is True:</code> </dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)
&gt;&gt;&gt; src = torch.rand(32, 10, 512)
&gt;&gt;&gt; out = encoder_layer(src)
</pre> </dd> <dt>Fast path:</dt>
<dd>
<p>forward() will use a special optimized implementation described in <a class="reference external" href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> if all of the following conditions are met:</p> <ul class="simple"> <li>Either autograd is disabled (using <code>torch.inference_mode</code> or <code>torch.no_grad</code>) or no tensor argument <code>requires_grad</code>
</li> <li>training is disabled (using <code>.eval()</code>)</li> <li>batch_first is <code>True</code> and the input is batched (i.e., <code>src.dim() == 3</code>)</li> <li>activation is one of: <code>"relu"</code>, <code>"gelu"</code>, <code>torch.functional.relu</code>, or <code>torch.functional.gelu</code>
</li> <li>at most one of <code>src_mask</code> and <code>src_key_padding_mask</code> is passed</li> <li>if src is a <a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a>, neither <code>src_mask</code> nor <code>src_key_padding_mask</code> is passed</li> <li>the two <code>LayerNorm</code> instances have a consistent <code>eps</code> value (this will naturally be the case unless the caller has manually modified one without modifying the other)</li> </ul> <p>If the optimized implementation is in use, a <a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a> can be passed for <code>src</code> to represent padding more efficiently than using a padding mask. In this case, a <a class="reference external" href="https://pytorch.org/docs/stable/nested.html">NestedTensor</a> will be returned, and an additional speedup proportional to the fraction of the input that is padding can be expected.</p> </dd> </dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.TransformerEncoderLayer.forward">
<code>forward(src, src_mask=None, src_key_padding_mask=None, is_causal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/transformer.html#TransformerEncoderLayer.forward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Pass the input through the encoder layer.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>src</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the sequence to the encoder layer (required).</li> <li>
<strong>src_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the mask for the src sequence (optional).</li> <li>
<strong>src_key_padding_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the mask for the src keys per batch (optional).</li> <li>
<strong>is_causal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If specified, applies a causal mask as <code>src mask</code>. Default: <code>False</code>. Warning: <code>is_causal</code> provides a hint that <code>src_mask</code> is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <dl class="simple"> <dt>Shape:</dt>
<dd>
<p>see the docs in Transformer class.</p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.TransformerEncoderLayer.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.TransformerEncoderLayer.html</a>
  </p>
</div>
