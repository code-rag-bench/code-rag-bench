<h1 id="transformer">Transformer</h1> <dl class="py class"> <dt class="sig sig-object py" id="torch.nn.Transformer">
<code>class torch.nn.Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1, activation=&lt;function relu&gt;, custom_encoder=None, custom_decoder=None, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/transformer.html#Transformer"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>A transformer model. User is able to modify the attributes as needed. The architecture is based on the paper “Attention Is All You Need”. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>d_model</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of expected features in the encoder/decoder inputs (default=512).</li> <li>
<strong>nhead</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of heads in the multiheadattention models (default=8).</li> <li>
<strong>num_encoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of sub-encoder-layers in the encoder (default=6).</li> <li>
<strong>num_decoder_layers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the number of sub-decoder-layers in the decoder (default=6).</li> <li>
<strong>dim_feedforward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – the dimension of the feedforward network model (default=2048).</li> <li>
<strong>dropout</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the dropout value (default=0.1).</li> <li>
<strong>activation</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a><em>[</em><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>, </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em><em>]</em>) – the activation function of encoder/decoder intermediate layer, can be a string (“relu” or “gelu”) or a unary callable. Default: relu</li> <li>
<strong>custom_encoder</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a><em>]</em>) – custom encoder (default=None).</li> <li>
<strong>custom_decoder</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">Any</a><em>]</em>) – custom decoder (default=None).</li> <li>
<strong>layer_norm_eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a>) – the eps value in layer normalization components (default=1e-5).</li> <li>
<strong>batch_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, then the input and output tensors are provided as (batch, seq, feature). Default: <code>False</code> (seq, batch, feature).</li> <li>
<strong>norm_first</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – if <code>True</code>, encoder and decoder layers will perform LayerNorms before other attention and feedforward operations, otherwise after. Default: <code>False</code> (after).</li> <li>
<strong>bias</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If set to <code>False</code>, <code>Linear</code> and <code>LayerNorm</code> layers will not learn an additive bias. Default: <code>True</code>.</li> </ul> </dd> </dl> <dl> <dt>Examples::</dt>
<dd>
<pre data-language="python">&gt;&gt;&gt; transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)
&gt;&gt;&gt; src = torch.rand((10, 32, 512))
&gt;&gt;&gt; tgt = torch.rand((20, 32, 512))
&gt;&gt;&gt; out = transformer_model(src, tgt)
</pre> </dd> </dl> <p>Note: A full example to apply nn.Transformer module for the word language model is available in <a class="reference external" href="https://github.com/pytorch/examples/tree/master/word_language_model">https://github.com/pytorch/examples/tree/master/word_language_model</a></p> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.Transformer.forward">
<code>forward(src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None, src_is_causal=None, tgt_is_causal=None, memory_is_causal=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/transformer.html#Transformer.forward"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Take in and process masked source/target sequences.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>src</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the sequence to the encoder (required).</li> <li>
<strong>tgt</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the sequence to the decoder (required).</li> <li>
<strong>src_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the additive mask for the src sequence (optional).</li> <li>
<strong>tgt_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the additive mask for the tgt sequence (optional).</li> <li>
<strong>memory_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the additive mask for the encoder output (optional).</li> <li>
<strong>src_key_padding_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the Tensor mask for src keys per batch (optional).</li> <li>
<strong>tgt_key_padding_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the Tensor mask for tgt keys per batch (optional).</li> <li>
<strong>memory_key_padding_mask</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>]</em>) – the Tensor mask for memory keys per batch (optional).</li> <li>
<strong>src_is_causal</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>]</em>) – If specified, applies a causal mask as <code>src_mask</code>. Default: <code>None</code>; try to detect a causal mask. Warning: <code>src_is_causal</code> provides a hint that <code>src_mask</code> is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.</li> <li>
<strong>tgt_is_causal</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>]</em>) – If specified, applies a causal mask as <code>tgt_mask</code>. Default: <code>None</code>; try to detect a causal mask. Warning: <code>tgt_is_causal</code> provides a hint that <code>tgt_mask</code> is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.</li> <li>
<strong>memory_is_causal</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If specified, applies a causal mask as <code>memory_mask</code>. Default: <code>False</code>. Warning: <code>memory_is_causal</code> provides a hint that <code>memory_mask</code> is the causal mask. Providing incorrect hints can result in incorrect execution, including forward and backward compatibility.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <dl> <dt>Shape:</dt>
<dd>
<ul class="simple"> <li>src: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, E)</annotation></semantics></math></span></span></span> for unbatched input, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, N, E)</annotation></semantics></math></span></span></span> if <code>batch_first=False</code> or <code>(N, S, E)</code> if <code>batch_first=True</code>.</li> <li>tgt: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, E)</annotation></semantics></math></span></span></span> for unbatched input, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, N, E)</annotation></semantics></math></span></span></span> if <code>batch_first=False</code> or <code>(N, T, E)</code> if <code>batch_first=True</code>.</li> <li>src_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S, S)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>⋅</mo><mtext>num_heads</mtext><mo separator="true">,</mo><mi>S</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N\cdot\text{num\_heads}, S, S)</annotation></semantics></math></span></span></span>.</li> <li>tgt_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, T)</annotation></semantics></math></span></span></span> or <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo>⋅</mo><mtext>num_heads</mtext><mo separator="true">,</mo><mi>T</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N\cdot\text{num\_heads}, T, T)</annotation></semantics></math></span></span></span>.</li> <li>memory_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, S)</annotation></semantics></math></span></span></span>.</li> <li>src_key_padding_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S)</annotation></semantics></math></span></span></span> for unbatched input otherwise <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span></span>.</li> <li>tgt_key_padding_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T)</annotation></semantics></math></span></span></span> for unbatched input otherwise <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>T</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, T)</annotation></semantics></math></span></span></span>.</li> <li>memory_key_padding_mask: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(S)</annotation></semantics></math></span></span></span> for unbatched input otherwise <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>N</mi><mo separator="true">,</mo><mi>S</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(N, S)</annotation></semantics></math></span></span></span>.</li> </ul> <p>Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked positions. If a BoolTensor is provided, positions with <code>True</code> are not allowed to attend while <code>False</code> values will be unchanged. If a FloatTensor is provided, it will be added to the attention weight. [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by the attention. If a BoolTensor is provided, the positions with the value of <code>True</code> will be ignored while the position with the value of <code>False</code> will be unchanged.</p> <ul class="simple"> <li>output: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, E)</annotation></semantics></math></span></span></span> for unbatched input, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>T</mi><mo separator="true">,</mo><mi>N</mi><mo separator="true">,</mo><mi>E</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(T, N, E)</annotation></semantics></math></span></span></span> if <code>batch_first=False</code> or <code>(N, T, E)</code> if <code>batch_first=True</code>.</li> </ul> <p>Note: Due to the multi-head attention architecture in the transformer model, the output sequence length of a transformer is same as the input sequence (i.e. target) length of the decoder.</p> <p>where S is the source sequence length, T is the target sequence length, N is the batch size, E is the feature number</p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
</pre> </dd>
</dl> <dl class="py method"> <dt class="sig sig-object py" id="torch.nn.Transformer.generate_square_subsequent_mask">
<code>static generate_square_subsequent_mask(sz, device=device(type='cpu'), dtype=torch.float32)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/nn/modules/transformer.html#Transformer.generate_square_subsequent_mask"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Generate a square causal mask for the sequence. The masked positions are filled with float(‘-inf’). Unmasked positions are filled with float(0.0).</p> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> </dd>
</dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.nn.Transformer.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.nn.Transformer.html</a>
  </p>
</div>
