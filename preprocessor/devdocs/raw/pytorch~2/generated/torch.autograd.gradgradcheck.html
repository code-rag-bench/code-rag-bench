<h1 id="torch-autograd-gradgradcheck">torch.autograd.gradgradcheck</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.gradgradcheck">
<code>torch.autograd.gradgradcheck(func, inputs, grad_outputs=None, *, eps=1e-06, atol=1e-05, rtol=0.001, gen_non_contig_grad_outputs=False, raise_exception=True, nondet_tol=0.0, check_undefined_grad=True, check_grad_dtypes=False, check_batched_grad=False, check_fwd_over_rev=False, check_rev_over_rev=True, fast_mode=False, masked=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/gradcheck.html#gradgradcheck"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Check gradients of gradients computed via small finite differences against analytical gradients wrt tensors in <code>inputs</code> and <code>grad_outputs</code> that are of floating point or complex type and with <code>requires_grad=True</code>.</p> <p>This function checks that backpropagating through the gradients computed to the given <code>grad_outputs</code> are correct.</p> <p>The check between numerical and analytical gradients uses <a class="reference internal" href="torch.allclose#torch.allclose" title="torch.allclose"><code>allclose()</code></a>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The default values are designed for <code>input</code> and <code>grad_outputs</code> of double precision. This check will likely fail if they are of less precision, e.g., <code>FloatTensor</code>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If any checked tensor in <code>input</code> and <code>grad_outputs</code> has overlapping memory, i.e., different indices pointing to the same memory address (e.g., from <code>torch.expand()</code>), this check will likely fail because the numerical gradients computed by point perturbation at such indices will change values at all other indices that share the same memory address.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor or a tuple of Tensors</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function</li> <li>
<strong>grad_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a><em>, </em><em>optional</em>) – The gradients with respect to the function’s outputs.</li> <li>
<strong>eps</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – perturbation for finite differences</li> <li>
<strong>atol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – absolute tolerance</li> <li>
<strong>rtol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – relative tolerance</li> <li>
<strong>gen_non_contig_grad_outputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>grad_outputs</code> is <code>None</code> and <code>gen_non_contig_grad_outputs</code> is <code>True</code>, the randomly generated gradient outputs are made to be noncontiguous</li> <li>
<strong>raise_exception</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – indicating whether to raise an exception if the check fails. The exception gives more information about the exact nature of the failure. This is helpful when debugging gradchecks.</li> <li>
<strong>nondet_tol</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">float</a><em>, </em><em>optional</em>) – tolerance for non-determinism. When running identical inputs through the differentiation, the results must either match exactly (default, 0.0) or be within this tolerance. Note that a small amount of nondeterminism in the gradient will lead to larger inaccuracies in the second derivative.</li> <li>
<strong>check_undefined_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if True, check if undefined output grads are supported and treated as zeros</li> <li>
<strong>check_batched_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if True, check if we can compute batched gradients using prototype vmap support. Defaults to False.</li> <li>
<strong>fast_mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if True, run a faster implementation of gradgradcheck that no longer computes the entire jacobian.</li> <li>
<strong>masked</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if True, the gradients of unspecified elements of sparse tensors are ignored (default, False).</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>True if all differences satisfy allclose condition</p> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a></p> </dd> </dl> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.gradgradcheck.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.gradgradcheck.html</a>
  </p>
</div>
