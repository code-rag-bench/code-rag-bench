<h1 id="torch-pca-lowrank">torch.pca_lowrank</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.pca_lowrank">
<code>torch.pca_lowrank(A, q=None, center=True, niter=2)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/_lowrank.html#pca_lowrank"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix.</p> <p>This function returns a namedtuple <code>(U, S, V)</code> which is the nearly optimal approximation of a singular value decomposition of a centered matrix <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> such that <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi><mo>=</mo><mi>U</mi><mi>d</mi><mi>i</mi><mi>a</mi><mi>g</mi><mo stretchy="false">(</mo><mi>S</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mi>T</mi></msup></mrow><annotation encoding="application/x-tex">A = U diag(S) V^T</annotation></semantics></math></span></span></span>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The relation of <code>(U, S, V)</code> to PCA is as follows:</p> <ul class="simple"> <li>
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span> is a data matrix with <code>m</code> samples and <code>n</code> features</li> <li>the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span></span></span> columns represent the principal directions</li> <li>
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mo>∗</mo><mo>∗</mo><mn>2</mn><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>m</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">S ** 2 / (m - 1)</annotation></semantics></math></span></span></span> contains the eigenvalues of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>T</mi></msup><mi>A</mi><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mi>m</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">A^T A / (m - 1)</annotation></semantics></math></span></span></span> which is the covariance of <code>A</code> when <code>center=True</code> is provided.</li> <li>
<code>matmul(A, V[:, :k])</code> projects data to the first k principal components</li> </ul> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Different from the standard SVD, the size of returned matrices depend on the specified rank and q values as follows:</p>  <ul class="simple"> <li>
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span></span></span> is m x q matrix</li> <li>
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi></mrow><annotation encoding="application/x-tex">S</annotation></semantics></math></span></span></span> is q-vector</li> <li>
<span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span></span></span> is n x q matrix</li> </ul>  </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>To obtain repeatable results, reset the seed for the pseudorandom number generator</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>A</strong> (<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – the input tensor of size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>m</mi><mo separator="true">,</mo><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, m, n)</annotation></semantics></math></span></span></span>
</li> <li>
<strong>q</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – a slightly overestimated rank of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span></span></span>. By default, <code>q = min(6, m,
n)</code>.</li> <li>
<strong>center</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if True, center the input tensor, otherwise, assume that the input is centered.</li> <li>
<strong>niter</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – the number of subspace iterations to conduct; niter must be a nonnegative integer, and defaults to 2.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a>[<a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>, <a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>]</p> </dd> </dl> <p>References:</p> <pre data-language="python">- Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding
  structure with randomness: probabilistic algorithms for
  constructing approximate matrix decompositions,
  arXiv:0909.4061 [math.NA; math.PR], 2009 (available at
  `arXiv &lt;http://arxiv.org/abs/0909.4061&gt;`_).
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.pca_lowrank.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.pca_lowrank.html</a>
  </p>
</div>
