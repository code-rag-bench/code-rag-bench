<h1 id="torch-autograd-functional-jvp">torch.autograd.functional.jvp</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.functional.jvp">
<code>torch.autograd.functional.jvp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/functional.html#jvp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that computes the dot product between the Jacobian of the given function at the point given by the inputs and a vector <code>v</code>.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a tuple of Tensors or a Tensor.</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>
<strong>v</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – The vector for which the Jacobian vector product is computed. Must be the same size as the input of <code>func</code>. This argument is optional when the input to <code>func</code> contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>
<strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the jvp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<dl> <dt>tuple with:</dt>
<dd>
<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>jvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the output.</p> </dd> </dl> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>output (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>)</p> </dd> </dl> <div class="admonition note"> <p class="admonition-title">Note</p> <p><code>autograd.functional.jvp</code> computes the jvp by using the backward of the backward (sometimes called the double backwards trick). This is not the most performant way of computing the jvp. Please consider using <a class="reference internal" href="torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>torch.func.jvp()</code></a> or the <a class="reference internal" href="../autograd#forward-mode-ad"><span class="std std-ref">low-level forward-mode AD API</span></a> instead.</p> </div> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; def exp_reducer(x):
...     return x.exp().sum(dim=1)
&gt;&gt;&gt; inputs = torch.rand(4, 4)
&gt;&gt;&gt; v = torch.ones(4, 4)
&gt;&gt;&gt; jvp(exp_reducer, inputs, v)
(tensor([6.3090, 4.6742, 7.9114, 8.2106]),
 tensor([6.3090, 4.6742, 7.9114, 8.2106]))
</pre> <pre data-language="python">&gt;&gt;&gt; jvp(exp_reducer, inputs, v, create_graph=True)
(tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SumBackward1&gt;),
 tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=&lt;SqueezeBackward1&gt;))
</pre> <pre data-language="python">&gt;&gt;&gt; def adder(x, y):
...     return 2 * x + 3 * y
&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))
&gt;&gt;&gt; v = (torch.ones(2), torch.ones(2))
&gt;&gt;&gt; jvp(adder, inputs, v)
(tensor([2.2399, 2.5005]),
 tensor([5., 5.]))
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.functional.jvp.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.functional.jvp.html</a>
  </p>
</div>
