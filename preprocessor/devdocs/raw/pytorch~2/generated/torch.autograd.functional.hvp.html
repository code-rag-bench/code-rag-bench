<h1 id="torch-autograd-functional-hvp">torch.autograd.functional.hvp</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.autograd.functional.hvp">
<code>torch.autograd.functional.hvp(func, inputs, v=None, create_graph=False, strict=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/autograd/functional.html#hvp"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that computes the dot product between the Hessian of a given scalar function and a vector <code>v</code> at the point given by the inputs.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>func</strong> (<em>function</em>) – a Python function that takes Tensor inputs and returns a Tensor with a single element.</li> <li>
<strong>inputs</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – inputs to the function <code>func</code>.</li> <li>
<strong>v</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a><em> of </em><em>Tensors</em><em> or </em><a class="reference internal" href="../tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – The vector for which the Hessian vector product is computed. Must be the same size as the input of <code>func</code>. This argument is optional when <code>func</code>’s input contains a single element and (if it is not provided) will be set as a Tensor containing a single <code>1</code>.</li> <li>
<strong>create_graph</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, both the output and result will be computed in a differentiable way. Note that when <code>strict</code> is <code>False</code>, the result can not require gradients or be disconnected from the inputs. Defaults to <code>False</code>.</li> <li>
<strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, an error will be raised when we detect that there exists an input such that all the outputs are independent of it. If <code>False</code>, we return a Tensor of zeros as the hvp for said inputs, which is the expected mathematical value. Defaults to <code>False</code>.</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">

<dl> <dt>tuple with:</dt>
<dd>
<p>func_output (tuple of Tensors or Tensor): output of <code>func(inputs)</code></p> <p>hvp (tuple of Tensors or Tensor): result of the dot product with the same shape as the inputs.</p> </dd> </dl> </dd> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p>output (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.12)">tuple</a>)</p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; def pow_reducer(x):
...     return x.pow(3).sum()
&gt;&gt;&gt; inputs = torch.rand(2, 2)
&gt;&gt;&gt; v = torch.ones(2, 2)
&gt;&gt;&gt; hvp(pow_reducer, inputs, v)
(tensor(0.1448),
 tensor([[2.0239, 1.6456],
         [2.4988, 1.4310]]))
</pre> <pre data-language="python">&gt;&gt;&gt; hvp(pow_reducer, inputs, v, create_graph=True)
(tensor(0.1448, grad_fn=&lt;SumBackward0&gt;),
 tensor([[2.0239, 1.6456],
         [2.4988, 1.4310]], grad_fn=&lt;MulBackward0&gt;))
</pre> <pre data-language="python">&gt;&gt;&gt; def pow_adder_reducer(x, y):
...     return (2 * x.pow(2) + 3 * y.pow(2)).sum()
&gt;&gt;&gt; inputs = (torch.rand(2), torch.rand(2))
&gt;&gt;&gt; v = (torch.zeros(2), torch.ones(2))
&gt;&gt;&gt; hvp(pow_adder_reducer, inputs, v)
(tensor(2.3030),
 (tensor([0., 0.]),
  tensor([6., 6.])))
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>This function is significantly slower than <code>vhp</code> due to backward mode AD constraints. If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you know that your function satisfies this condition, you should use vhp instead that is much faster with the current implementation.</p> </div> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.autograd.functional.hvp.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.autograd.functional.hvp.html</a>
  </p>
</div>
