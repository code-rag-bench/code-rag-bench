<h1 id="torch-jit-load">torch.jit.load</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.jit.load">
<code>torch.jit.load(f, map_location=None, _extra_files=None, _restore_shapes=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/jit/_serialization.html#load"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Load a <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a> or <a class="reference internal" href="torch.jit.scriptfunction#torch.jit.ScriptFunction" title="torch.jit.ScriptFunction"><code>ScriptFunction</code></a> previously saved with <a class="reference internal" href="torch.jit.save#torch.jit.save" title="torch.jit.save"><code>torch.jit.save</code></a></p> <p>All previously saved modules, no matter their device, are first loaded onto CPU, and then are moved to the devices they were saved from. If this fails (e.g. because the run time system doesn’t have certain devices), an exception is raised.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>f</strong> – a file-like object (has to implement read, readline, tell, and seek), or a string containing a file name</li> <li>
<strong>map_location</strong> (<em>string</em><em> or </em><a class="reference internal" href="../tensor_attributes#torch.device" title="torch.device">torch.device</a>) – A simplified version of <code>map_location</code> in <code>torch.jit.save</code> used to dynamically remap storages to an alternative set of devices.</li> <li>
<strong>_extra_files</strong> (<em>dictionary</em><em> of </em><em>filename to content</em>) – The extra filenames given in the map would be loaded and their content would be stored in the provided map.</li> <li>
<strong>_restore_shapes</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – Whether or not to retrace the module on load using stored inputs</li> </ul> </dd> <dt class="field-even">Returns</dt> <dd class="field-even">
<p>A <a class="reference internal" href="torch.jit.scriptmodule#torch.jit.ScriptModule" title="torch.jit.ScriptModule"><code>ScriptModule</code></a> object.</p> </dd> </dl> <p>Example:</p> <pre data-language="python">import torch
import io

torch.jit.load('scriptmodule.pt')

# Load ScriptModule from io.BytesIO object
with open('scriptmodule.pt', 'rb') as f:
    buffer = io.BytesIO(f.read())

# Load all tensors to the original device
torch.jit.load(buffer)

# Load all tensors onto CPU, using a device
buffer.seek(0)
torch.jit.load(buffer, map_location=torch.device('cpu'))

# Load all tensors onto CPU, using a string
buffer.seek(0)
torch.jit.load(buffer, map_location='cpu')

# Load with extra files.
extra_files = {'foo.txt': ''}  # values will be replaced with data
torch.jit.load('scriptmodule.pt', _extra_files=extra_files)
print(extra_files['foo.txt'])
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/generated/torch.jit.load.html" class="_attribution-link">https://pytorch.org/docs/2.1/generated/torch.jit.load.html</a>
  </p>
</div>
