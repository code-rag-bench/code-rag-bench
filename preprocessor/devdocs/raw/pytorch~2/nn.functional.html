<h1 id="torch-nn-functional">torch.nn.functional</h1>  <h2 id="convolution-functions">Convolution functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv1d#torch.nn.functional.conv1d" title="torch.nn.functional.conv1d"><code>conv1d</code></a>
</td> <td><p>Applies a 1D convolution over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv2d#torch.nn.functional.conv2d" title="torch.nn.functional.conv2d"><code>conv2d</code></a>
</td> <td><p>Applies a 2D convolution over an input image composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv3d#torch.nn.functional.conv3d" title="torch.nn.functional.conv3d"><code>conv3d</code></a>
</td> <td><p>Applies a 3D convolution over an input image composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv_transpose1d#torch.nn.functional.conv_transpose1d" title="torch.nn.functional.conv_transpose1d"><code>conv_transpose1d</code></a>
</td> <td><p>Applies a 1D transposed convolution operator over an input signal composed of several input planes, sometimes also called "deconvolution".</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv_transpose2d#torch.nn.functional.conv_transpose2d" title="torch.nn.functional.conv_transpose2d"><code>conv_transpose2d</code></a>
</td> <td><p>Applies a 2D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution".</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.conv_transpose3d#torch.nn.functional.conv_transpose3d" title="torch.nn.functional.conv_transpose3d"><code>conv_transpose3d</code></a>
</td> <td><p>Applies a 3D transposed convolution operator over an input image composed of several input planes, sometimes also called "deconvolution"</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.unfold#torch.nn.functional.unfold" title="torch.nn.functional.unfold"><code>unfold</code></a>
</td> <td><p>Extracts sliding local blocks from a batched input tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.fold#torch.nn.functional.fold" title="torch.nn.functional.fold"><code>fold</code></a>
</td> <td><p>Combines an array of sliding local blocks into a large containing tensor.</p></td> </tr>  </table>   <h2 id="pooling-functions">Pooling functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.avg_pool1d#torch.nn.functional.avg_pool1d" title="torch.nn.functional.avg_pool1d"><code>avg_pool1d</code></a>
</td> <td><p>Applies a 1D average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.avg_pool2d#torch.nn.functional.avg_pool2d" title="torch.nn.functional.avg_pool2d"><code>avg_pool2d</code></a>
</td> <td><p>Applies 2D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kH \times kW</annotation></semantics></math></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sH \times sW</annotation></semantics></math></span></span></span> steps.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.avg_pool3d#torch.nn.functional.avg_pool3d" title="torch.nn.functional.avg_pool3d"><code>avg_pool3d</code></a>
</td> <td><p>Applies 3D average-pooling operation in <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mi>T</mi><mo>×</mo><mi>k</mi><mi>H</mi><mo>×</mo><mi>k</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">kT \times kH \times kW</annotation></semantics></math></span></span></span> regions by step size <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>T</mi><mo>×</mo><mi>s</mi><mi>H</mi><mo>×</mo><mi>s</mi><mi>W</mi></mrow><annotation encoding="application/x-tex">sT \times sH \times sW</annotation></semantics></math></span></span></span> steps.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_pool1d#torch.nn.functional.max_pool1d" title="torch.nn.functional.max_pool1d"><code>max_pool1d</code></a>
</td> <td><p>Applies a 1D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_pool2d#torch.nn.functional.max_pool2d" title="torch.nn.functional.max_pool2d"><code>max_pool2d</code></a>
</td> <td><p>Applies a 2D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_pool3d#torch.nn.functional.max_pool3d" title="torch.nn.functional.max_pool3d"><code>max_pool3d</code></a>
</td> <td><p>Applies a 3D max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_unpool1d#torch.nn.functional.max_unpool1d" title="torch.nn.functional.max_unpool1d"><code>max_unpool1d</code></a>
</td> <td><p>Computes a partial inverse of <code>MaxPool1d</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_unpool2d#torch.nn.functional.max_unpool2d" title="torch.nn.functional.max_unpool2d"><code>max_unpool2d</code></a>
</td> <td><p>Computes a partial inverse of <code>MaxPool2d</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.max_unpool3d#torch.nn.functional.max_unpool3d" title="torch.nn.functional.max_unpool3d"><code>max_unpool3d</code></a>
</td> <td><p>Computes a partial inverse of <code>MaxPool3d</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.lp_pool1d#torch.nn.functional.lp_pool1d" title="torch.nn.functional.lp_pool1d"><code>lp_pool1d</code></a>
</td> <td><p>Applies a 1D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.lp_pool2d#torch.nn.functional.lp_pool2d" title="torch.nn.functional.lp_pool2d"><code>lp_pool2d</code></a>
</td> <td><p>Applies a 2D power-average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool1d#torch.nn.functional.adaptive_max_pool1d" title="torch.nn.functional.adaptive_max_pool1d"><code>adaptive_max_pool1d</code></a>
</td> <td><p>Applies a 1D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool2d#torch.nn.functional.adaptive_max_pool2d" title="torch.nn.functional.adaptive_max_pool2d"><code>adaptive_max_pool2d</code></a>
</td> <td><p>Applies a 2D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_max_pool3d#torch.nn.functional.adaptive_max_pool3d" title="torch.nn.functional.adaptive_max_pool3d"><code>adaptive_max_pool3d</code></a>
</td> <td><p>Applies a 3D adaptive max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool1d#torch.nn.functional.adaptive_avg_pool1d" title="torch.nn.functional.adaptive_avg_pool1d"><code>adaptive_avg_pool1d</code></a>
</td> <td><p>Applies a 1D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool2d#torch.nn.functional.adaptive_avg_pool2d" title="torch.nn.functional.adaptive_avg_pool2d"><code>adaptive_avg_pool2d</code></a>
</td> <td><p>Applies a 2D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.adaptive_avg_pool3d#torch.nn.functional.adaptive_avg_pool3d" title="torch.nn.functional.adaptive_avg_pool3d"><code>adaptive_avg_pool3d</code></a>
</td> <td><p>Applies a 3D adaptive average pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool2d#torch.nn.functional.fractional_max_pool2d" title="torch.nn.functional.fractional_max_pool2d"><code>fractional_max_pool2d</code></a>
</td> <td><p>Applies 2D fractional max pooling over an input signal composed of several input planes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.fractional_max_pool3d#torch.nn.functional.fractional_max_pool3d" title="torch.nn.functional.fractional_max_pool3d"><code>fractional_max_pool3d</code></a>
</td> <td><p>Applies 3D fractional max pooling over an input signal composed of several input planes.</p></td> </tr>  </table>   <h2 id="attention-mechanisms">Attention Mechanisms</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.scaled_dot_product_attention#torch.nn.functional.scaled_dot_product_attention" title="torch.nn.functional.scaled_dot_product_attention"><code>scaled_dot_product_attention</code></a>
</td> <td><p>Computes scaled dot product attention on query, key and value tensors, using an optional attention mask if passed, and applying dropout if a probability greater than 0.0 is specified.</p></td> </tr>  </table>   <h2 id="non-linear-activation-functions">Non-linear activation functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.threshold#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code>threshold</code></a>
</td> <td><p>Thresholds each element of the input Tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.threshold_#torch.nn.functional.threshold_" title="torch.nn.functional.threshold_"><code>threshold_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.threshold#torch.nn.functional.threshold" title="torch.nn.functional.threshold"><code>threshold()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.relu#torch.nn.functional.relu" title="torch.nn.functional.relu"><code>relu</code></a>
</td> <td><p>Applies the rectified linear unit function element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.relu_#torch.nn.functional.relu_" title="torch.nn.functional.relu_"><code>relu_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.relu#torch.nn.functional.relu" title="torch.nn.functional.relu"><code>relu()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hardtanh#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code>hardtanh</code></a>
</td> <td><p>Applies the HardTanh function element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hardtanh_#torch.nn.functional.hardtanh_" title="torch.nn.functional.hardtanh_"><code>hardtanh_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.hardtanh#torch.nn.functional.hardtanh" title="torch.nn.functional.hardtanh"><code>hardtanh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hardswish#torch.nn.functional.hardswish" title="torch.nn.functional.hardswish"><code>hardswish</code></a>
</td> <td><p>Applies the hardswish function, element-wise, as described in the paper:</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.relu6#torch.nn.functional.relu6" title="torch.nn.functional.relu6"><code>relu6</code></a>
</td> <td><p>Applies the element-wise function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU6</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><mn>6</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{ReLU6}(x) = \min(\max(0,x), 6)</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.elu#torch.nn.functional.elu" title="torch.nn.functional.elu"><code>elu</code></a>
</td> <td><p>Applies the Exponential Linear Unit (ELU) function element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.elu_#torch.nn.functional.elu_" title="torch.nn.functional.elu_"><code>elu_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.elu#torch.nn.functional.elu" title="torch.nn.functional.elu"><code>elu()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.selu#torch.nn.functional.selu" title="torch.nn.functional.selu"><code>selu</code></a>
</td> <td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{SELU}(x) = scale * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))</annotation></semantics></math></span></span></span>, with <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1.6732632423543772848170429916717</mn></mrow><annotation encoding="application/x-tex">\alpha=1.6732632423543772848170429916717</annotation></semantics></math></span></span></span> and <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi><mi>c</mi><mi>a</mi><mi>l</mi><mi>e</mi><mo>=</mo><mn>1.0507009873554804934193349852946</mn></mrow><annotation encoding="application/x-tex">scale=1.0507009873554804934193349852946</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.celu#torch.nn.functional.celu" title="torch.nn.functional.celu"><code>celu</code></a>
</td> <td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>CELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>α</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">/</mi><mi>α</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.leaky_relu#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code>leaky_relu</code></a>
</td> <td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LeakyReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>negative_slope</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.leaky_relu_#torch.nn.functional.leaky_relu_" title="torch.nn.functional.leaky_relu_"><code>leaky_relu_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.leaky_relu#torch.nn.functional.leaky_relu" title="torch.nn.functional.leaky_relu"><code>leaky_relu()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.prelu#torch.nn.functional.prelu" title="torch.nn.functional.prelu"><code>prelu</code></a>
</td> <td><p>Applies element-wise the function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>PReLU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mtext>weight</mtext><mo>∗</mo><mi>min</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{PReLU}(x) = \max(0,x) + \text{weight} * \min(0,x)</annotation></semantics></math></span></span></span> where weight is a learnable parameter.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.rrelu#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code>rrelu</code></a>
</td> <td><p>Randomized leaky ReLU.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.rrelu_#torch.nn.functional.rrelu_" title="torch.nn.functional.rrelu_"><code>rrelu_</code></a>
</td> <td><p>In-place version of <a class="reference internal" href="generated/torch.nn.functional.rrelu#torch.nn.functional.rrelu" title="torch.nn.functional.rrelu"><code>rrelu()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.glu#torch.nn.functional.glu" title="torch.nn.functional.glu"><code>glu</code></a>
</td> <td><p>The gated linear unit.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.gelu#torch.nn.functional.gelu" title="torch.nn.functional.gelu"><code>gelu</code></a>
</td> <td><p>When the approximate argument is 'none', it applies element-wise the function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>GELU</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>∗</mo><mi mathvariant="normal">Φ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{GELU}(x) = x * \Phi(x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.logsigmoid#torch.nn.functional.logsigmoid" title="torch.nn.functional.logsigmoid"><code>logsigmoid</code></a>
</td> <td><p>Applies element-wise <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>LogSigmoid</mtext><mo stretchy="false">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><msub><mi>x</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow></mfrac><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{LogSigmoid}(x_i) = \log \left(\frac{1}{1 + \exp(-x_i)}\right)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hardshrink#torch.nn.functional.hardshrink" title="torch.nn.functional.hardshrink"><code>hardshrink</code></a>
</td> <td><p>Applies the hard shrinkage function element-wise</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.tanhshrink#torch.nn.functional.tanhshrink" title="torch.nn.functional.tanhshrink"><code>tanhshrink</code></a>
</td> <td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Tanhshrink</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>x</mi><mo>−</mo><mtext>Tanh</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Tanhshrink}(x) = x - \text{Tanh}(x)</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.softsign#torch.nn.functional.softsign" title="torch.nn.functional.softsign"><code>softsign</code></a>
</td> <td><p>Applies element-wise, the function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>SoftSign</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mi>x</mi><mrow><mn>1</mn><mo>+</mo><mi mathvariant="normal">∣</mi><mi>x</mi><mi mathvariant="normal">∣</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{SoftSign}(x) = \frac{x}{1 + |x|}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.softplus#torch.nn.functional.softplus" title="torch.nn.functional.softplus"><code>softplus</code></a>
</td> <td><p>Applies element-wise, the function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Softplus</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mi>β</mi></mfrac><mo>∗</mo><mi>log</mi><mo>⁡</mo><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>β</mi><mo>∗</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.softmin#torch.nn.functional.softmin" title="torch.nn.functional.softmin"><code>softmin</code></a>
</td> <td><p>Applies a softmin function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.softmax#torch.nn.functional.softmax" title="torch.nn.functional.softmax"><code>softmax</code></a>
</td> <td><p>Applies a softmax function.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.softshrink#torch.nn.functional.softshrink" title="torch.nn.functional.softshrink"><code>softshrink</code></a>
</td> <td><p>Applies the soft shrinkage function elementwise</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.gumbel_softmax#torch.nn.functional.gumbel_softmax" title="torch.nn.functional.gumbel_softmax"><code>gumbel_softmax</code></a>
</td> <td><p>Samples from the Gumbel-Softmax distribution (<a class="reference external" href="https://arxiv.org/abs/1611.00712">Link 1</a> <a class="reference external" href="https://arxiv.org/abs/1611.01144">Link 2</a>) and optionally discretizes.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.log_softmax#torch.nn.functional.log_softmax" title="torch.nn.functional.log_softmax"><code>log_softmax</code></a>
</td> <td><p>Applies a softmax followed by a logarithm.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.tanh#torch.nn.functional.tanh" title="torch.nn.functional.tanh"><code>tanh</code></a>
</td> <td><p>Applies element-wise, <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Tanh</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.sigmoid#torch.nn.functional.sigmoid" title="torch.nn.functional.sigmoid"><code>sigmoid</code></a>
</td> <td><p>Applies the element-wise function <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>Sigmoid</mtext><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mn>1</mn><mrow><mn>1</mn><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy="false">(</mo><mo>−</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\text{Sigmoid}(x) = \frac{1}{1 + \exp(-x)}</annotation></semantics></math></span></span></span></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hardsigmoid#torch.nn.functional.hardsigmoid" title="torch.nn.functional.hardsigmoid"><code>hardsigmoid</code></a>
</td> <td><p>Applies the element-wise function</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.silu#torch.nn.functional.silu" title="torch.nn.functional.silu"><code>silu</code></a>
</td> <td><p>Applies the Sigmoid Linear Unit (SiLU) function, element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.mish#torch.nn.functional.mish" title="torch.nn.functional.mish"><code>mish</code></a>
</td> <td><p>Applies the Mish function, element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.batch_norm#torch.nn.functional.batch_norm" title="torch.nn.functional.batch_norm"><code>batch_norm</code></a>
</td> <td><p>Applies Batch Normalization for each channel across a batch of data.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.group_norm#torch.nn.functional.group_norm" title="torch.nn.functional.group_norm"><code>group_norm</code></a>
</td> <td><p>Applies Group Normalization for last certain number of dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.instance_norm#torch.nn.functional.instance_norm" title="torch.nn.functional.instance_norm"><code>instance_norm</code></a>
</td> <td><p>Applies Instance Normalization for each channel in each data sample in a batch.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.layer_norm#torch.nn.functional.layer_norm" title="torch.nn.functional.layer_norm"><code>layer_norm</code></a>
</td> <td><p>Applies Layer Normalization for last certain number of dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.local_response_norm#torch.nn.functional.local_response_norm" title="torch.nn.functional.local_response_norm"><code>local_response_norm</code></a>
</td> <td><p>Applies local response normalization over an input signal composed of several input planes, where channels occupy the second dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.normalize#torch.nn.functional.normalize" title="torch.nn.functional.normalize"><code>normalize</code></a>
</td> <td><p>Performs <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mi>p</mi></msub></mrow><annotation encoding="application/x-tex">L_p</annotation></semantics></math></span></span></span> normalization of inputs over specified dimension.</p></td> </tr>  </table>   <h2 id="linear-functions">Linear functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.linear#torch.nn.functional.linear" title="torch.nn.functional.linear"><code>linear</code></a>
</td> <td><p>Applies a linear transformation to the incoming data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>x</mi><msup><mi>A</mi><mi>T</mi></msup><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = xA^T + b</annotation></semantics></math></span></span></span>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.bilinear#torch.nn.functional.bilinear" title="torch.nn.functional.bilinear"><code>bilinear</code></a>
</td> <td><p>Applies a bilinear transformation to the incoming data: <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msubsup><mi>x</mi><mn>1</mn><mi>T</mi></msubsup><mi>A</mi><msub><mi>x</mi><mn>2</mn></msub><mo>+</mo><mi>b</mi></mrow><annotation encoding="application/x-tex">y = x_1^T A x_2 + b</annotation></semantics></math></span></span></span></p></td> </tr>  </table>   <h2 id="dropout-functions">Dropout functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.dropout#torch.nn.functional.dropout" title="torch.nn.functional.dropout"><code>dropout</code></a>
</td> <td><p>During training, randomly zeroes some of the elements of the input tensor with probability <code>p</code> using samples from a Bernoulli distribution.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.alpha_dropout#torch.nn.functional.alpha_dropout" title="torch.nn.functional.alpha_dropout"><code>alpha_dropout</code></a>
</td> <td><p>Applies alpha dropout to the input.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.feature_alpha_dropout#torch.nn.functional.feature_alpha_dropout" title="torch.nn.functional.feature_alpha_dropout"><code>feature_alpha_dropout</code></a>
</td> <td><p>Randomly masks out entire channels (a channel is a feature map, e.g.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.dropout1d#torch.nn.functional.dropout1d" title="torch.nn.functional.dropout1d"><code>dropout1d</code></a>
</td> <td><p>Randomly zero out entire channels (a channel is a 1D feature map, e.g., the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span></span></span>-th channel of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 1D tensor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.dropout2d#torch.nn.functional.dropout2d" title="torch.nn.functional.dropout2d"><code>dropout2d</code></a>
</td> <td><p>Randomly zero out entire channels (a channel is a 2D feature map, e.g., the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span></span></span>-th channel of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 2D tensor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.dropout3d#torch.nn.functional.dropout3d" title="torch.nn.functional.dropout3d"><code>dropout3d</code></a>
</td> <td><p>Randomly zero out entire channels (a channel is a 3D feature map, e.g., the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span></span></span>-th channel of the <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span></span></span>-th sample in the batched input is a 3D tensor <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo stretchy="false">[</mo><mi>i</mi><mo separator="true">,</mo><mi>j</mi><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">\text{input}[i, j]</annotation></semantics></math></span></span></span>) of the input tensor).</p></td> </tr>  </table>   <h2 id="sparse-functions">Sparse functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.embedding#torch.nn.functional.embedding" title="torch.nn.functional.embedding"><code>embedding</code></a>
</td> <td><p>A simple lookup table that looks up embeddings in a fixed dictionary and size.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.embedding_bag#torch.nn.functional.embedding_bag" title="torch.nn.functional.embedding_bag"><code>embedding_bag</code></a>
</td> <td><p>Computes sums, means or maxes of <code>bags</code> of embeddings, without instantiating the intermediate embeddings.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.one_hot#torch.nn.functional.one_hot" title="torch.nn.functional.one_hot"><code>one_hot</code></a>
</td> <td><p>Takes LongTensor with index values of shape <code>(*)</code> and returns a tensor of shape <code>(*, num_classes)</code> that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1.</p></td> </tr>  </table>   <h2 id="distance-functions">Distance functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.pairwise_distance#torch.nn.functional.pairwise_distance" title="torch.nn.functional.pairwise_distance"><code>pairwise_distance</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.pairwisedistance#torch.nn.PairwiseDistance" title="torch.nn.PairwiseDistance"><code>torch.nn.PairwiseDistance</code></a> for details</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.cosine_similarity#torch.nn.functional.cosine_similarity" title="torch.nn.functional.cosine_similarity"><code>cosine_similarity</code></a>
</td> <td><p>Returns cosine similarity between <code>x1</code> and <code>x2</code>, computed along dim.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.pdist#torch.nn.functional.pdist" title="torch.nn.functional.pdist"><code>pdist</code></a>
</td> <td><p>Computes the p-norm distance between every pair of row vectors in the input.</p></td> </tr>  </table>   <h2 id="loss-functions">Loss functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy#torch.nn.functional.binary_cross_entropy" title="torch.nn.functional.binary_cross_entropy"><code>binary_cross_entropy</code></a>
</td> <td><p>Function that measures the Binary Cross Entropy between the target and input probabilities.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.binary_cross_entropy_with_logits#torch.nn.functional.binary_cross_entropy_with_logits" title="torch.nn.functional.binary_cross_entropy_with_logits"><code>binary_cross_entropy_with_logits</code></a>
</td> <td><p>Function that measures Binary Cross Entropy between target and input logits.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.poisson_nll_loss#torch.nn.functional.poisson_nll_loss" title="torch.nn.functional.poisson_nll_loss"><code>poisson_nll_loss</code></a>
</td> <td><p>Poisson negative log likelihood loss.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.cosine_embedding_loss#torch.nn.functional.cosine_embedding_loss" title="torch.nn.functional.cosine_embedding_loss"><code>cosine_embedding_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.cosineembeddingloss#torch.nn.CosineEmbeddingLoss" title="torch.nn.CosineEmbeddingLoss"><code>CosineEmbeddingLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.cross_entropy#torch.nn.functional.cross_entropy" title="torch.nn.functional.cross_entropy"><code>cross_entropy</code></a>
</td> <td><p>This criterion computes the cross entropy loss between input logits and target.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.ctc_loss#torch.nn.functional.ctc_loss" title="torch.nn.functional.ctc_loss"><code>ctc_loss</code></a>
</td> <td><p>The Connectionist Temporal Classification loss.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.gaussian_nll_loss#torch.nn.functional.gaussian_nll_loss" title="torch.nn.functional.gaussian_nll_loss"><code>gaussian_nll_loss</code></a>
</td> <td><p>Gaussian negative log likelihood loss.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.hinge_embedding_loss#torch.nn.functional.hinge_embedding_loss" title="torch.nn.functional.hinge_embedding_loss"><code>hinge_embedding_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.hingeembeddingloss#torch.nn.HingeEmbeddingLoss" title="torch.nn.HingeEmbeddingLoss"><code>HingeEmbeddingLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.kl_div#torch.nn.functional.kl_div" title="torch.nn.functional.kl_div"><code>kl_div</code></a>
</td> <td><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Kullback-Leibler_divergence">Kullback-Leibler divergence Loss</a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.l1_loss#torch.nn.functional.l1_loss" title="torch.nn.functional.l1_loss"><code>l1_loss</code></a>
</td> <td><p>Function that takes the mean element-wise absolute value difference.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.mse_loss#torch.nn.functional.mse_loss" title="torch.nn.functional.mse_loss"><code>mse_loss</code></a>
</td> <td><p>Measures the element-wise mean squared error.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.margin_ranking_loss#torch.nn.functional.margin_ranking_loss" title="torch.nn.functional.margin_ranking_loss"><code>margin_ranking_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.marginrankingloss#torch.nn.MarginRankingLoss" title="torch.nn.MarginRankingLoss"><code>MarginRankingLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.multilabel_margin_loss#torch.nn.functional.multilabel_margin_loss" title="torch.nn.functional.multilabel_margin_loss"><code>multilabel_margin_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.multilabelmarginloss#torch.nn.MultiLabelMarginLoss" title="torch.nn.MultiLabelMarginLoss"><code>MultiLabelMarginLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.multilabel_soft_margin_loss#torch.nn.functional.multilabel_soft_margin_loss" title="torch.nn.functional.multilabel_soft_margin_loss"><code>multilabel_soft_margin_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.multilabelsoftmarginloss#torch.nn.MultiLabelSoftMarginLoss" title="torch.nn.MultiLabelSoftMarginLoss"><code>MultiLabelSoftMarginLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.multi_margin_loss#torch.nn.functional.multi_margin_loss" title="torch.nn.functional.multi_margin_loss"><code>multi_margin_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.multimarginloss#torch.nn.MultiMarginLoss" title="torch.nn.MultiMarginLoss"><code>MultiMarginLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.nll_loss#torch.nn.functional.nll_loss" title="torch.nn.functional.nll_loss"><code>nll_loss</code></a>
</td> <td><p>The negative log likelihood loss.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.huber_loss#torch.nn.functional.huber_loss" title="torch.nn.functional.huber_loss"><code>huber_loss</code></a>
</td> <td><p>Function that uses a squared term if the absolute element-wise error falls below delta and a delta-scaled L1 term otherwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.smooth_l1_loss#torch.nn.functional.smooth_l1_loss" title="torch.nn.functional.smooth_l1_loss"><code>smooth_l1_loss</code></a>
</td> <td><p>Function that uses a squared term if the absolute element-wise error falls below beta and an L1 term otherwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.soft_margin_loss#torch.nn.functional.soft_margin_loss" title="torch.nn.functional.soft_margin_loss"><code>soft_margin_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.softmarginloss#torch.nn.SoftMarginLoss" title="torch.nn.SoftMarginLoss"><code>SoftMarginLoss</code></a> for details.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.triplet_margin_loss#torch.nn.functional.triplet_margin_loss" title="torch.nn.functional.triplet_margin_loss"><code>triplet_margin_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.tripletmarginloss#torch.nn.TripletMarginLoss" title="torch.nn.TripletMarginLoss"><code>TripletMarginLoss</code></a> for details</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.triplet_margin_with_distance_loss#torch.nn.functional.triplet_margin_with_distance_loss" title="torch.nn.functional.triplet_margin_with_distance_loss"><code>triplet_margin_with_distance_loss</code></a>
</td> <td><p>See <a class="reference internal" href="generated/torch.nn.tripletmarginwithdistanceloss#torch.nn.TripletMarginWithDistanceLoss" title="torch.nn.TripletMarginWithDistanceLoss"><code>TripletMarginWithDistanceLoss</code></a> for details.</p></td> </tr>  </table>   <h2 id="vision-functions">Vision functions</h2> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.pixel_shuffle#torch.nn.functional.pixel_shuffle" title="torch.nn.functional.pixel_shuffle"><code>pixel_shuffle</code></a>
</td> <td><p>Rearranges elements in a tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math></span></span></span> to a tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C, H \times r, W \times r)</annotation></semantics></math></span></span></span>, where r is the <code>upscale_factor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.pixel_unshuffle#torch.nn.functional.pixel_unshuffle" title="torch.nn.functional.pixel_unshuffle"><code>pixel_unshuffle</code></a>
</td> <td><p>Reverses the <a class="reference internal" href="generated/torch.nn.pixelshuffle#torch.nn.PixelShuffle" title="torch.nn.PixelShuffle"><code>PixelShuffle</code></a> operation by rearranging elements in a tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo separator="true">,</mo><mi>H</mi><mo>×</mo><mi>r</mi><mo separator="true">,</mo><mi>W</mi><mo>×</mo><mi>r</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C, H \times r, W \times r)</annotation></semantics></math></span></span></span> to a tensor of shape <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mo>∗</mo><mo separator="true">,</mo><mi>C</mi><mo>×</mo><msup><mi>r</mi><mn>2</mn></msup><mo separator="true">,</mo><mi>H</mi><mo separator="true">,</mo><mi>W</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(*, C \times r^2, H, W)</annotation></semantics></math></span></span></span>, where r is the <code>downscale_factor</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.pad#torch.nn.functional.pad" title="torch.nn.functional.pad"><code>pad</code></a>
</td> <td><p>Pads tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.interpolate#torch.nn.functional.interpolate" title="torch.nn.functional.interpolate"><code>interpolate</code></a>
</td> <td><p>Down/up samples the input to either the given <code>size</code> or the given <code>scale_factor</code></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.upsample#torch.nn.functional.upsample" title="torch.nn.functional.upsample"><code>upsample</code></a>
</td> <td><p>Upsamples the input to either the given <code>size</code> or the given <code>scale_factor</code></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.upsample_nearest#torch.nn.functional.upsample_nearest" title="torch.nn.functional.upsample_nearest"><code>upsample_nearest</code></a>
</td> <td><p>Upsamples the input, using nearest neighbours' pixel values.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.upsample_bilinear#torch.nn.functional.upsample_bilinear" title="torch.nn.functional.upsample_bilinear"><code>upsample_bilinear</code></a>
</td> <td><p>Upsamples the input, using bilinear upsampling.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.grid_sample#torch.nn.functional.grid_sample" title="torch.nn.functional.grid_sample"><code>grid_sample</code></a>
</td> <td><p>Given an <code>input</code> and a flow-field <code>grid</code>, computes the <code>output</code> using <code>input</code> values and pixel locations from <code>grid</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nn.functional.affine_grid#torch.nn.functional.affine_grid" title="torch.nn.functional.affine_grid"><code>affine_grid</code></a>
</td> <td><p>Generates a 2D or 3D flow field (sampling grid), given a batch of affine matrices <code>theta</code>.</p></td> </tr>  </table>   <h2 id="dataparallel-functions-multi-gpu-distributed">DataParallel functions (multi-GPU, distributed)</h2>  <h3 id="data-parallel"><span class="hidden-section">data_parallel</span></h3> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td><p><code>torch.nn.parallel.data_parallel</code></p></td> <td><p>Evaluates module(input) in parallel across the GPUs given in device_ids.</p></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/nn.functional.html" class="_attribution-link">https://pytorch.org/docs/2.1/nn.functional.html</a>
  </p>
</div>
