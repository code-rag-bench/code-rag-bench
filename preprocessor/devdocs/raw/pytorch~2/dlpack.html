<h1 id="torch-utils-dlpack">torch.utils.dlpack</h1> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.dlpack.from_dlpack">
<code>torch.utils.dlpack.from_dlpack(ext_tensor) → Tensor</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/dlpack.html#from_dlpack"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Converts a tensor from an external library into a <code>torch.Tensor</code>.</p> <p>The returned PyTorch tensor will share the memory with the input tensor (which may have come from another library). Note that in-place operations will therefore also affect the data of the input tensor. This may lead to unexpected issues (e.g., other libraries may have read-only flags or immutable data structures), so the user should only do this if they know for sure that this is fine.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>ext_tensor</strong> (object with <code>__dlpack__</code> attribute, or a DLPack capsule) – </p>
<p>The tensor or DLPack capsule to convert.</p> <p>If <code>ext_tensor</code> is a tensor (or ndarray) object, it must support the <code>__dlpack__</code> protocol (i.e., have a <code>ext_tensor.__dlpack__</code> method). Otherwise <code>ext_tensor</code> may be a DLPack capsule, which is an opaque <code>PyCapsule</code> instance, typically produced by a <code>to_dlpack</code> function or method.</p> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a></p> </dd> </dl> <p>Examples:</p> <pre data-language="python">&gt;&gt;&gt; import torch.utils.dlpack
&gt;&gt;&gt; t = torch.arange(4)

# Convert a tensor directly (supported in PyTorch &gt;= 1.10)
&gt;&gt;&gt; t2 = torch.from_dlpack(t)
&gt;&gt;&gt; t2[:2] = -1  # show that memory is shared
&gt;&gt;&gt; t2
tensor([-1, -1,  2,  3])
&gt;&gt;&gt; t
tensor([-1, -1,  2,  3])

# The old-style DLPack usage, with an intermediate capsule object
&gt;&gt;&gt; capsule = torch.utils.dlpack.to_dlpack(t)
&gt;&gt;&gt; capsule
&lt;capsule object "dltensor" at ...&gt;
&gt;&gt;&gt; t3 = torch.from_dlpack(capsule)
&gt;&gt;&gt; t3
tensor([-1, -1,  2,  3])
&gt;&gt;&gt; t3[0] = -9  # now we're sharing memory between 3 tensors
&gt;&gt;&gt; t3
tensor([-9, -1,  2,  3])
&gt;&gt;&gt; t2
tensor([-9, -1,  2,  3])
&gt;&gt;&gt; t
tensor([-9, -1,  2,  3])
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.dlpack.to_dlpack">
<code>torch.utils.dlpack.to_dlpack(tensor) → PyCapsule</code> </dt> <dd>
<p>Returns an opaque object (a “DLPack capsule”) representing the tensor.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><code>to_dlpack</code> is a legacy DLPack interface. The capsule it returns cannot be used for anything in Python other than use it as input to <code>from_dlpack</code>. The more idiomatic use of DLPack is to call <code>from_dlpack</code> directly on the tensor object - this works when that object has a <code>__dlpack__</code> method, which PyTorch and most other libraries indeed have now.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Only call <code>from_dlpack</code> once per capsule produced with <code>to_dlpack</code>. Behavior when a capsule is consumed multiple times is undefined.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>tensor</strong> – a tensor to be exported</p> </dd> </dl> <p>The DLPack capsule shares the tensor’s memory.</p> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/dlpack.html" class="_attribution-link">https://pytorch.org/docs/2.1/dlpack.html</a>
  </p>
</div>
