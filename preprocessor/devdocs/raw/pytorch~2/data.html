<h1 id="torch-utils-data">torch.utils.data</h1> <p id="module-torch.utils.data">At the heart of PyTorch data loading utility is the <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>torch.utils.data.DataLoader</code></a> class. It represents a Python iterable over a dataset, with support for</p> <ul class="simple"> <li>
<a class="reference internal" href="#dataset-types">map-style and iterable-style datasets</a>,</li> <li>
<a class="reference internal" href="#data-loading-order-and-sampler">customizing data loading order</a>,</li> <li>
<a class="reference internal" href="#loading-batched-and-non-batched-data">automatic batching</a>,</li> <li>
<a class="reference internal" href="#single-and-multi-process-data-loading">single- and multi-process data loading</a>,</li> <li>
<a class="reference internal" href="#memory-pinning">automatic memory pinning</a>.</li> </ul> <p>These options are configured by the constructor arguments of a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>, which has signature:</p> <pre data-language="python">DataLoader(dataset, batch_size=1, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, *, prefetch_factor=2,
           persistent_workers=False)
</pre> <p>The sections below describe in details the effects and usages of these options.</p>  <h2 id="dataset-types">Dataset Types</h2> <p>The most important argument of <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> constructor is <code>dataset</code>, which indicates a dataset object to load data from. PyTorch supports two different types of datasets:</p> <ul class="simple"> <li>
<a class="reference internal" href="#map-style-datasets">map-style datasets</a>,</li> <li>
<a class="reference internal" href="#iterable-style-datasets">iterable-style datasets</a>.</li> </ul>  <h3 id="map-style-datasets">Map-style datasets</h3> <p>A map-style dataset is one that implements the <code>__getitem__()</code> and <code>__len__()</code> protocols, and represents a map from (possibly non-integral) indices/keys to data samples.</p> <p>For example, such a dataset, when accessed with <code>dataset[idx]</code>, could read the <code>idx</code>-th image and its corresponding label from a folder on the disk.</p> <p>See <a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><code>Dataset</code></a> for more details.</p>   <h3 id="iterable-style-datasets">Iterable-style datasets</h3> <p>An iterable-style dataset is an instance of a subclass of <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> that implements the <code>__iter__()</code> protocol, and represents an iterable over data samples. This type of datasets is particularly suitable for cases where random reads are expensive or even improbable, and where the batch size depends on the fetched data.</p> <p>For example, such a dataset, when called <code>iter(dataset)</code>, could return a stream of data reading from a database, a remote server, or even logs generated in real time.</p> <p>See <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> for more details.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When using a <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> with <a class="reference internal" href="#multi-process-data-loading">multi-process data loading</a>. The same dataset object is replicated on each worker process, and thus the replicas must be configured differently to avoid duplicated data. See <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> documentations for how to achieve this.</p> </div>    <h2 id="data-loading-order-and-sampler">Data Loading Order and Sampler</h2> <p>For <a class="reference internal" href="#iterable-style-datasets">iterable-style datasets</a>, data loading order is entirely controlled by the user-defined iterable. This allows easier implementations of chunk-reading and dynamic batch size (e.g., by yielding a batched sample at each time).</p> <p>The rest of this section concerns the case with <a class="reference internal" href="#map-style-datasets">map-style datasets</a>. <a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><code>torch.utils.data.Sampler</code></a> classes are used to specify the sequence of indices/keys used in data loading. They represent iterable objects over the indices to datasets. E.g., in the common case with stochastic gradient decent (SGD), a <a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><code>Sampler</code></a> could randomly permute a list of indices and yield each one at a time, or yield a small number of them for mini-batch SGD.</p> <p>A sequential or shuffled sampler will be automatically constructed based on the <code>shuffle</code> argument to a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>. Alternatively, users may use the <code>sampler</code> argument to specify a custom <a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><code>Sampler</code></a> object that at each time yields the next index/key to fetch.</p> <p>A custom <a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><code>Sampler</code></a> that yields a list of batch indices at a time can be passed as the <code>batch_sampler</code> argument. Automatic batching can also be enabled via <code>batch_size</code> and <code>drop_last</code> arguments. See <a class="reference internal" href="#loading-batched-and-non-batched-data">the next section</a> for more details on this.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Neither <code>sampler</code> nor <code>batch_sampler</code> is compatible with iterable-style datasets, since such datasets have no notion of a key or an index.</p> </div>   <h2 id="loading-batched-and-non-batched-data">Loading Batched and Non-Batched Data</h2> <p><a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> supports automatically collating individual fetched data samples into batches via arguments <code>batch_size</code>, <code>drop_last</code>, <code>batch_sampler</code>, and <code>collate_fn</code> (which has a default function).</p>  <h3 id="automatic-batching-default">Automatic batching (default)</h3> <p>This is the most common case, and corresponds to fetching a minibatch of data and collating them into batched samples, i.e., containing Tensors with one dimension being the batch dimension (usually the first).</p> <p>When <code>batch_size</code> (default <code>1</code>) is not <code>None</code>, the data loader yields batched samples instead of individual samples. <code>batch_size</code> and <code>drop_last</code> arguments are used to specify how the data loader obtains batches of dataset keys. For map-style datasets, users can alternatively specify <code>batch_sampler</code>, which yields a list of keys at a time.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>batch_size</code> and <code>drop_last</code> arguments essentially are used to construct a <code>batch_sampler</code> from <code>sampler</code>. For map-style datasets, the <code>sampler</code> is either provided by user or constructed based on the <code>shuffle</code> argument. For iterable-style datasets, the <code>sampler</code> is a dummy infinite one. See <a class="reference internal" href="#data-loading-order-and-sampler">this section</a> on more details on samplers.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When fetching from <a class="reference internal" href="#iterable-style-datasets">iterable-style datasets</a> with <a class="reference internal" href="#multi-process-data-loading">multi-processing</a>, the <code>drop_last</code> argument drops the last non-full batch of each worker’s dataset replica.</p> </div> <p>After fetching a list of samples using the indices from sampler, the function passed as the <code>collate_fn</code> argument is used to collate lists of samples into batches.</p> <p>In this case, loading from a map-style dataset is roughly equivalent with:</p> <pre data-language="python">for indices in batch_sampler:
    yield collate_fn([dataset[i] for i in indices])
</pre> <p>and loading from an iterable-style dataset is roughly equivalent with:</p> <pre data-language="python">dataset_iter = iter(dataset)
for indices in batch_sampler:
    yield collate_fn([next(dataset_iter) for _ in indices])
</pre> <p>A custom <code>collate_fn</code> can be used to customize collation, e.g., padding sequential data to max length of a batch. See <a class="reference internal" href="#dataloader-collate-fn">this section</a> on more about <code>collate_fn</code>.</p>   <h3 id="disable-automatic-batching">Disable automatic batching</h3> <p>In certain cases, users may want to handle batching manually in dataset code, or simply load individual samples. For example, it could be cheaper to directly load batched data (e.g., bulk reads from a database or reading continuous chunks of memory), or the batch size is data dependent, or the program is designed to work on individual samples. Under these scenarios, it’s likely better to not use automatic batching (where <code>collate_fn</code> is used to collate the samples), but let the data loader directly return each member of the <code>dataset</code> object.</p> <p>When both <code>batch_size</code> and <code>batch_sampler</code> are <code>None</code> (default value for <code>batch_sampler</code> is already <code>None</code>), automatic batching is disabled. Each sample obtained from the <code>dataset</code> is processed with the function passed as the <code>collate_fn</code> argument.</p> <p><strong>When automatic batching is disabled</strong>, the default <code>collate_fn</code> simply converts NumPy arrays into PyTorch Tensors, and keeps everything else untouched.</p> <p>In this case, loading from a map-style dataset is roughly equivalent with:</p> <pre data-language="python">for index in sampler:
    yield collate_fn(dataset[index])
</pre> <p>and loading from an iterable-style dataset is roughly equivalent with:</p> <pre data-language="python">for data in iter(dataset):
    yield collate_fn(data)
</pre> <p>See <a class="reference internal" href="#dataloader-collate-fn">this section</a> on more about <code>collate_fn</code>.</p>   <h3 id="dataloader-collate-fn">Working with <code>collate_fn</code>
</h3> <p id="working-with-collate-fn">The use of <code>collate_fn</code> is slightly different when automatic batching is enabled or disabled.</p> <p><strong>When automatic batching is disabled</strong>, <code>collate_fn</code> is called with each individual data sample, and the output is yielded from the data loader iterator. In this case, the default <code>collate_fn</code> simply converts NumPy arrays in PyTorch tensors.</p> <p><strong>When automatic batching is enabled</strong>, <code>collate_fn</code> is called with a list of data samples at each time. It is expected to collate the input samples into a batch for yielding from the data loader iterator. The rest of this section describes the behavior of the default <code>collate_fn</code> (<a class="reference internal" href="#torch.utils.data.default_collate" title="torch.utils.data.default_collate"><code>default_collate()</code></a>).</p> <p>For instance, if each data sample consists of a 3-channel image and an integral class label, i.e., each element of the dataset returns a tuple <code>(image, class_index)</code>, the default <code>collate_fn</code> collates a list of such tuples into a single tuple of a batched image tensor and a batched class label Tensor. In particular, the default <code>collate_fn</code> has the following properties:</p> <ul class="simple"> <li>It always prepends a new dimension as the batch dimension.</li> <li>It automatically converts NumPy arrays and Python numerical values into PyTorch Tensors.</li> <li>It preserves the data structure, e.g., if each sample is a dictionary, it outputs a dictionary with the same set of keys but batched Tensors as values (or lists if the values can not be converted into Tensors). Same for <code>list</code> s, <code>tuple</code> s, <code>namedtuple</code> s, etc.</li> </ul> <p>Users may use customized <code>collate_fn</code> to achieve custom batching, e.g., collating along a dimension other than the first, padding sequences of various lengths, or adding support for custom data types.</p> <p>If you run into a situation where the outputs of <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> have dimensions or type that is different from your expectation, you may want to check your <code>collate_fn</code>.</p>    <h2 id="single-and-multi-process-data-loading">Single- and Multi-process Data Loading</h2> <p>A <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> uses single-process data loading by default.</p> <p>Within a Python process, the <a class="reference external" href="https://wiki.python.org/moin/GlobalInterpreterLock">Global Interpreter Lock (GIL)</a> prevents true fully parallelizing Python code across threads. To avoid blocking computation code with data loading, PyTorch provides an easy switch to perform multi-process data loading by simply setting the argument <code>num_workers</code> to a positive integer.</p>  <h3 id="single-process-data-loading-default">Single-process data loading (default)</h3> <p>In this mode, data fetching is done in the same process a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> is initialized. Therefore, data loading may block computing. However, this mode may be preferred when resource(s) used for sharing data among processes (e.g., shared memory, file descriptors) is limited, or when the entire dataset is small and can be loaded entirely in memory. Additionally, single-process loading often shows more readable error traces and thus is useful for debugging.</p>   <h3 id="multi-process-data-loading">Multi-process data loading</h3> <p>Setting the argument <code>num_workers</code> as a positive integer will turn on multi-process data loading with the specified number of loader worker processes.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>After several iterations, the loader worker processes will consume the same amount of CPU memory as the parent process for all Python objects in the parent process which are accessed from the worker processes. This can be problematic if the Dataset contains a lot of data (e.g., you are loading a very large list of filenames at Dataset construction time) and/or you are using a lot of workers (overall memory usage is <code>number of workers * size of parent process</code>). The simplest workaround is to replace Python objects with non-refcounted representations such as Pandas, Numpy or PyArrow objects. Check out <a class="reference external" href="https://github.com/pytorch/pytorch/issues/13246#issuecomment-905703662">issue #13246</a> for more details on why this occurs and example code for how to workaround these problems.</p> </div> <p>In this mode, each time an iterator of a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> is created (e.g., when you call <code>enumerate(dataloader)</code>), <code>num_workers</code> worker processes are created. At this point, the <code>dataset</code>, <code>collate_fn</code>, and <code>worker_init_fn</code> are passed to each worker, where they are used to initialize, and fetch data. This means that dataset access together with its internal IO, transforms (including <code>collate_fn</code>) runs in the worker process.</p> <p><a class="reference internal" href="#torch.utils.data.get_worker_info" title="torch.utils.data.get_worker_info"><code>torch.utils.data.get_worker_info()</code></a> returns various useful information in a worker process (including the worker id, dataset replica, initial seed, etc.), and returns <code>None</code> in main process. Users may use this function in dataset code and/or <code>worker_init_fn</code> to individually configure each dataset replica, and to determine whether the code is running in a worker process. For example, this can be particularly helpful in sharding the dataset.</p> <p>For map-style datasets, the main process generates the indices using <code>sampler</code> and sends them to the workers. So any shuffle randomization is done in the main process which guides loading by assigning indices to load.</p> <p>For iterable-style datasets, since each worker process gets a replica of the <code>dataset</code> object, naive multi-process loading will often result in duplicated data. Using <a class="reference internal" href="#torch.utils.data.get_worker_info" title="torch.utils.data.get_worker_info"><code>torch.utils.data.get_worker_info()</code></a> and/or <code>worker_init_fn</code>, users may configure each replica independently. (See <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> documentations for how to achieve this. ) For similar reasons, in multi-process loading, the <code>drop_last</code> argument drops the last non-full batch of each worker’s iterable-style dataset replica.</p> <p>Workers are shut down once the end of the iteration is reached, or when the iterator becomes garbage collected.</p> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>It is generally not recommended to return CUDA tensors in multi-process loading because of many subtleties in using CUDA and sharing CUDA tensors in multiprocessing (see <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/multiprocessing.html#multiprocessing-cuda-note"><span class="std std-ref">CUDA in multiprocessing</span></a>). Instead, we recommend using <a class="reference internal" href="#memory-pinning">automatic memory pinning</a> (i.e., setting <code>pin_memory=True</code>), which enables fast data transfer to CUDA-enabled GPUs.</p> </div>  <h4 id="platform-specific-behaviors">Platform-specific behaviors</h4> <p>Since workers rely on Python <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.12)"><code>multiprocessing</code></a>, worker launch behavior is different on Windows compared to Unix.</p> <ul class="simple"> <li>On Unix, <code>fork()</code> is the default <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.12)"><code>multiprocessing</code></a> start method. Using <code>fork()</code>, child workers typically can access the <code>dataset</code> and Python argument functions directly through the cloned address space.</li> <li>On Windows or MacOS, <code>spawn()</code> is the default <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing" title="(in Python v3.12)"><code>multiprocessing</code></a> start method. Using <code>spawn()</code>, another interpreter is launched which runs your main script, followed by the internal worker function that receives the <code>dataset</code>, <code>collate_fn</code> and other arguments through <a class="reference external" href="https://docs.python.org/3/library/pickle.html#module-pickle" title="(in Python v3.12)"><code>pickle</code></a> serialization.</li> </ul> <p>This separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:</p> <ul class="simple"> <li>Wrap most of you main script’s code within <code>if __name__ == '__main__':</code> block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> instance creation logic here, as it doesn’t need to be re-executed in workers.</li> <li>Make sure that any custom <code>collate_fn</code>, <code>worker_init_fn</code> or <code>dataset</code> code is declared as top level definitions, outside of the <code>__main__</code> check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not <code>bytecode</code>.)</li> </ul>   <h4 id="data-loading-randomness">Randomness in multi-process data loading</h4> <p id="randomness-in-multi-process-data-loading">By default, each worker will have its PyTorch seed set to <code>base_seed + worker_id</code>, where <code>base_seed</code> is a long generated by main process using its RNG (thereby, consuming a RNG state mandatorily) or a specified <code>generator</code>. However, seeds for other libraries may be duplicated upon initializing workers, causing each worker to return identical random numbers. (See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/faq.html#dataloader-workers-random-seed"><span class="std std-ref">this section</span></a> in FAQ.).</p> <p>In <code>worker_init_fn</code>, you may access the PyTorch seed set for each worker with either <a class="reference internal" href="#torch.utils.data.get_worker_info" title="torch.utils.data.get_worker_info"><code>torch.utils.data.get_worker_info().seed</code></a> or <a class="reference internal" href="generated/torch.initial_seed#torch.initial_seed" title="torch.initial_seed"><code>torch.initial_seed()</code></a>, and use it to seed other libraries before data loading.</p>     <h2 id="memory-pinning">Memory Pinning</h2> <p>Host to GPU copies are much faster when they originate from pinned (page-locked) memory. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/cuda.html#cuda-memory-pinning"><span class="std std-ref">Use pinned memory buffers</span></a> for more details on when and how to use pinned memory generally.</p> <p>For data loading, passing <code>pin_memory=True</code> to a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> will automatically put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs.</p> <p>The default memory pinning logic only recognizes Tensors and maps and iterables containing Tensors. By default, if the pinning logic sees a batch that is a custom type (which will occur if you have a <code>collate_fn</code> that returns a custom batch type), or if each element of your batch is a custom type, the pinning logic will not recognize them, and it will return that batch (or those elements) without pinning the memory. To enable memory pinning for custom batch or data type(s), define a <code>pin_memory()</code> method on your custom type(s).</p> <p>See the example below.</p> <p>Example:</p> <pre data-language="python">class SimpleCustomBatch:
    def __init__(self, data):
        transposed_data = list(zip(*data))
        self.inp = torch.stack(transposed_data[0], 0)
        self.tgt = torch.stack(transposed_data[1], 0)

    # custom memory pinning method on custom type
    def pin_memory(self):
        self.inp = self.inp.pin_memory()
        self.tgt = self.tgt.pin_memory()
        return self

def collate_wrapper(batch):
    return SimpleCustomBatch(batch)

inps = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
tgts = torch.arange(10 * 5, dtype=torch.float32).view(10, 5)
dataset = TensorDataset(inps, tgts)

loader = DataLoader(dataset, batch_size=2, collate_fn=collate_wrapper,
                    pin_memory=True)

for batch_ndx, sample in enumerate(loader):
    print(sample.inp.is_pinned())
    print(sample.tgt.is_pinned())
</pre> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.DataLoader">
<code>class torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=None, sampler=None, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None, generator=None, *, prefetch_factor=None, persistent_workers=False, pin_memory_device='')</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataloader.html#DataLoader"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Data loader. Combines a dataset and a sampler, and provides an iterable over the given dataset.</p> <p>The <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning.</p> <p>See <a class="reference internal" href="#module-torch.utils.data" title="torch.utils.data"><code>torch.utils.data</code></a> documentation page for more details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dataset</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – dataset from which to load the data.</li> <li>
<strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – how many samples per batch to load (default: <code>1</code>).</li> <li>
<strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – set to <code>True</code> to have the data reshuffled at every epoch (default: <code>False</code>).</li> <li>
<strong>sampler</strong> (<a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler">Sampler</a><em> or </em><em>Iterable</em><em>, </em><em>optional</em>) – defines the strategy to draw samples from the dataset. Can be any <code>Iterable</code> with <code>__len__</code> implemented. If specified, <code>shuffle</code> must not be specified.</li> <li>
<strong>batch_sampler</strong> (<a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler">Sampler</a><em> or </em><em>Iterable</em><em>, </em><em>optional</em>) – like <code>sampler</code>, but returns a batch of indices at a time. Mutually exclusive with <code>batch_size</code>, <code>shuffle</code>, <code>sampler</code>, and <code>drop_last</code>.</li> <li>
<strong>num_workers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – how many subprocesses to use for data loading. <code>0</code> means that the data will be loaded in the main process. (default: <code>0</code>)</li> <li>
<strong>collate_fn</strong> (<em>Callable</em><em>, </em><em>optional</em>) – merges a list of samples to form a mini-batch of Tensor(s). Used when using batched loading from a map-style dataset.</li> <li>
<strong>pin_memory</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the data loader will copy Tensors into device/CUDA pinned memory before returning them. If your data elements are a custom type, or your <code>collate_fn</code> returns a batch that is a custom type, see the example below.</li> <li>
<strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – set to <code>True</code> to drop the last incomplete batch, if the dataset size is not divisible by the batch size. If <code>False</code> and the size of dataset is not divisible by the batch size, then the last batch will be smaller. (default: <code>False</code>)</li> <li>
<strong>timeout</strong> (<em>numeric</em><em>, </em><em>optional</em>) – if positive, the timeout value for collecting a batch from workers. Should always be non-negative. (default: <code>0</code>)</li> <li>
<strong>worker_init_fn</strong> (<em>Callable</em><em>, </em><em>optional</em>) – If not <code>None</code>, this will be called on each worker subprocess with the worker id (an int in <code>[0, num_workers - 1]</code>) as input, after seeding and before data loading. (default: <code>None</code>)</li> <li>
<strong>multiprocessing_context</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em> or </em><em>multiprocessing.context.BaseContext</em><em>, </em><em>optional</em>) – If <code>None</code>, the default <a class="reference external" href="https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods">multiprocessing context</a> of your operating system will be used. (default: <code>None</code>)</li> <li>
<strong>generator</strong> (<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator">torch.Generator</a><em>, </em><em>optional</em>) – If not <code>None</code>, this RNG will be used by RandomSampler to generate random indexes and multiprocessing to generate <code>base_seed</code> for workers. (default: <code>None</code>)</li> <li>
<strong>prefetch_factor</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em><em>, </em><em>keyword-only arg</em>) – Number of batches loaded in advance by each worker. <code>2</code> means there will be a total of 2 * num_workers batches prefetched across all workers. (default value depends on the set value for num_workers. If value of num_workers=0 default is <code>None</code>. Otherwise, if value of <code>num_workers &gt; 0</code> default is <code>2</code>).</li> <li>
<strong>persistent_workers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code>, the data loader will not shut down the worker processes after a dataset has been consumed once. This allows to maintain the workers <code>Dataset</code> instances alive. (default: <code>False</code>)</li> <li>
<strong>pin_memory_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">str</a><em>, </em><em>optional</em>) – the device to <code>pin_memory</code> to if <code>pin_memory</code> is <code>True</code>.</li> </ul> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>If the <code>spawn</code> start method is used, <code>worker_init_fn</code> cannot be an unpicklable object, e.g., a lambda function. See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/multiprocessing.html#multiprocessing-best-practices"><span class="std std-ref">Multiprocessing best practices</span></a> on more details related to multiprocessing in PyTorch.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p><code>len(dataloader)</code> heuristic is based on the length of the sampler used. When <code>dataset</code> is an <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a>, it instead returns an estimate based on <code>len(dataset) / batch_size</code>, with proper rounding depending on <code>drop_last</code>, regardless of multi-process loading configurations. This represents the best guess PyTorch can make because PyTorch trusts user <code>dataset</code> code in correctly handling multi-process loading to avoid duplicate data.</p> <p>However, if sharding results in multiple workers having incomplete last batches, this estimate can still be inaccurate, because (1) an otherwise complete batch can be broken into multiple ones and (2) more than one batch worth of samples can be dropped when <code>drop_last</code> is set. Unfortunately, PyTorch can not detect such cases in general.</p> <p>See <a class="reference internal" href="#dataset-types">Dataset Types</a> for more details on these two types of datasets and how <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> interacts with <a class="reference internal" href="#multi-process-data-loading">Multi-process data loading</a>.</p> </div> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>See <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/randomness.html#reproducibility"><span class="std std-ref">Reproducibility</span></a>, and <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/faq.html#dataloader-workers-random-seed"><span class="std std-ref">My data loader workers return identical random numbers</span></a>, and <a class="reference internal" href="#data-loading-randomness"><span class="std std-ref">Randomness in multi-process data loading</span></a> notes for random seed related questions.</p> </div> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.Dataset">
<code>class torch.utils.data.Dataset(*args, **kwds)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#Dataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>An abstract class representing a <a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset"><code>Dataset</code></a>.</p> <p>All datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite <code>__getitem__()</code>, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite <code>__len__()</code>, which is expected to return the size of the dataset by many <a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler"><code>Sampler</code></a> implementations and the default options of <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>. Subclasses could also optionally implement <code>__getitems__()</code>, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> by default constructs a index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.</p> </div> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.IterableDataset">
<code>class torch.utils.data.IterableDataset(*args, **kwds)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#IterableDataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>An iterable Dataset.</p> <p>All datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.</p> <p>All subclasses should overwrite <code>__iter__()</code>, which would return an iterator of samples in this dataset.</p> <p>When a subclass is used with <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>, each item in the dataset will be yielded from the <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> iterator. When <code>num_workers &gt; 0</code>, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. <a class="reference internal" href="#torch.utils.data.get_worker_info" title="torch.utils.data.get_worker_info"><code>get_worker_info()</code></a>, when called in a worker process, returns information about the worker. It can be used in either the dataset’s <code>__iter__()</code> method or the <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> ‘s <code>worker_init_fn</code> option to modify each copy’s behavior.</p> <p>Example 1: splitting workload across all workers in <code>__iter__()</code>:</p> <pre data-language="python">&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         worker_info = torch.utils.data.get_worker_info()
...         if worker_info is None:  # single-process data loading, return the full iterator
...             iter_start = self.start
...             iter_end = self.end
...         else:  # in a worker process
...             # split workload
...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))
...             worker_id = worker_info.id
...             iter_start = self.start + worker_id * per_worker
...             iter_end = min(iter_start + per_worker, self.end)
...         return iter(range(iter_start, iter_end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[tensor([3]), tensor([4]), tensor([5]), tensor([6])]

&gt;&gt;&gt; # Mult-process loading with two worker processes
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[tensor([3]), tensor([5]), tensor([4]), tensor([6])]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12)))
[tensor([3]), tensor([5]), tensor([4]), tensor([6])]
</pre> <p>Example 2: splitting workload across all workers using <code>worker_init_fn</code>:</p> <pre data-language="python">&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):
...     def __init__(self, start, end):
...         super(MyIterableDataset).__init__()
...         assert end &gt; start, "this example code only works with end &gt;= start"
...         self.start = start
...         self.end = end
...
...     def __iter__(self):
...         return iter(range(self.start, self.end))
...
&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].
&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)

&gt;&gt;&gt; # Single-process loading
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))
[3, 4, 5, 6]
&gt;&gt;&gt;
&gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))
[3, 3, 4, 4, 5, 5, 6, 6]

&gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently
&gt;&gt;&gt; def worker_init_fn(worker_id):
...     worker_info = torch.utils.data.get_worker_info()
...     dataset = worker_info.dataset  # the dataset copy in this worker process
...     overall_start = dataset.start
...     overall_end = dataset.end
...     # configure the dataset to only process the split workload
...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))
...     worker_id = worker_info.id
...     dataset.start = overall_start + worker_id * per_worker
...     dataset.end = min(dataset.start + per_worker, overall_end)
...

&gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn`
&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))
[3, 5, 4, 6]

&gt;&gt;&gt; # With even more workers
&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))
[3, 4, 5, 6]
</pre> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.TensorDataset">
<code>class torch.utils.data.TensorDataset(*tensors)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#TensorDataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dataset wrapping tensors.</p> <p>Each sample will be retrieved by indexing tensors along the first dimension.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>*tensors</strong> (<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor">Tensor</a>) – tensors that have the same size of the first dimension.</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.StackDataset">
<code>class torch.utils.data.StackDataset(*args, **kwargs)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#StackDataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dataset as a stacking of multiple datasets.</p> <p>This class is useful to assemble different parts of complex input data, given as datasets.</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; images = ImageDataset()
&gt;&gt;&gt; texts = TextDataset()
&gt;&gt;&gt; tuple_stack = StackDataset(images, texts)
&gt;&gt;&gt; tuple_stack[0] == (images[0], texts[0])
&gt;&gt;&gt; dict_stack = StackDataset(image=images, text=texts)
&gt;&gt;&gt; dict_stack[0] == {'image': images[0], 'text': texts[0]}
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>*args</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – Datasets for stacking returned as tuple.</li> <li>
<strong>**kwargs</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – Datasets for stacking returned as dict.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.ConcatDataset">
<code>class torch.utils.data.ConcatDataset(datasets)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#ConcatDataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dataset as a concatenation of multiple datasets.</p> <p>This class is useful to assemble different existing datasets.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>datasets</strong> (<em>sequence</em>) – List of datasets to be concatenated</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.ChainDataset">
<code>class torch.utils.data.ChainDataset(datasets)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#ChainDataset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Dataset for chaining multiple <a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset"><code>IterableDataset</code></a> s.</p> <p>This class is useful to assemble different existing dataset streams. The chaining operation is done on-the-fly, so concatenating large-scale datasets with this class will be efficient.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>datasets</strong> (<em>iterable</em><em> of </em><a class="reference internal" href="#torch.utils.data.IterableDataset" title="torch.utils.data.IterableDataset">IterableDataset</a>) – datasets to be chained together</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.Subset">
<code>class torch.utils.data.Subset(dataset, indices)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#Subset"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Subset of a dataset at specified indices.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dataset</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – The whole Dataset</li> <li>
<strong>indices</strong> (<em>sequence</em>) – Indices in the whole set selected for subset</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.data._utils.collate.collate">
<code>torch.utils.data._utils.collate.collate(batch, *, collate_fn_map=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/_utils/collate.html#collate"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>General collate function that handles collection type of element within each batch and opens function registry to deal with specific element types. <code>default_collate_fn_map</code> provides default collate functions for tensors, numpy arrays, numbers and strings.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>batch</strong> – a single batch to be collated</li> <li>
<strong>collate_fn_map</strong> (<a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">Dict</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">Union</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">Type</a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.12)">Tuple</a><em>[</em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">Type</a><em>, </em><em>...</em><em>]</em><em>]</em><em>, </em><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">Callable</a><em>]</em><em>]</em>) – Optional dictionary mapping from element type to the corresponding collate function. If the element type isn’t present in this dictionary, this function will go through each key of the dictionary in the insertion order to invoke the corresponding collate function if the element type is a subclass of the key.</li> </ul> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; # Extend this function to handle batch of tensors
&gt;&gt;&gt; def collate_tensor_fn(batch, *, collate_fn_map):
...     return torch.stack(batch, 0)
&gt;&gt;&gt; def custom_collate(batch):
...     collate_map = {torch.Tensor: collate_tensor_fn}
...     return collate(batch, collate_fn_map=collate_map)
&gt;&gt;&gt; # Extend `default_collate` by in-place modifying `default_collate_fn_map`
&gt;&gt;&gt; default_collate_fn_map.update({torch.Tensor: collate_tensor_fn})
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Each collate function requires a positional argument for batch and a keyword argument for the dictionary of collate functions as <code>collate_fn_map</code>.</p> </div> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.data.default_collate">
<code>torch.utils.data.default_collate(batch)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/_utils/collate.html#default_collate"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that takes in a batch of data and puts the elements within the batch into a tensor with an additional outer dimension - batch size. The exact output type can be a <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>, a <code>Sequence</code> of <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>, a Collection of <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>, or left unchanged, depending on the input type. This is used as the default function for collation when <code>batch_size</code> or <code>batch_sampler</code> is defined in <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>.</p> <p>Here is the general input type (based on the type of the element within the batch) to output type mapping:</p>  <ul class="simple"> <li>
<a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> -&gt; <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> (with an added outer dimension batch size)</li> <li>NumPy Arrays -&gt; <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>
</li> <li>
<code>float</code> -&gt; <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>
</li> <li>
<code>int</code> -&gt; <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>
</li> <li>
<code>str</code> -&gt; <code>str</code> (unchanged)</li> <li>
<code>bytes</code> -&gt; <code>bytes</code> (unchanged)</li> <li>
<code>Mapping[K, V_i]</code> -&gt; <code>Mapping[K, default_collate([V_1, V_2, …])]</code>
</li> <li>
<code>NamedTuple[V1_i, V2_i, …]</code> -&gt; <code>NamedTuple[default_collate([V1_1, V1_2, …]), default_collate([V2_1, V2_2, …]), …]</code>
</li> <li>
<code>Sequence[V1_i, V2_i, …]</code> -&gt; <code>Sequence[default_collate([V1_1, V1_2, …]), default_collate([V2_1, V2_2, …]), …]</code>
</li> </ul>  <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>batch</strong> – a single batch to be collated</p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; # Example with a batch of `int`s:
&gt;&gt;&gt; default_collate([0, 1, 2, 3])
tensor([0, 1, 2, 3])
&gt;&gt;&gt; # Example with a batch of `str`s:
&gt;&gt;&gt; default_collate(['a', 'b', 'c'])
['a', 'b', 'c']
&gt;&gt;&gt; # Example with `Map` inside the batch:
&gt;&gt;&gt; default_collate([{'A': 0, 'B': 1}, {'A': 100, 'B': 100}])
{'A': tensor([  0, 100]), 'B': tensor([  1, 100])}
&gt;&gt;&gt; # Example with `NamedTuple` inside the batch:
&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])
&gt;&gt;&gt; default_collate([Point(0, 0), Point(1, 1)])
Point(x=tensor([0, 1]), y=tensor([0, 1]))
&gt;&gt;&gt; # Example with `Tuple` inside the batch:
&gt;&gt;&gt; default_collate([(0, 1), (2, 3)])
[tensor([0, 2]), tensor([1, 3])]
&gt;&gt;&gt; # Example with `List` inside the batch:
&gt;&gt;&gt; default_collate([[0, 1], [2, 3]])
[tensor([0, 2]), tensor([1, 3])]
&gt;&gt;&gt; # Two options to extend `default_collate` to handle specific type
&gt;&gt;&gt; # Option 1: Write custom collate function and invoke `default_collate`
&gt;&gt;&gt; def custom_collate(batch):
...     elem = batch[0]
...     if isinstance(elem, CustomType):  # Some custom condition
...         return ...
...     else:  # Fall back to `default_collate`
...         return default_collate(batch)
&gt;&gt;&gt; # Option 2: In-place modify `default_collate_fn_map`
&gt;&gt;&gt; def collate_customtype_fn(batch, *, collate_fn_map=None):
...     return ...
&gt;&gt;&gt; default_collate_fn_map.update(CustoType, collate_customtype_fn)
&gt;&gt;&gt; default_collate(batch)  # Handle `CustomType` automatically
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.data.default_convert">
<code>torch.utils.data.default_convert(data)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/_utils/collate.html#default_convert"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Function that converts each NumPy array element into a <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>. If the input is a <code>Sequence</code>, <code>Collection</code>, or <code>Mapping</code>, it tries to convert each element inside to a <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a>. If the input is not an NumPy array, it is left unchanged. This is used as the default function for collation when both <code>batch_sampler</code> and <code>batch_size</code> are NOT defined in <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>.</p> <p>The general input type to output type mapping is similar to that of <a class="reference internal" href="#torch.utils.data.default_collate" title="torch.utils.data.default_collate"><code>default_collate()</code></a>. See the description there for more details.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>data</strong> – a single data point to be converted</p> </dd> </dl> <h4 class="rubric">Examples</h4> <pre data-language="python">&gt;&gt;&gt; # Example with `int`
&gt;&gt;&gt; default_convert(0)
0
&gt;&gt;&gt; # Example with NumPy array
&gt;&gt;&gt; default_convert(np.array([0, 1]))
tensor([0, 1])
&gt;&gt;&gt; # Example with NamedTuple
&gt;&gt;&gt; Point = namedtuple('Point', ['x', 'y'])
&gt;&gt;&gt; default_convert(Point(0, 0))
Point(x=0, y=0)
&gt;&gt;&gt; default_convert(Point(np.array(0), np.array(0)))
Point(x=tensor(0), y=tensor(0))
&gt;&gt;&gt; # Example with List
&gt;&gt;&gt; default_convert([np.array([0, 1]), np.array([2, 3])])
[tensor([0, 1]), tensor([2, 3])]
</pre> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.data.get_worker_info">
<code>torch.utils.data.get_worker_info()</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/_utils/worker.html#get_worker_info"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Returns the information about the current <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> iterator worker process.</p> <p>When called in a worker, this returns an object guaranteed to have the following attributes:</p> <ul class="simple"> <li>
<code>id</code>: the current worker id.</li> <li>
<code>num_workers</code>: the total number of workers.</li> <li>
<code>seed</code>: the random seed set for the current worker. This value is determined by main process RNG and the worker id. See <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>’s documentation for more details.</li> <li>
<code>dataset</code>: the copy of the dataset object in <strong>this</strong> process. Note that this will be a different object in a different process than the one in the main process.</li> </ul> <p>When called in the main process, this returns <code>None</code>.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>When used in a <code>worker_init_fn</code> passed over to <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>, this method can be useful to set up each worker process differently, for instance, using <code>worker_id</code> to configure the <code>dataset</code> object to only read a specific fraction of a sharded dataset, or use <code>seed</code> to seed other libraries used in dataset code.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Return type</dt> <dd class="field-odd">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">Optional</a>[<em>WorkerInfo</em>]</p> </dd> </dl> </dd>
</dl> <dl class="py function"> <dt class="sig sig-object py" id="torch.utils.data.random_split">
<code>torch.utils.data.random_split(dataset, lengths, generator=&lt;torch._C.Generator object&gt;)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/dataset.html#random_split"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Randomly split a dataset into non-overlapping new datasets of given lengths.</p> <p>If a list of fractions that sum up to 1 is given, the lengths will be computed automatically as floor(frac * len(dataset)) for each fraction provided.</p> <p>After computing the lengths, if there are any remainders, 1 count will be distributed in round-robin fashion to the lengths until there are no remainders left.</p> <p>Optionally fix the generator for reproducible results, e.g.:</p> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; generator1 = torch.Generator().manual_seed(42)
&gt;&gt;&gt; generator2 = torch.Generator().manual_seed(42)
&gt;&gt;&gt; random_split(range(10), [3, 7], generator=generator1)
&gt;&gt;&gt; random_split(range(30), [0.3, 0.3, 0.4], generator=generator2)
</pre> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dataset</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – Dataset to be split</li> <li>
<strong>lengths</strong> (<em>sequence</em>) – lengths or fractions of splits to be produced</li> <li>
<strong>generator</strong> (<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator">Generator</a>) – Generator used for the random permutation.</li> </ul> </dd> <dt class="field-even">Return type</dt> <dd class="field-even">
<p><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.12)">List</a>[<a class="reference internal" href="#torch.utils.data.Subset" title="torch.utils.data.dataset.Subset">Subset</a>[<em>T</em>]]</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.Sampler">
<code>class torch.utils.data.Sampler(data_source=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#Sampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Base class for all Samplers.</p> <p>Every Sampler subclass has to provide an <code>__iter__()</code> method, providing a way to iterate over indices or lists of indices (batches) of dataset elements, and a <code>__len__()</code> method that returns the length of the returned iterators.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>data_source</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – This argument is not used and will be removed in 2.2.0. You may still have custom implementation that utilizes it.</p> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; class AccedingSequenceLengthSampler(Sampler[int]):
&gt;&gt;&gt;     def __init__(self, data: List[str]) -&gt; None:
&gt;&gt;&gt;         self.data = data
&gt;&gt;&gt;
&gt;&gt;&gt;     def __len__(self) -&gt; int:
&gt;&gt;&gt;         return len(self.data)
&gt;&gt;&gt;
&gt;&gt;&gt;     def __iter__(self) -&gt; Iterator[int]:
&gt;&gt;&gt;         sizes = torch.tensor([len(x) for x in self.data])
&gt;&gt;&gt;         yield from torch.argsort(sizes).tolist()
&gt;&gt;&gt;
&gt;&gt;&gt; class AccedingSequenceLengthBatchSampler(Sampler[List[int]]):
&gt;&gt;&gt;     def __init__(self, data: List[str], batch_size: int) -&gt; None:
&gt;&gt;&gt;         self.data = data
&gt;&gt;&gt;         self.batch_size = batch_size
&gt;&gt;&gt;
&gt;&gt;&gt;     def __len__(self) -&gt; int:
&gt;&gt;&gt;         return (len(self.data) + self.batch_size - 1) // self.batch_size
&gt;&gt;&gt;
&gt;&gt;&gt;     def __iter__(self) -&gt; Iterator[List[int]]:
&gt;&gt;&gt;         sizes = torch.tensor([len(x) for x in self.data])
&gt;&gt;&gt;         for batch in torch.chunk(torch.argsort(sizes), len(self)):
&gt;&gt;&gt;             yield batch.tolist()
</pre> <div class="admonition note"> <p class="admonition-title">Note</p> <p>The <code>__len__()</code> method isn’t strictly required by <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>, but is expected in any calculation involving the length of a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a>.</p> </div> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.SequentialSampler">
<code>class torch.utils.data.SequentialSampler(data_source)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#SequentialSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Samples elements sequentially, always in the same order.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<p><strong>data_source</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – dataset to sample from</p> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.RandomSampler">
<code>class torch.utils.data.RandomSampler(data_source, replacement=False, num_samples=None, generator=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#RandomSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Samples elements randomly. If without replacement, then sample from a shuffled dataset. If with replacement, then user can specify <code>num_samples</code> to draw.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>data_source</strong> (<a class="reference internal" href="#torch.utils.data.Dataset" title="torch.utils.data.Dataset">Dataset</a>) – dataset to sample from</li> <li>
<strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – samples are drawn on-demand with replacement if <code>True</code>, default=``False``</li> <li>
<strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – number of samples to draw, default=`len(dataset)`.</li> <li>
<strong>generator</strong> (<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator">Generator</a>) – Generator used in sampling.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.SubsetRandomSampler">
<code>class torch.utils.data.SubsetRandomSampler(indices, generator=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#SubsetRandomSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Samples elements randomly from a given list of indices, without replacement.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>indices</strong> (<em>sequence</em>) – a sequence of indices</li> <li>
<strong>generator</strong> (<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator">Generator</a>) – Generator used in sampling.</li> </ul> </dd> </dl> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.WeightedRandomSampler">
<code>class torch.utils.data.WeightedRandomSampler(weights, num_samples, replacement=True, generator=None)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#WeightedRandomSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Samples elements from <code>[0,..,len(weights)-1]</code> with given probabilities (weights).</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>weights</strong> (<em>sequence</em>) – a sequence of weights, not necessary summing up to one</li> <li>
<strong>num_samples</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – number of samples to draw</li> <li>
<strong>replacement</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – if <code>True</code>, samples are drawn with replacement. If not, they are drawn without replacement, which means that when a sample index is drawn for a row, it cannot be drawn again for that row.</li> <li>
<strong>generator</strong> (<a class="reference internal" href="generated/torch.generator#torch.Generator" title="torch.Generator">Generator</a>) – Generator used in sampling.</li> </ul> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))
[4, 4, 1, 4, 5]
&gt;&gt;&gt; list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))
[0, 1, 4, 3, 2]
</pre> </dd>
</dl> <dl class="py class"> <dt class="sig sig-object py" id="torch.utils.data.BatchSampler">
<code>class torch.utils.data.BatchSampler(sampler, batch_size, drop_last)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/sampler.html#BatchSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Wraps another sampler to yield a mini-batch of indices.</p> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>sampler</strong> (<a class="reference internal" href="#torch.utils.data.Sampler" title="torch.utils.data.Sampler">Sampler</a><em> or </em><em>Iterable</em>) – Base sampler. Can be any iterable object</li> <li>
<strong>batch_size</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a>) – Size of mini-batch.</li> <li>
<strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a>) – If <code>True</code>, the sampler will drop the last batch if its size would be less than <code>batch_size</code>
</li> </ul> </dd> </dl> <h4 class="rubric">Example</h4> <pre data-language="python">&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=False))
[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]
&gt;&gt;&gt; list(BatchSampler(SequentialSampler(range(10)), batch_size=3, drop_last=True))
[[0, 1, 2], [3, 4, 5], [6, 7, 8]]
</pre> </dd>
</dl> <dl class="py class" id="module-torch.utils.data.datapipes.utils"> <dt class="sig sig-object py" id="torch.utils.data.distributed.DistributedSampler">
<code>class torch.utils.data.distributed.DistributedSampler(dataset, num_replicas=None, rank=None, shuffle=True, seed=0, drop_last=False)</code> <a class="reference internal" href="https://pytorch.org/docs/2.1/_modules/torch/utils/data/distributed.html#DistributedSampler"><span class="viewcode-link">[source]</span></a>
</dt> <dd>
<p>Sampler that restricts data loading to a subset of the dataset.</p> <p>It is especially useful in conjunction with <a class="reference internal" href="generated/torch.nn.parallel.distributeddataparallel#torch.nn.parallel.DistributedDataParallel" title="torch.nn.parallel.DistributedDataParallel"><code>torch.nn.parallel.DistributedDataParallel</code></a>. In such a case, each process can pass a <code>DistributedSampler</code> instance as a <a class="reference internal" href="#torch.utils.data.DataLoader" title="torch.utils.data.DataLoader"><code>DataLoader</code></a> sampler, and load a subset of the original dataset that is exclusive to it.</p> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Dataset is assumed to be of constant size and that any instance of it always returns the same elements in the same order.</p> </div> <dl class="field-list simple"> <dt class="field-odd">Parameters</dt> <dd class="field-odd">
<ul class="simple"> <li>
<strong>dataset</strong> – Dataset used for sampling.</li> <li>
<strong>num_replicas</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Number of processes participating in distributed training. By default, <code>world_size</code> is retrieved from the current distributed group.</li> <li>
<strong>rank</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – Rank of the current process within <code>num_replicas</code>. By default, <code>rank</code> is retrieved from the current distributed group.</li> <li>
<strong>shuffle</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – If <code>True</code> (default), sampler will shuffle the indices.</li> <li>
<strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">int</a><em>, </em><em>optional</em>) – random seed used to shuffle the sampler if <code>shuffle=True</code>. This number should be identical across all processes in the distributed group. Default: <code>0</code>.</li> <li>
<strong>drop_last</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">bool</a><em>, </em><em>optional</em>) – if <code>True</code>, then the sampler will drop the tail of the data to make it evenly divisible across the number of replicas. If <code>False</code>, the sampler will add extra indices to make the data evenly divisible across the replicas. Default: <code>False</code>.</li> </ul> </dd> </dl> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>In distributed mode, calling the <code>set_epoch()</code> method at the beginning of each epoch <strong>before</strong> creating the <code>DataLoader</code> iterator is necessary to make shuffling work properly across multiple epochs. Otherwise, the same ordering will be always used.</p> </div> <p>Example:</p> <pre data-language="python">&gt;&gt;&gt; sampler = DistributedSampler(dataset) if is_distributed else None
&gt;&gt;&gt; loader = DataLoader(dataset, shuffle=(sampler is None),
...                     sampler=sampler)
&gt;&gt;&gt; for epoch in range(start_epoch, n_epochs):
...     if is_distributed:
...         sampler.set_epoch(epoch)
...     train(loader)
</pre> </dd>
</dl><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/data.html" class="_attribution-link">https://pytorch.org/docs/2.1/data.html</a>
  </p>
</div>
