<h1 id="id1">UX Limitations</h1> <p id="ux-limitations">torch.func, like <a class="reference external" href="https://github.com/google/jax">JAX</a>, has restrictions around what can be transformed. In general, JAX’s limitations are that transforms only work with pure functions: that is, functions where the output is completely determined by the input and that do not involve side effects (like mutation).</p> <p>We have a similar guarantee: our transforms work well with pure functions. However, we do support certain in-place operations. On one hand, writing code compatible with function transforms may involve changing how you write PyTorch code, on the other hand, you may find that our transforms let you express things that were previously difficult to express in PyTorch.</p>  <h2 id="general-limitations">General limitations</h2> <p>All torch.func transforms share a limitation in that a function should not assign to global variables. Instead, all outputs to a function must be returned from the function. This restriction comes from how torch.func is implemented: each transform wraps Tensor inputs in special torch.func Tensor subclasses that facilitate the transform.</p> <p>So, instead of the following:</p> <pre data-language="python">import torch
from torch.func import grad

# Don't do this
intermediate = None

def f(x):
  global intermediate
  intermediate = x.sin()
  z = intermediate.sin()
  return z

x = torch.randn([])
grad_x = grad(f)(x)
</pre> <p>Please rewrite <code>f</code> to return <code>intermediate</code>:</p> <pre data-language="python">def f(x):
  intermediate = x.sin()
  z = intermediate.sin()
  return z, intermediate

grad_x, intermediate = grad(f, has_aux=True)(x)
</pre>   <h2 id="torch-autograd-apis">torch.autograd APIs</h2> <p>If you are trying to use a <code>torch.autograd</code> API like <code>torch.autograd.grad</code> or <code>torch.autograd.backward</code> inside of a function being transformed by <a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> or one of torch.func’s AD transforms (<a class="reference internal" href="generated/torch.func.vjp#torch.func.vjp" title="torch.func.vjp"><code>vjp()</code></a>, <a class="reference internal" href="generated/torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>jvp()</code></a>, <a class="reference internal" href="generated/torch.func.jacrev#torch.func.jacrev" title="torch.func.jacrev"><code>jacrev()</code></a>, <a class="reference internal" href="generated/torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd()</code></a>), the transform may not be able to transform over it. If it is unable to do so, you’ll receive an error message.</p> <p>This is a fundamental design limitation in how PyTorch’s AD support is implemented and the reason why we designed the torch.func library. Please instead use the torch.func equivalents of the <code>torch.autograd</code> APIs: - <code>torch.autograd.grad</code>, <code>Tensor.backward</code> -&gt; <code>torch.func.vjp</code> or <code>torch.func.grad</code> - <code>torch.autograd.functional.jvp</code> -&gt; <code>torch.func.jvp</code> - <code>torch.autograd.functional.jacobian</code> -&gt; <code>torch.func.jacrev</code> or <code>torch.func.jacfwd</code> - <code>torch.autograd.functional.hessian</code> -&gt; <code>torch.func.hessian</code></p>   <h2 id="vmap-limitations">vmap limitations</h2> <div class="admonition note"> <p class="admonition-title">Note</p> <p><a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> is our most restrictive transform. The grad-related transforms (<a class="reference internal" href="generated/torch.func.grad#torch.func.grad" title="torch.func.grad"><code>grad()</code></a>, <a class="reference internal" href="generated/torch.func.vjp#torch.func.vjp" title="torch.func.vjp"><code>vjp()</code></a>, <a class="reference internal" href="generated/torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>jvp()</code></a>) do not have these limitations. <a class="reference internal" href="generated/torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd()</code></a> (and <a class="reference internal" href="generated/torch.func.hessian#torch.func.hessian" title="torch.func.hessian"><code>hessian()</code></a>, which is implemented with <a class="reference internal" href="generated/torch.func.jacfwd#torch.func.jacfwd" title="torch.func.jacfwd"><code>jacfwd()</code></a>) is a composition of <a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> and <a class="reference internal" href="generated/torch.func.jvp#torch.func.jvp" title="torch.func.jvp"><code>jvp()</code></a> so it also has these limitations.</p> </div> <p><code>vmap(func)</code> is a transform that returns a function that maps <code>func</code> over some new dimension of each input Tensor. The mental model for vmap is that it is like running a for-loop: for pure functions (i.e. in the absence of side effects), <code>vmap(f)(x)</code> is equivalent to:</p> <pre data-language="python">torch.stack([f(x_i) for x_i in x.unbind(0)])
</pre>  <h3 id="mutation-arbitrary-mutation-of-python-data-structures">Mutation: Arbitrary mutation of Python data structures</h3> <p>In the presence of side effects, <a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> no longer acts like it is running a for-loop. For example, the following function:</p> <pre data-language="python">def f(x, list):
  list.pop()
  print("hello!")
  return x.sum(0)

x = torch.randn(3, 1)
lst = [0, 1, 2, 3]

result = vmap(f, in_dims=(0, None))(x, lst)
</pre> <p>will print “hello!” once and pop only one element from <code>lst</code>.</p> <p><a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> executes <code>f</code> a single time, so all side effects only happen once.</p> <p>This is a consequence of how vmap is implemented. torch.func has a special, internal BatchedTensor class. <code>vmap(f)(*inputs)</code> takes all Tensor inputs, turns them into BatchedTensors, and calls <code>f(*batched_tensor_inputs)</code>. BatchedTensor overrides the PyTorch API to produce batched (i.e. vectorized) behavior for each PyTorch operator.</p>   <h3 id="mutation-in-place-pytorch-operations">Mutation: in-place PyTorch Operations</h3> <p>You might be here due to receiving an error about vmap-incompatible in-place operations. <a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> will raise an error if it encounters an unsupported PyTorch in-place operation and it will succeed otherwise. Unsupported operations are those that would cause a Tensor with more elements to be written to a Tensor with fewer elements. Here’s an example of how this can occur:</p> <pre data-language="python">def f(x, y):
  x.add_(y)
  return x

x = torch.randn(1)
y = torch.randn(3, 1)  # When vmapped over, looks like it has shape [1]

# Raises an error because `x` has fewer elements than `y`.
vmap(f, in_dims=(None, 0))(x, y)
</pre> <p><code>x</code> is a Tensor with one element, <code>y</code> is a Tensor with three elements. <code>x + y</code> has three elements (due to broadcasting), but attempting to write three elements back into <code>x</code>, which only has one element, raises an error due to attempting to write three elements into a Tensor with a single element.</p> <p>There is no problem if the Tensor being written to is batched under <a class="reference internal" href="generated/torch.vmap#torch.vmap" title="torch.vmap"><code>vmap()</code></a> (i.e. it is being vmapped over).</p> <pre data-language="python">def f(x, y):
  x.add_(y)
  return x

x = torch.randn(3, 1)
y = torch.randn(3, 1)
expected = x + y

# Does not raise an error because x is being vmapped over.
vmap(f, in_dims=(0, 0))(x, y)
assert torch.allclose(x, expected)
</pre> <p>One common fix for this is to replace calls to factory functions with their “new_*” equivalent. For example:</p> <ul class="simple"> <li>Replace <a class="reference internal" href="generated/torch.zeros#torch.zeros" title="torch.zeros"><code>torch.zeros()</code></a> with <code>Tensor.new_zeros()</code>
</li> <li>Replace <a class="reference internal" href="generated/torch.empty#torch.empty" title="torch.empty"><code>torch.empty()</code></a> with <code>Tensor.new_empty()</code>
</li> </ul> <p>To see why this helps, consider the following.</p> <pre data-language="python">def diag_embed(vec):
  assert vec.dim() == 1
  result = torch.zeros(vec.shape[0], vec.shape[0])
  result.diagonal().copy_(vec)
  return result

vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])

# RuntimeError: vmap: inplace arithmetic(self, *extra_args) is not possible ...
vmap(diag_embed)(vecs)
</pre> <p>Inside of <a class="reference internal" href="generated/torch.vmap#torch.vmap" title="torch.vmap"><code>vmap()</code></a>, <code>result</code> is a Tensor of shape [3, 3]. However, although <code>vec</code> looks like it has shape [3], <code>vec</code> actually has underlying shape [2, 3]. It is not possible to copy <code>vec</code> into <code>result.diagonal()</code>, which has shape [3], because it has too many elements.</p> <pre data-language="python">def diag_embed(vec):
  assert vec.dim() == 1
  result = vec.new_zeros(vec.shape[0], vec.shape[0])
  result.diagonal().copy_(vec)
  return result

vecs = torch.tensor([[0., 1, 2], [3., 4, 5]])
vmap(diag_embed)(vecs)
</pre> <p>Replacing <a class="reference internal" href="generated/torch.zeros#torch.zeros" title="torch.zeros"><code>torch.zeros()</code></a> with <code>Tensor.new_zeros()</code> makes it so that <code>result</code> has an underlying Tensor of shape [2, 3, 3], so it is now possible to copy <code>vec</code>, which has underlying shape [2, 3], into <code>result.diagonal()</code>.</p>   <h3 id="mutation-out-pytorch-operations">Mutation: out= PyTorch Operations</h3> <p><a class="reference internal" href="generated/torch.func.vmap#torch.func.vmap" title="torch.func.vmap"><code>vmap()</code></a> doesn’t support the <code>out=</code> keyword argument in PyTorch operations. It will error out gracefully if it encounters that in your code.</p> <p>This is not a fundamental limitation; we could theoretically support this in the future but we have chosen not to for now.</p>   <h3 id="data-dependent-python-control-flow">Data-dependent Python control flow</h3> <p>We don’t yet support <code>vmap</code> over data-dependent control flow. Data-dependent control flow is when the condition of an if-statement, while-loop, or for-loop is a Tensor that is being <code>vmap</code>’ed over. For example, the following will raise an error message:</p> <pre data-language="python">def relu(x):
  if x &gt; 0:
    return x
  return 0

x = torch.randn(3)
vmap(relu)(x)
</pre> <p>However, any control flow that is not dependent on the values in <code>vmap</code>’ed tensors will work:</p> <pre data-language="python">def custom_dot(x):
  if x.dim() == 1:
    return torch.dot(x, x)
  return (x * x).sum()

x = torch.randn(3)
vmap(custom_dot)(x)
</pre> <p>JAX supports transforming over <a class="reference external" href="https://jax.readthedocs.io/en/latest/jax.lax.html#control-flow-operators">data-dependent control flow</a> using special control flow operators (e.g. <code>jax.lax.cond</code>, <code>jax.lax.while_loop</code>). We’re investigating adding equivalents of those to PyTorch.</p>   <h3 id="data-dependent-operations-item">Data-dependent operations (.item())</h3> <p>We do not (and will not) support vmap over a user-defined function that calls <code>.item()</code> on a Tensor. For example, the following will raise an error message:</p> <pre data-language="python">def f(x):
  return x.item()

x = torch.randn(3)
vmap(f)(x)
</pre> <p>Please try to rewrite your code to not use <code>.item()</code> calls.</p> <p>You may also encounter an error message about using <code>.item()</code> but you might not have used it. In those cases, it is possible that PyTorch internally is calling <code>.item()</code> – please file an issue on GitHub and we’ll fix PyTorch internals.</p>   <h3 id="dynamic-shape-operations-nonzero-and-friends">Dynamic shape operations (nonzero and friends)</h3> <p><code>vmap(f)</code> requires that <code>f</code> applied to every “example” in your input returns a Tensor with the same shape. Operations such as <code>torch.nonzero</code>, <code>torch.is_nonzero</code> are not supported and will error as a result.</p> <p>To see why, consider the following example:</p> <pre data-language="python">xs = torch.tensor([[0, 1, 2], [0, 0, 3]])
vmap(torch.nonzero)(xs)
</pre> <p><code>torch.nonzero(xs[0])</code> returns a Tensor of shape 2; but <code>torch.nonzero(xs[1])</code> returns a Tensor of shape 1. We are unable to construct a single Tensor as an output; the output would need to be a ragged Tensor (and PyTorch does not yet have the concept of a ragged Tensor).</p>    <h2 id="randomness">Randomness</h2> <p>The user’s intention when calling a random operation can be unclear. Specifically, some users may want the random behavior to be the same across batches while others may want it to differ across batches. To address this, <code>vmap</code> takes a randomness flag.</p> <p>The flag can only be passed to vmap and can take on 3 values, “error,” “different,” or “same,” defaulting to error. Under “error” mode, any call to a random function will produce an error asking the user to use one of the other two flags based on their use case.</p> <p>Under “different” randomness, elements in a batch produce different random values. For instance,</p> <pre data-language="python">def add_noise(x):
  y = torch.randn(())  # y will be different across the batch
  return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="different")(x)  # we get 3 different values
</pre> <p>Under “same” randomness, elements in a batch produce same random values. For instance,</p> <pre data-language="python">def add_noise(x):
  y = torch.randn(())  # y will be the same across the batch
  return x + y

x = torch.ones(3)
result = vmap(add_noise, randomness="same")(x)  # we get the same value, repeated 3 times
</pre> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>Our system only determine the randomness behavior of PyTorch operators and cannot control the behavior of other libraries, like numpy. This is similar to JAX’s limitations with their solutions</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Multiple vmap calls using either type of supported randomness will not produce the same results. Like with standard PyTorch, a user can get randomness reproducibility through either using <code>torch.manual_seed()</code> outside of vmap or by using generators.</p> </div> <div class="admonition note"> <p class="admonition-title">Note</p> <p>Finally, our randomness differs from JAX because we aren’t using a stateless PRNG, in part because PyTorch doesn’t have full support for a stateless PRNG. Instead, we’ve introduced a flag system to allow for the most common forms of randomness that we see. If your use case does not fit these forms of randomness, please file an issue.</p> </div><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/func.ux_limitations.html" class="_attribution-link">https://pytorch.org/docs/2.1/func.ux_limitations.html</a>
  </p>
</div>
