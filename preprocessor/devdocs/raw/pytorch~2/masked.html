<h1 id="masked-docs">torch.masked</h1>  <h2 id="torch-masked">Introduction</h2>  <h3 id="motivation">Motivation</h3> <div class="admonition warning"> <p class="admonition-title">Warning</p> <p>The PyTorch API of masked tensors is in the prototype stage and may or may not change in the future.</p> </div> <p>MaskedTensor serves as an extension to <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> that provides the user with the ability to:</p> <ul class="simple"> <li>use any masked semantics (e.g. variable length tensors, nan* operators, etc.)</li> <li>differentiate between 0 and NaN gradients</li> <li>various sparse applications (see tutorial below)</li> </ul> <p>“Specified” and “unspecified” have a long history in PyTorch without formal semantics and certainly without consistency; indeed, MaskedTensor was born out of a build up of issues that the vanilla <a class="reference internal" href="tensors#torch.Tensor" title="torch.Tensor"><code>torch.Tensor</code></a> class could not properly address. Thus, a primary goal of MaskedTensor is to become the source of truth for said “specified” and “unspecified” values in PyTorch where they are a first class citizen instead of an afterthought. In turn, this should further unlock <a class="reference external" href="https://pytorch.org/docs/stable/sparse.html">sparsity’s</a> potential, enable safer and more consistent operators, and provide a smoother and more intuitive experience for users and developers alike.</p>   <h3 id="what-is-a-maskedtensor">What is a MaskedTensor?</h3> <p>A MaskedTensor is a tensor subclass that consists of 1) an input (data), and 2) a mask. The mask tells us which entries from the input should be included or ignored.</p> <p>By way of example, suppose that we wanted to mask out all values that are equal to 0 (represented by the gray) and take the max:</p> <a class="reference internal image-reference" href="_images/tensor_comparison.jpg"><img alt="_images/tensor_comparison.jpg" src="https://pytorch.org/docs/2.1/_images/tensor_comparison.jpg" style="width: 762.0px; height: 540.0px;"></a> <p>On top is the vanilla tensor example while the bottom is MaskedTensor where all the 0’s are masked out. This clearly yields a different result depending on whether we have the mask, but this flexible structure allows the user to systematically ignore any elements they’d like during computation.</p> <p>There are already a number of existing tutorials that we’ve written to help users onboard, such as:</p> <ul class="simple"> <li><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_overview">Overview - the place to start for new users, discusses how to use MaskedTensors and why they’re useful</a></li> <li><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_sparsity">Sparsity - MaskedTensor supports sparse COO and CSR data and mask Tensors</a></li> <li><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_adagrad">Adagrad sparse semantics - a practical example of how MaskedTensor can simplify sparse semantics and implementations</a></li> <li><a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics">Advanced semantics - discussion on why certain decisions were made (e.g. requiring masks to match for binary/reduction operations), differences with NumPy’s MaskedArray, and reduction semantics</a></li> </ul>    <h2 id="supported-operators">Supported Operators</h2>  <h3 id="unary-operators">Unary Operators</h3> <p>Unary operators are operators that only contain only a single input. Applying them to MaskedTensors is relatively straightforward: if the data is masked out at a given index, we apply the operator, otherwise we’ll continue to mask out the data.</p> <p>The available unary operators are:</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>abs</code></a>
</td> <td><p>Computes the absolute value of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.absolute#torch.absolute" title="torch.absolute"><code>absolute</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.abs#torch.abs" title="torch.abs"><code>torch.abs()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>acos</code></a>
</td> <td><p>Computes the inverse cosine of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arccos#torch.arccos" title="torch.arccos"><code>arccos</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.acos#torch.acos" title="torch.acos"><code>torch.acos()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.acosh#torch.acosh" title="torch.acosh"><code>acosh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arccosh#torch.arccosh" title="torch.arccosh"><code>arccosh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.acosh#torch.acosh" title="torch.acosh"><code>torch.acosh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.angle#torch.angle" title="torch.angle"><code>angle</code></a>
</td> <td><p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>asin</code></a>
</td> <td><p>Returns a new tensor with the arcsine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arcsin#torch.arcsin" title="torch.arcsin"><code>arcsin</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.asin#torch.asin" title="torch.asin"><code>torch.asin()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.asinh#torch.asinh" title="torch.asinh"><code>asinh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arcsinh#torch.arcsinh" title="torch.arcsinh"><code>arcsinh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.asinh#torch.asinh" title="torch.asinh"><code>torch.asinh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>atan</code></a>
</td> <td><p>Returns a new tensor with the arctangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctan#torch.arctan" title="torch.arctan"><code>arctan</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atan#torch.atan" title="torch.atan"><code>torch.atan()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atanh#torch.atanh" title="torch.atanh"><code>atanh</code></a>
</td> <td><p>Returns a new tensor with the inverse hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctanh#torch.arctanh" title="torch.arctanh"><code>arctanh</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atanh#torch.atanh" title="torch.atanh"><code>torch.atanh()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_not#torch.bitwise_not" title="torch.bitwise_not"><code>bitwise_not</code></a>
</td> <td><p>Computes the bitwise NOT of the given input tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ceil#torch.ceil" title="torch.ceil"><code>ceil</code></a>
</td> <td><p>Returns a new tensor with the ceil of the elements of <code>input</code>, the smallest integer greater than or equal to each element.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.clamp#torch.clamp" title="torch.clamp"><code>clamp</code></a>
</td> <td><p>Clamps all elements in <code>input</code> into the range <code>[</code> <a class="reference internal" href="generated/torch.min#torch.min" title="torch.min"><code>min</code></a>, <a class="reference internal" href="generated/torch.max#torch.max" title="torch.max"><code>max</code></a> <code>]</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.clip#torch.clip" title="torch.clip"><code>clip</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.clamp#torch.clamp" title="torch.clamp"><code>torch.clamp()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.conj_physical#torch.conj_physical" title="torch.conj_physical"><code>conj_physical</code></a>
</td> <td><p>Computes the element-wise conjugate of the given <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cos#torch.cos" title="torch.cos"><code>cos</code></a>
</td> <td><p>Returns a new tensor with the cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cosh#torch.cosh" title="torch.cosh"><code>cosh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic cosine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.deg2rad#torch.deg2rad" title="torch.deg2rad"><code>deg2rad</code></a>
</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in degrees to radians.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.digamma#torch.digamma" title="torch.digamma"><code>digamma</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.digamma" title="torch.special.digamma"><code>torch.special.digamma()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erf#torch.erf" title="torch.erf"><code>erf</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erf" title="torch.special.erf"><code>torch.special.erf()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erfc#torch.erfc" title="torch.erfc"><code>erfc</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erfc" title="torch.special.erfc"><code>torch.special.erfc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.erfinv#torch.erfinv" title="torch.erfinv"><code>erfinv</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.erfinv" title="torch.special.erfinv"><code>torch.special.erfinv()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.exp#torch.exp" title="torch.exp"><code>exp</code></a>
</td> <td><p>Returns a new tensor with the exponential of the elements of the input tensor <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.exp2#torch.exp2" title="torch.exp2"><code>exp2</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.exp2" title="torch.special.exp2"><code>torch.special.exp2()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.expm1#torch.expm1" title="torch.expm1"><code>expm1</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.expm1" title="torch.special.expm1"><code>torch.special.expm1()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fix#torch.fix" title="torch.fix"><code>fix</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>torch.trunc()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.floor#torch.floor" title="torch.floor"><code>floor</code></a>
</td> <td><p>Returns a new tensor with the floor of the elements of <code>input</code>, the largest integer less than or equal to each element.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.frac#torch.frac" title="torch.frac"><code>frac</code></a>
</td> <td><p>Computes the fractional portion of each element in <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lgamma#torch.lgamma" title="torch.lgamma"><code>lgamma</code></a>
</td> <td><p>Computes the natural logarithm of the absolute value of the gamma function on <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log#torch.log" title="torch.log"><code>log</code></a>
</td> <td><p>Returns a new tensor with the natural logarithm of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log10#torch.log10" title="torch.log10"><code>log10</code></a>
</td> <td><p>Returns a new tensor with the logarithm to the base 10 of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log1p#torch.log1p" title="torch.log1p"><code>log1p</code></a>
</td> <td><p>Returns a new tensor with the natural logarithm of (1 + <code>input</code>).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.log2#torch.log2" title="torch.log2"><code>log2</code></a>
</td> <td><p>Returns a new tensor with the logarithm to the base 2 of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logit#torch.logit" title="torch.logit"><code>logit</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.logit" title="torch.special.logit"><code>torch.special.logit()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.i0#torch.i0" title="torch.i0"><code>i0</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.i0" title="torch.special.i0"><code>torch.special.i0()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isnan#torch.isnan" title="torch.isnan"><code>isnan</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is NaN or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nan_to_num#torch.nan_to_num" title="torch.nan_to_num"><code>nan_to_num</code></a>
</td> <td><p>Replaces <code>NaN</code>, positive infinity, and negative infinity values in <code>input</code> with the values specified by <code>nan</code>, <code>posinf</code>, and <code>neginf</code>, respectively.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>neg</code></a>
</td> <td><p>Returns a new tensor with the negative of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.negative#torch.negative" title="torch.negative"><code>negative</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.neg#torch.neg" title="torch.neg"><code>torch.neg()</code></a></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.positive#torch.positive" title="torch.positive"><code>positive</code></a>
</td> <td><p>Returns <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.pow#torch.pow" title="torch.pow"><code>pow</code></a>
</td> <td><p>Takes the power of each element in <code>input</code> with <code>exponent</code> and returns a tensor with the result.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rad2deg#torch.rad2deg" title="torch.rad2deg"><code>rad2deg</code></a>
</td> <td><p>Returns a new tensor with each of the elements of <code>input</code> converted from angles in radians to degrees.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.reciprocal#torch.reciprocal" title="torch.reciprocal"><code>reciprocal</code></a>
</td> <td><p>Returns a new tensor with the reciprocal of the elements of <code>input</code></p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.round#torch.round" title="torch.round"><code>round</code></a>
</td> <td><p>Rounds elements of <code>input</code> to the nearest integer.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.rsqrt#torch.rsqrt" title="torch.rsqrt"><code>rsqrt</code></a>
</td> <td><p>Returns a new tensor with the reciprocal of the square-root of each of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sigmoid#torch.sigmoid" title="torch.sigmoid"><code>sigmoid</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.expit" title="torch.special.expit"><code>torch.special.expit()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sign#torch.sign" title="torch.sign"><code>sign</code></a>
</td> <td><p>Returns a new tensor with the signs of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sgn#torch.sgn" title="torch.sgn"><code>sgn</code></a>
</td> <td><p>This function is an extension of torch.sign() to complex tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.signbit#torch.signbit" title="torch.signbit"><code>signbit</code></a>
</td> <td><p>Tests if each element of <code>input</code> has its sign bit set or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sin#torch.sin" title="torch.sin"><code>sin</code></a>
</td> <td><p>Returns a new tensor with the sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sinc#torch.sinc" title="torch.sinc"><code>sinc</code></a>
</td> <td><p>Alias for <a class="reference internal" href="special#torch.special.sinc" title="torch.special.sinc"><code>torch.special.sinc()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sinh#torch.sinh" title="torch.sinh"><code>sinh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic sine of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sqrt#torch.sqrt" title="torch.sqrt"><code>sqrt</code></a>
</td> <td><p>Returns a new tensor with the square-root of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.square#torch.square" title="torch.square"><code>square</code></a>
</td> <td><p>Returns a new tensor with the square of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tan#torch.tan" title="torch.tan"><code>tan</code></a>
</td> <td><p>Returns a new tensor with the tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.tanh#torch.tanh" title="torch.tanh"><code>tanh</code></a>
</td> <td><p>Returns a new tensor with the hyperbolic tangent of the elements of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.trunc#torch.trunc" title="torch.trunc"><code>trunc</code></a>
</td> <td><p>Returns a new tensor with the truncated integer values of the elements of <code>input</code>.</p></td> </tr>  </table> <p>The available inplace unary operators are all of the above <strong>except</strong>:</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.angle#torch.angle" title="torch.angle"><code>angle</code></a>
</td> <td><p>Computes the element-wise angle (in radians) of the given <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.positive#torch.positive" title="torch.positive"><code>positive</code></a>
</td> <td><p>Returns <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.signbit#torch.signbit" title="torch.signbit"><code>signbit</code></a>
</td> <td><p>Tests if each element of <code>input</code> has its sign bit set or not.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.isnan#torch.isnan" title="torch.isnan"><code>isnan</code></a>
</td> <td><p>Returns a new tensor with boolean elements representing if each element of <code>input</code> is NaN or not.</p></td> </tr>  </table>   <h3 id="binary-operators">Binary Operators</h3> <p>As you may have seen in the tutorial, <code>MaskedTensor</code> also has binary operations implemented with the caveat that the masks in the two MaskedTensors must match or else an error will be raised. As noted in the error, if you need support for a particular operator or have proposed semantics for how they should behave instead, please open an issue on GitHub. For now, we have decided to go with the most conservative implementation to ensure that users know exactly what is going on and are being intentional about their decisions with masked semantics.</p> <p>The available binary operators are:</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.add#torch.add" title="torch.add"><code>add</code></a>
</td> <td><p>Adds <code>other</code>, scaled by <code>alpha</code>, to <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.atan2#torch.atan2" title="torch.atan2"><code>atan2</code></a>
</td> <td><p>Element-wise arctangent of <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext>input</mtext><mi>i</mi></msub><mi mathvariant="normal">/</mi><msub><mtext>other</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{input}_{i} / \text{other}_{i}</annotation></semantics></math></span></span></span> with consideration of the quadrant.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.arctan2#torch.arctan2" title="torch.arctan2"><code>arctan2</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.atan2#torch.atan2" title="torch.atan2"><code>torch.atan2()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_and#torch.bitwise_and" title="torch.bitwise_and"><code>bitwise_and</code></a>
</td> <td><p>Computes the bitwise AND of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_or#torch.bitwise_or" title="torch.bitwise_or"><code>bitwise_or</code></a>
</td> <td><p>Computes the bitwise OR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_xor#torch.bitwise_xor" title="torch.bitwise_xor"><code>bitwise_xor</code></a>
</td> <td><p>Computes the bitwise XOR of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_left_shift#torch.bitwise_left_shift" title="torch.bitwise_left_shift"><code>bitwise_left_shift</code></a>
</td> <td><p>Computes the left arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.bitwise_right_shift#torch.bitwise_right_shift" title="torch.bitwise_right_shift"><code>bitwise_right_shift</code></a>
</td> <td><p>Computes the right arithmetic shift of <code>input</code> by <code>other</code> bits.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>div</code></a>
</td> <td><p>Divides each element of the input <code>input</code> by the corresponding element of <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.divide#torch.divide" title="torch.divide"><code>divide</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>torch.div()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.floor_divide#torch.floor_divide" title="torch.floor_divide"><code>floor_divide</code></a>
</td> <td></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmod#torch.fmod" title="torch.fmod"><code>fmod</code></a>
</td> <td><p>Applies C++'s <a class="reference external" href="https://en.cppreference.com/w/cpp/numeric/math/fmod">std::fmod</a> entrywise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp#torch.logaddexp" title="torch.logaddexp"><code>logaddexp</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp2#torch.logaddexp2" title="torch.logaddexp2"><code>logaddexp2</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mul#torch.mul" title="torch.mul"><code>mul</code></a>
</td> <td><p>Multiplies <code>input</code> by <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.multiply#torch.multiply" title="torch.multiply"><code>multiply</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.mul#torch.mul" title="torch.mul"><code>torch.mul()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.nextafter#torch.nextafter" title="torch.nextafter"><code>nextafter</code></a>
</td> <td><p>Return the next floating-point value after <code>input</code> towards <code>other</code>, elementwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.remainder#torch.remainder" title="torch.remainder"><code>remainder</code></a>
</td> <td><p>Computes <a class="reference external" href="https://docs.python.org/3/reference/expressions.html#binary-arithmetic-operations">Python's modulus operation</a> entrywise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.sub#torch.sub" title="torch.sub"><code>sub</code></a>
</td> <td><p>Subtracts <code>other</code>, scaled by <code>alpha</code>, from <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.subtract#torch.subtract" title="torch.subtract"><code>subtract</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.sub#torch.sub" title="torch.sub"><code>torch.sub()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.true_divide#torch.true_divide" title="torch.true_divide"><code>true_divide</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.div#torch.div" title="torch.div"><code>torch.div()</code></a> with <code>rounding_mode=None</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.eq#torch.eq" title="torch.eq"><code>eq</code></a>
</td> <td><p>Computes element-wise equality</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ne#torch.ne" title="torch.ne"><code>ne</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo mathvariant="normal">≠</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \neq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.le#torch.le" title="torch.le"><code>le</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≤</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \leq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ge#torch.ge" title="torch.ge"><code>ge</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>≥</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} \geq \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.greater#torch.greater" title="torch.greater"><code>greater</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.gt#torch.gt" title="torch.gt"><code>torch.gt()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.greater_equal#torch.greater_equal" title="torch.greater_equal"><code>greater_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.ge#torch.ge" title="torch.ge"><code>torch.ge()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.gt#torch.gt" title="torch.gt"><code>gt</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&gt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &gt; \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.less_equal#torch.less_equal" title="torch.less_equal"><code>less_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.le#torch.le" title="torch.le"><code>torch.le()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.lt#torch.lt" title="torch.lt"><code>lt</code></a>
</td> <td><p>Computes <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>input</mtext><mo>&lt;</mo><mtext>other</mtext></mrow><annotation encoding="application/x-tex">\text{input} &lt; \text{other}</annotation></semantics></math></span></span></span> element-wise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.less#torch.less" title="torch.less"><code>less</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.lt#torch.lt" title="torch.lt"><code>torch.lt()</code></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.maximum#torch.maximum" title="torch.maximum"><code>maximum</code></a>
</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.minimum#torch.minimum" title="torch.minimum"><code>minimum</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmax#torch.fmax" title="torch.fmax"><code>fmax</code></a>
</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmin#torch.fmin" title="torch.fmin"><code>fmin</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.not_equal#torch.not_equal" title="torch.not_equal"><code>not_equal</code></a>
</td> <td><p>Alias for <a class="reference internal" href="generated/torch.ne#torch.ne" title="torch.ne"><code>torch.ne()</code></a>.</p></td> </tr>  </table> <p>The available inplace binary operators are all of the above <strong>except</strong>:</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp#torch.logaddexp" title="torch.logaddexp"><code>logaddexp</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.logaddexp2#torch.logaddexp2" title="torch.logaddexp2"><code>logaddexp2</code></a>
</td> <td><p>Logarithm of the sum of exponentiations of the inputs in base-2.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.equal#torch.equal" title="torch.equal"><code>equal</code></a>
</td> <td><p><code>True</code> if two tensors have the same size and elements, <code>False</code> otherwise.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmin#torch.fmin" title="torch.fmin"><code>fmin</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.minimum#torch.minimum" title="torch.minimum"><code>minimum</code></a>
</td> <td><p>Computes the element-wise minimum of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.fmax#torch.fmax" title="torch.fmax"><code>fmax</code></a>
</td> <td><p>Computes the element-wise maximum of <code>input</code> and <code>other</code>.</p></td> </tr>  </table>   <h3 id="reductions">Reductions</h3> <p>The following reductions are available (with autograd support). For more information, the <a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_overview.html">Overview</a> tutorial details some examples of reductions, while the <a class="reference external" href="https://pytorch.org/tutorials/prototype/maskedtensor_advanced_semantics.html">Advanced semantics</a> tutorial has some further in-depth discussions about how we decided on certain reduction semantics.</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.sum#torch.sum" title="torch.sum"><code>sum</code></a>
</td> <td><p>Returns the sum of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.mean#torch.mean" title="torch.mean"><code>mean</code></a>
</td> <td><p>Returns the mean value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.amin#torch.amin" title="torch.amin"><code>amin</code></a>
</td> <td><p>Returns the minimum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.amax#torch.amax" title="torch.amax"><code>amax</code></a>
</td> <td><p>Returns the maximum value of each slice of the <code>input</code> tensor in the given dimension(s) <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.argmin#torch.argmin" title="torch.argmin"><code>argmin</code></a>
</td> <td><p>Returns the indices of the minimum value(s) of the flattened tensor or along a dimension</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.argmax#torch.argmax" title="torch.argmax"><code>argmax</code></a>
</td> <td><p>Returns the indices of the maximum value of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.prod#torch.prod" title="torch.prod"><code>prod</code></a>
</td> <td><p>Returns the product of all elements in the <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.all#torch.all" title="torch.all"><code>all</code></a>
</td> <td><p>Tests if all elements in <code>input</code> evaluate to <code>True</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.norm#torch.norm" title="torch.norm"><code>norm</code></a>
</td> <td><p>Returns the matrix norm or vector norm of a given tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.var#torch.var" title="torch.var"><code>var</code></a>
</td> <td><p>Calculates the variance over the dimensions specified by <code>dim</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.std#torch.std" title="torch.std"><code>std</code></a>
</td> <td><p>Calculates the standard deviation over the dimensions specified by <code>dim</code>.</p></td> </tr>  </table>   <h3 id="view-and-select-functions">View and select functions</h3> <p>We’ve included a number of view and select functions as well; intuitively, these operators will apply to both the data and the mask and then wrap the result in a <code>MaskedTensor</code>. For a quick example, consider <a class="reference internal" href="generated/torch.select#torch.select" title="torch.select"><code>select()</code></a>:</p> <pre data-language="python">&gt;&gt;&gt; data = torch.arange(12, dtype=torch.float).reshape(3, 4)
&gt;&gt;&gt; data
tensor([[ 0.,  1.,  2.,  3.],
        [ 4.,  5.,  6.,  7.],
        [ 8.,  9., 10., 11.]])
&gt;&gt;&gt; mask = torch.tensor([[True, False, False, True], [False, True, False, False], [True, True, True, True]])
&gt;&gt;&gt; mt = masked_tensor(data, mask)
&gt;&gt;&gt; data.select(0, 1)
tensor([4., 5., 6., 7.])
&gt;&gt;&gt; mask.select(0, 1)
tensor([False,  True, False, False])
&gt;&gt;&gt; mt.select(0, 1)
MaskedTensor(
  [      --,   5.0000,       --,       --]
)
</pre> <p>The following ops are currently supported:</p> <table class="autosummary longtable docutils colwidths-auto align-default">  <tr>
<td>


<a class="reference internal" href="generated/torch.atleast_1d#torch.atleast_1d" title="torch.atleast_1d"><code>atleast_1d</code></a>
</td> <td><p>Returns a 1-dimensional view of each input tensor with zero dimensions.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.broadcast_tensors#torch.broadcast_tensors" title="torch.broadcast_tensors"><code>broadcast_tensors</code></a>
</td> <td><p>Broadcasts the given tensors according to <a class="reference internal" href="https://pytorch.org/docs/2.1/notes/broadcasting.html#broadcasting-semantics"><span class="std std-ref">Broadcasting semantics</span></a>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.broadcast_to#torch.broadcast_to" title="torch.broadcast_to"><code>broadcast_to</code></a>
</td> <td><p>Broadcasts <code>input</code> to the shape <code>shape</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.cat#torch.cat" title="torch.cat"><code>cat</code></a>
</td> <td><p>Concatenates the given sequence of <code>seq</code> tensors in the given dimension.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.chunk#torch.chunk" title="torch.chunk"><code>chunk</code></a>
</td> <td><p>Attempts to split a tensor into the specified number of chunks.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.column_stack#torch.column_stack" title="torch.column_stack"><code>column_stack</code></a>
</td> <td><p>Creates a new tensor by horizontally stacking the tensors in <code>tensors</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.dsplit#torch.dsplit" title="torch.dsplit"><code>dsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with three or more dimensions, into multiple tensors depthwise according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.flatten#torch.flatten" title="torch.flatten"><code>flatten</code></a>
</td> <td><p>Flattens <code>input</code> by reshaping it into a one-dimensional tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hsplit#torch.hsplit" title="torch.hsplit"><code>hsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with one or more dimensions, into multiple tensors horizontally according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.hstack#torch.hstack" title="torch.hstack"><code>hstack</code></a>
</td> <td><p>Stack tensors in sequence horizontally (column wise).</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.kron#torch.kron" title="torch.kron"><code>kron</code></a>
</td> <td><p>Computes the Kronecker product, denoted by <span class="math"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>⊗</mo></mrow><annotation encoding="application/x-tex">\otimes</annotation></semantics></math></span></span></span>, of <code>input</code> and <code>other</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.meshgrid#torch.meshgrid" title="torch.meshgrid"><code>meshgrid</code></a>
</td> <td><p>Creates grids of coordinates specified by the 1D inputs in <code>attr</code>:tensors.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.narrow#torch.narrow" title="torch.narrow"><code>narrow</code></a>
</td> <td><p>Returns a new tensor that is a narrowed version of <code>input</code> tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.ravel#torch.ravel" title="torch.ravel"><code>ravel</code></a>
</td> <td><p>Return a contiguous flattened tensor.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.select#torch.select" title="torch.select"><code>select</code></a>
</td> <td><p>Slices the <code>input</code> tensor along the selected dimension at the given index.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.split#torch.split" title="torch.split"><code>split</code></a>
</td> <td><p>Splits the tensor into chunks.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.t#torch.t" title="torch.t"><code>t</code></a>
</td> <td><p>Expects <code>input</code> to be &lt;= 2-D tensor and transposes dimensions 0 and 1.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.transpose#torch.transpose" title="torch.transpose"><code>transpose</code></a>
</td> <td><p>Returns a tensor that is a transposed version of <code>input</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vsplit#torch.vsplit" title="torch.vsplit"><code>vsplit</code></a>
</td> <td><p>Splits <code>input</code>, a tensor with two or more dimensions, into multiple tensors vertically according to <code>indices_or_sections</code>.</p></td> </tr> <tr>
<td>


<a class="reference internal" href="generated/torch.vstack#torch.vstack" title="torch.vstack"><code>vstack</code></a>
</td> <td><p>Stack tensors in sequence vertically (row wise).</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.tensor.expand#torch.Tensor.expand" title="torch.Tensor.expand"><code>Tensor.expand</code></a></p></td> <td><p>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.tensor.expand_as#torch.Tensor.expand_as" title="torch.Tensor.expand_as"><code>Tensor.expand_as</code></a></p></td> <td><p>Expand this tensor to the same size as <code>other</code>.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.tensor.reshape#torch.Tensor.reshape" title="torch.Tensor.reshape"><code>Tensor.reshape</code></a></p></td> <td><p>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.tensor.reshape_as#torch.Tensor.reshape_as" title="torch.Tensor.reshape_as"><code>Tensor.reshape_as</code></a></p></td> <td><p>Returns this tensor as the same shape as <code>other</code>.</p></td> </tr> <tr>
<td><p><a class="reference internal" href="generated/torch.tensor.view#torch.Tensor.view" title="torch.Tensor.view"><code>Tensor.view</code></a></p></td> <td><p>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</p></td> </tr>  </table><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/masked.html" class="_attribution-link">https://pytorch.org/docs/2.1/masked.html</a>
  </p>
</div>
