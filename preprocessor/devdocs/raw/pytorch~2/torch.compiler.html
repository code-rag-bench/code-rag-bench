<h1 id="torch-compiler-overview">torch.compiler</h1> <p id="torch-compiler"><code>torch.compiler</code> is a namespace through which some of the internal compiler methods are surfaced for user consumption. The main function and the feature in this namespace is <code>torch.compile</code>.</p> <p><code>torch.compile</code> is a PyTorch function introduced in PyTorch 2.x that aims to solve the problem of accurate graph capturing in PyTorch and ultimately enable software engineers to run their PyTorch programs faster. <code>torch.compile</code> is written in Python and it marks the transition of PyTorch from C++ to Python.</p> <p><code>torch.compile</code> leverages the following underlying technologies:</p> <ul class="simple"> <li>
<strong>TorchDynamo (torch._dynamo)</strong> is an internal API that uses a CPython feature called the Frame Evaluation API to safely capture PyTorch graphs. Methods that are available externally for PyTorch users are surfaced through the <code>torch.compiler</code> namespace.</li> <li>
<strong>TorchInductor</strong> is the default <code>torch.compile</code> deep learning compiler that generates fast code for multiple accelerators and backends. You need to use a backend compiler to make speedups through <code>torch.compile</code> possible. For NVIDIA and AMD GPUs, it leverages OpenAI Triton as the key building block.</li> <li>
<strong>AOT Autograd</strong> captures not only the user-level code, but also backpropagation, which results in capturing the backwards pass “ahead-of-time”. This enables acceleration of both forwards and backwards pass using TorchInductor.</li> </ul> <div class="admonition note"> <p class="admonition-title">Note</p> <p>In some cases, the terms <code>torch.compile</code>, TorchDynamo, <code>torch.compiler</code> might be used interchangeably in this documentation.</p> </div> <p>As mentioned above, to run your workflows faster, <code>torch.compile</code> through TorchDynamo requires a backend that converts the captured graphs into a fast machine code. Different backends can result in various optimization gains. The default backend is called TorchInductor, also known as <em>inductor</em>, TorchDynamo has a list of supported backends developed by our partners, which can be see by running <code>torch.compiler.list_backends()</code> each of which with its optional dependencies.</p> <p>Some of the most commonly used backends include:</p> <p><strong>Training &amp; inference backends</strong></p> <table class="colwidths-given docutils colwidths-auto align-default">  <thead> <tr>
<th class="head"><p>Backend</p></th> <th class="head"><p>Description</p></th> </tr> </thead>  <tr>
<td><p><code>torch.compile(m, backend="inductor")</code></p></td> <td><p>Uses the TorchInductor backend. <a class="reference external" href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">Read more</a></p></td> </tr> <tr>
<td><p><code>torch.compile(m, backend="cudagraphs")</code></p></td> <td><p>CUDA graphs with AOT Autograd. <a class="reference external" href="https://github.com/pytorch/torchdynamo/pull/757">Read more</a></p></td> </tr> <tr>
<td><p><code>torch.compile(m, backend="ipex")</code></p></td> <td><p>Uses IPEX on CPU. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Read more</a></p></td> </tr> <tr>
<td><p><code>torch.compile(m, backend="onnxrt")</code></p></td> <td><p>Uses ONNX Runtime for training on CPU/GPU. <a class="reference internal" href="onnx_dynamo_onnxruntime_backend"><span class="doc">Read more</span></a></p></td> </tr>  </table> <p><strong>Inference-only backends</strong></p> <table class="colwidths-given docutils colwidths-auto align-default">  <thead> <tr>
<th class="head"><p>Backend</p></th> <th class="head"><p>Description</p></th> </tr> </thead>  <tr>
<td><p><code>torch.compile(m, backend="tensorrt")</code></p></td> <td><p>Uses ONNX Runtime to run TensorRT for inference optimizations. <a class="reference external" href="https://github.com/onnx/onnx-tensorrt">Read more</a></p></td> </tr> <tr>
<td><p><code>torch.compile(m, backend="ipex")</code></p></td> <td><p>Uses IPEX for inference on CPU. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Read more</a></p></td> </tr> <tr>
<td><p><code>torch.compile(m, backend="tvm")</code></p></td> <td><p>Uses Apache TVM for inference optimizations. <a class="reference external" href="https://tvm.apache.org/">Read more</a></p></td> </tr>  </table>  <h2 id="read-more">Read More</h2>  <p class="caption" role="heading"><span class="caption-text">Getting Started for PyTorch Users</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_get_started">Getting Started</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_api">torch.compiler API reference</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_performance_dashboard">PyTorch 2.0 Performance Dashboard</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_fine_grain_apis">TorchDynamo APIs for fine-grained tracing</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_inductor_profiling">TorchInductor GPU Profiling</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_profiling_torch_compile">Profiling to understand torch.compile performance</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_faq">Frequently Asked Questions</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_troubleshooting">PyTorch 2.0 Troubleshooting</a></li> </ul>   <p class="caption" role="heading"><span class="caption-text">Deep Dive for PyTorch Developers</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_deepdive">TorchDynamo Deep Dive</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_guards_overview">Guards Overview</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_dynamic_shapes">Dynamic shapes</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_nn_module">PyTorch 2.0 NNModule Support</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_best_practices_for_backends">Best Practices for Backends</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_cudagraph_trees">CUDAGraph Trees</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_fake_tensor">Fake tensor</a></li> </ul>   <p class="caption" role="heading"><span class="caption-text">HowTo for PyTorch Backend Vendors</span></p> <ul> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_custom_backends">Custom Backends</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_transformations">Writing Graph Transformations on ATen IR</a></li> <li class="toctree-l1"><a class="reference internal" href="torch.compiler_ir">IRs</a></li> </ul><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.compiler.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.compiler.html</a>
  </p>
</div>
