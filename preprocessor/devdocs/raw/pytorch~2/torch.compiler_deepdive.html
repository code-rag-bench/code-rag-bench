<h1 id="torchdynamo-deep-dive">TorchDynamo Deep Dive</h1> <p>Before you read this section, read <a class="reference internal" href="torch.compiler#torch-compiler-overview"><span class="std std-ref">torch.compiler</span></a>.</p> <p><strong>TorchDynamo</strong> is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation API in CPython (<a class="reference external" href="https://peps.python.org/pep-0523/">PEP 523</a>) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">FX Graph</a> which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds — usability and performance.</p> <p>TorchDynamo makes it easy to experiment with different compiler backends to make PyTorch code faster with a single line decorator <code>torch._dynamo.optimize()</code> which is wrapped for convenience by <code>torch.compile()</code></p> <p>The following diagram demonstrates how PyTorch works with <code>torch.compile</code> and without it:</p> <img alt="_images/TorchDynamo.png" src="https://pytorch.org/docs/2.1/_images/TorchDynamo.png"> <p><code>TorchInductor</code> is one of the backends supported by <a class="reference external" href="https://pytorch.org/docs/stable/fx.html">TorchDynamo Graph</a> into <a class="reference external" href="https://github.com/openai/triton">Triton</a> for GPUs or <a class="reference external" href="https://www.openmp.org/">C++/OpenMP</a> for CPUs. We have a <a class="reference external" href="https://github.com/pytorch/torchdynamo/issues/681#issuecomment-1233828468">training performance dashboard</a> that provides performance comparison for different training backends. You can read more in the <a class="reference external" href="https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747">TorchInductor post on PyTorch dev-discuss</a>.</p> <p>For an in-depth overview, read the sections below, watch the deep-dive video, and check out the dev-discuss topics.</p>  <ul class="simple"> <li><a class="reference external" href="https://www.youtube.com/watch?v=egZB5Uxki0I">TorchDynamo deep-dive video</a></li> <li><a class="reference external" href="https://dev-discuss.pytorch.org/search?q=TorchDynamo%20order%3Alatest">dev-discuss topics</a></li> </ul>   <h2 id="torchdynamo-internals">TorchDynamo Internals</h2> <p><strong>Author</strong>: <a class="reference external" href="https://github.com/jansel">Jason Ansel</a> and <a class="reference external" href="https://github.com/youkaichao">Kaichao You</a></p> <p>This section will go over some of the TorchDynamo internals and will demonstrate how TorchDynamo works under the hood.</p>  <h3 id="what-is-a-guard">What is a guard?</h3> <p>TorchDynamo operates just-in-time and specializes graphs based on dynamic properties. Below is a basic example of how to use TorchDynamo. One can decorate a function or a method using <code>torchdynamo.optimize</code> to enable TorchDynamo optimization:</p> <pre data-language="python">from typing import List
import torch
from torch import _dynamo as torchdynamo
def my_compiler(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):
    print("my_compiler() called with FX graph:")
    gm.graph.print_tabular()
    return gm.forward  # return a python callable

@torchdynamo.optimize(my_compiler)
def toy_example(a, b):
    x = a / (torch.abs(a) + 1)
    if b.sum() &lt; 0:
        b = b * -1
    return x * b
for _ in range(100):
    toy_example(torch.randn(10), torch.randn(10))
</pre> <p>For example, the first graph above has the following guards:</p> <pre data-language="python">GUARDS:
 - local 'a' TENSOR_MATCH
 - local 'b' TENSOR_MATCH
 - global 'torch' FUNCTION_MATCH
</pre> <p>If any of those guards fail, the graph will be recaptured and recompiled. The interesting guard type there is <code>TENSOR_MATCH</code>, which checks the following <code>torch.Tensor</code> properties:</p> <ul class="simple"> <li>Python class of the tensor (tensor subclassing, etc)</li> <li>dtype</li> <li>device</li> <li>requires_grad</li> <li>dispatch_key (with thread-local includes/excludes applied)</li> <li>ndim</li> <li>sizes*</li> <li>strides*</li> </ul> <p>The full specialization mode allows the backend compiler to assume an entirely static graph. Unfortunately, most backends require this. Operators which return dynamic shapes will trigger a graph break when not in dynamic shape mode.</p>   <h3 id="what-is-dynamo-doing">What is Dynamo doing?</h3> <p>If you want to understand better what TorchDynamo is doing, you can set:</p> <pre data-language="python">import torch._dynamo.config
import logging

torch._dynamo.config.log_level = logging.INFO
torch._dynamo.config.output_code = True
</pre> <p>This code triggers useful (but spammy) printouts.</p> <p>For example, the printouts for the first graph in the <code>toy_example</code> are:</p> <pre data-language="python">__compiled_fn_0 &lt;eval_with_key&gt;.1
opcode         name     target                                                  args              kwargs
-------------  -------  ------------------------------------------------------  ----------------  --------
placeholder    a        a                                                       ()                {}
placeholder    b        b                                                       ()                {}
call_function  abs_1    &lt;built-in method abs of type object at 0x7f9ca082f8a0&gt;  (a,)              {}
call_function  add      &lt;built-in function add&gt;                                 (abs_1, 1)        {}
call_function  truediv  &lt;built-in function truediv&gt;                             (a, add)          {}
call_method    sum_1    sum                                                     (b,)              {}
call_function  lt       &lt;built-in function lt&gt;                                  (sum_1, 0)        {}
output         output   output                                                  ((truediv, lt),)  {}

ORIGINAL BYTECODE toy_example example.py 9
 10           0 LOAD_FAST                0 (a)
              2 LOAD_GLOBAL              0 (torch)
              4 LOAD_METHOD              1 (abs)
              6 LOAD_FAST                0 (a)
              8 CALL_METHOD              1
             10 LOAD_CONST               1 (1)
             12 BINARY_ADD
             14 BINARY_TRUE_DIVIDE
             16 STORE_FAST               2 (x)

 11          18 LOAD_FAST                1 (b)
             20 LOAD_METHOD              2 (sum)
             22 CALL_METHOD              0
             24 LOAD_CONST               2 (0)
             26 COMPARE_OP               0 (&lt;)
             28 POP_JUMP_IF_FALSE       38

 12          30 LOAD_FAST                1 (b)
             32 LOAD_CONST               3 (-1)
             34 BINARY_MULTIPLY
             36 STORE_FAST               1 (b)

 13     &gt;&gt;   38 LOAD_FAST                2 (x)
             40 LOAD_FAST                1 (b)
             42 BINARY_MULTIPLY
             44 RETURN_VALUE

MODIFIED BYTECODE
  9           0 LOAD_GLOBAL              3 (__compiled_fn_0)
              2 LOAD_FAST                0 (a)
              4 LOAD_FAST                1 (b)
              6 CALL_FUNCTION            2
              8 UNPACK_SEQUENCE          2
             10 STORE_FAST               2 (x)
             12 POP_JUMP_IF_FALSE       24
             14 LOAD_GLOBAL              4 (__resume_at_30_1)
             16 LOAD_FAST                1 (b)
             18 LOAD_FAST                2 (x)
             20 CALL_FUNCTION            2
             22 RETURN_VALUE
        &gt;&gt;   24 LOAD_GLOBAL              5 (__resume_at_38_2)
             26 LOAD_FAST                1 (b)
             28 LOAD_FAST                2 (x)
             30 CALL_FUNCTION            2
             32 RETURN_VALUE

GUARDS:
 - local 'a' TENSOR_MATCH
 - local 'b' TENSOR_MATCH
 - global 'torch' FUNCTION_MATCH
</pre> <p>At the top you can see the FX graph. Next, you see the original bytecode of the function, followed by the modified bytecode generated by TorchDynamo. Finally, you see the guards which we covered above.</p> <p>In the modified bytecode, <code>__compiled_fn_0</code> is the return value of <code>my_compiler()</code> (the compiled graph). <code>__resume_at_30_1</code> and <code>__resume_at_38_2</code> are both generated continuation functions that pick up execution after a graph break (at bytecode offsets 30 and 38). Each of these functions take the form:</p> <pre data-language="python">__resume_at_&lt;offset&gt;:
    ... restore stack state if needed ...
    JUMP_ABSOLUTE &lt;offset&gt; into toy_example
    ... original bytecode of toy_example ...
</pre> <p>By generating this <code>resume_at</code> function, we force the remainder of the function to be executed in a new Python frame which recursively triggers TorchDynamo to restart its capture once execution reaches that point for the first time.</p>   <h3 id="how-to-inspect-artifacts-generated-by-torchdynamo">How to inspect artifacts generated by TorchDynamo?</h3> <p>To inspect the artifacts generated by TorchDynamo, there is an API <code>torch._dynamo.eval_frame._debug_get_cache_entry_list</code> that retrieves compiled code and guards out of a function’s <code>__code__</code> object. A compiled function can have several cache entries, and each cache entry consists a generated function to check guards, and a <code>types.CodeType</code> object to keep the code to be executed if the guarding conditions are satisfied.</p> <pre data-language="python">from torch._dynamo.eval_frame import _debug_get_cache_entry_list
cache_entries = _debug_get_cache_entry_list(toy_example._torchdynamo_orig_callable.__code__)
guard, code = cache_entries[0]
# the guard takes an input frame, and tells whether a re-compilation should be triggered.
import inspect
print(inspect.getfullargspec(guard))
# if you know python bytecode, you can understand the following code.
import dis
dis.dis(guard)
dis.dis(code)
</pre> <p>The compiled bytecode, printed by <code>dis.dis(code)</code>, will call the result of the backend compiler function which is stored inside a global variable such as <code>__compiled_fn_0</code> in the module containing the original function.</p> <p>The generated bytecodes are roughly equivalent to the following Python (converted manually for illustration purposes).</p> <pre data-language="python">def compiled_example(a, b):
    # behind the scene, pytorch C code checks the guarding condition
    # if all guard fails, trigger re-compile
    # else, run the compiled code
    # after some setup work, the code finally looks like the following
    x, b_sum_less_than_0 = __compiled_fn_0._torchdynamo_orig_callable(a, b)
    # the condition test on tensor value leads to graph break here
    # we use python interpreter to select the branch
    # depending on the value, the rest graph is either `__resume_at_30_1`
    # or `__resume_at_38_2`
    if b_sum_less_than_0:
        return __resume_at_30_1(b, x)
     return __resume_at_38_2(b, x)

def __resume_at_38_2(b, x):
    return x * b

def __resume_at_30_1(b, x):
    b = b * -1
    return x * b

def fn(a, b):
    x = a / (torch.abs(a) + 1)
    lt = b.sum() &lt; 0
    return x, lt

__compiled_fn_0._torchdynamo_orig_callable = fn
</pre> <p>Note that we pass a simple <code>my_compiler</code> function as the backend compiler, therefore the subgraph code <code>__resume_at_38_2</code>, <code>__resume_at_30_1</code>, and <code>__compiled_fn_0._torchdynamo_orig_callable</code> remain python code. However, if we use other backends like the built-in <code>inductor</code>, the subgraph code will be compiled CUDA kernels for GPU or C++ code for CPU.</p><div class="_attribution">
  <p class="_attribution-p">
    &copy; 2024, PyTorch Contributors<br>PyTorch has a BSD-style license, as found in the <a href="https://github.com/pytorch/pytorch/blob/main/LICENSE">LICENSE</a> file.<br>
    <a href="https://pytorch.org/docs/2.1/torch.compiler_deepdive.html" class="_attribution-link">https://pytorch.org/docs/2.1/torch.compiler_deepdive.html</a>
  </p>
</div>
