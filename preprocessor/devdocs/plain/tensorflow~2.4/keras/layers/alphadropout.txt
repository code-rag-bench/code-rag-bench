# tf.keras.layers.AlphaDropout

View source on GitHub  Applies Alpha Dropout to the input. Inherits From:
`Layer`, `Module`

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.keras.layers.AlphaDropout`

    
    tf.keras.layers.AlphaDropout(
        rate, noise_shape=None, seed=None, **kwargs
    )
    
Alpha Dropout is a `Dropout` that keeps mean and variance of inputs to their original values, in order to ensure the self-normalizing property even after this dropout. Alpha Dropout fits well to Scaled Exponential Linear Units by randomly setting activations to the negative saturation value. | Arguments  
---  
`rate` |  float, drop probability (as with `Dropout`). The multiplicative noise will have standard deviation `sqrt(rate / (1 - rate))`.   
`seed` |  A Python integer to use as random seed.   
#### Call arguments:

  * `inputs`: Input tensor (of any rank).
  * `training`: Python boolean indicating whether the layer should behave in training mode (adding dropout) or in inference mode (doing nothing).

#### Input shape:

Arbitrary. Use the keyword argument `input_shape` (tuple of integers, does not
include the samples axis) when using this layer as the first layer in a model.

#### Output shape:

Same shape as input.

Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/keras/layers/AlphaDropout

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

