# tf.tensor_scatter_nd_update

View source on GitHub  "Scatter `updates` into an existing tensor according to
`indices`.

#### View aliases

Compat aliases for migration See Migration guide for more details.
`tf.compat.v1.tensor_scatter_nd_update`, `tf.compat.v1.tensor_scatter_update`

    
    tf.tensor_scatter_nd_update(
        tensor, indices, updates, name=None
    )
    
This operation creates a new tensor by applying sparse `updates` to the input
`tensor`. This is similar to an index assignment.

    
    # Not implemented: tensors cannot be updated inplace.
    tensor[indices] = updates
    
If an out of bound index is found on CPU, an error is returned.

> Warning: There are some GPU specific semantics for this operation.
>   * If an out of bound index is found, the index is ignored.
>   * The order in which updates are applied is nondeterministic, so the
> output will be nondeterministic if `indices` contains duplicates.
>

This operation is very similar to `tf.scatter_nd`, except that the updates are
scattered onto an existing tensor (as opposed to a zero-tensor). If the memory
for the existing tensor cannot be re-used, a copy is made and updated.

#### In general:

  * `indices` is an integer tensor - the indices to update in `tensor`.
  * `indices` has at least two axes, the last axis is the depth of the index vectors.
  * For each index vector in `indices` there is a corresponding entry in `updates`.
  * If the length of the index vectors matches the rank of the `tensor`, then the index vectors each point to scalars in `tensor` and each update is a scalar.
  * If the length of the index vectors is less than the rank of `tensor`, then the index vectors each point to slices of `tensor` and shape of the updates must match that slice.

Overall this leads to the following shape constraints:

    
    assert tf.rank(indices) >= 2
    index_depth = indices.shape[-1]
    batch_shape = indices.shape[:-1]
    assert index_depth <= tf.rank(tensor)
    outer_shape = tensor.shape[:index_depth]
    inner_shape = tensor.shape[index_depth:]
    assert updates.shape == batch_shape + inner_shape
    
Typical usage is often much simpler than this general form, and it can be
better understood starting with simple examples:

### Scalar updates

The simplest usage inserts scalar elements into a tensor by index. In this
case, the `index_depth` must equal the rank of the input `tensor`, slice each
column of `indices` is an index into an axis of the input `tensor`. In this
simplest case the shape constraints are:

    
    num_updates, index_depth = indices.shape.as_list()
    assert updates.shape == [num_updates]
    assert index_depth == tf.rank(tensor)`
    
For example, to insert 4 scattered elements in a rank-1 tensor with 8
elements.
![](data:image/png;base64,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)
This scatter operation would look like this:

    
    tensor = [0, 0, 0, 0, 0, 0, 0, 0]    # tf.rank(tensor) == 1
    indices = [[1], [3], [4], [7]]       # num_updates == 4, index_depth == 1
    updates = [9, 10, 11, 12]            # num_updates == 4
    print(tf.tensor_scatter_nd_update(tensor, indices, updates))
    tf.Tensor([ 0 9  0 10  11  0  0 12], shape=(8,), dtype=int32)
    
The length (first axis) of `updates` must equal the length of the `indices`:
`num_updates`. This is the the number of updates being inserted. Each scalar
update is inserted into `tensor` at the indexed location. For a higher rank
input `tensor` scalar updates can be inserted by using an `index_depth` that
matches `tf.rank(tensor)`:

    
    tensor = [[1, 1], [1, 1], [1, 1]]    # tf.rank(tensor) == 2
    indices = [[0, 1], [2, 0]]           # num_updates == 2, index_depth == 2
    updates = [5, 10]                    # num_updates == 2
    print(tf.tensor_scatter_nd_update(tensor, indices, updates))
    tf.Tensor(
        [[ 1  5]
         [ 1  1]
         [10  1]], shape=(3, 2), dtype=int32)
    
### Slice updates

When the input `tensor` has more than one axis scatter can be used to update
entire slices. In this case it's helpful to think of the input `tensor` as
being a two level array-of-arrays. The shape of this two level array is split
into the `outer_shape` and the `inner_shape`. `indices` indexes into the outer
level of the input tensor (`outer_shape`). and replaces the sub-array at that
location with the coresponding item from the `updates` list. The shape of each
update is `inner_shape`. When updating a list of slices the shape constraints
are:

    
    num_updates, index_depth = indices.shape.as_list()
    inner_shape = tensor.shape[:index_depth]
    outer_shape = tensor.shape[index_depth:]
    assert updates.shape == [num_updates, inner_shape]
    
For example, to update rows of a `(6, 3)` `tensor`:

    
    tensor = tf.zeros([6, 3], dtype=tf.int32)
    
Use an index depth of one.

    
    indices = tf.constant([[2], [4]])     # num_updates == 2, index_depth == 1
    num_updates, index_depth = indices.shape.as_list()
    
The `outer_shape` is `6`, the inner shape is `3`:

    
    outer_shape = tensor.shape[:index_depth]
    inner_shape = tensor.shape[index_depth:]
    
2 rows are being indexed so 2 `updates` must be supplied. Each update must be
shaped to match the `inner_shape`.

    
    # num_updates == 2, inner_shape==3
    updates = tf.constant([[1, 2, 3],
                           [4, 5, 6]])
    
Alltogether this gives:

    
    tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
    array([[0, 0, 0],
           [0, 0, 0],
           [1, 2, 3],
           [0, 0, 0],
           [4, 5, 6],
           [0, 0, 0]], dtype=int32)
    
#### More slice update examples

A tensor representing a batch of uniformly sized video clips naturally has 5
axes: `[batch_size, time, width, height, channels]`.

#### For example:

    
    batch_size, time, width, height, channels = 13,11,7,5,3
    video_batch = tf.zeros([batch_size, time, width, height, channels])
    
To replace a selection of video clips:

  * Use an `index_depth` of 1 (indexing the `outer_shape`: `[batch_size]`)
  * Provide updates each with a shape matching the `inner_shape`: `[time, width, height, channels]`.

To relace the first two clips with ones:

    
    indices = [[0],[1]]
    new_clips = tf.ones([2, time, width, height, channels])
    tf.tensor_scatter_nd_update(video_batch, indices, new_clips)
    
To replace a selection of frames in the videos:

  * `indices` must have an `index_depth` of 2 for the `outer_shape`: `[batch_size, time]`.
  * `updates` must be shaped like a list of images. Each update must have a shape, matching the `inner_shape`: `[width, height, channels]`.

To replace the first frame of the first three video clips:

    
    indices = [[0, 0], [1, 0], [2, 0]] # num_updates=3, index_depth=2
    new_images = tf.ones([
      # num_updates=3, inner_shape=(width, height, channels)
      3, width, height, channels])
    tf.tensor_scatter_nd_update(video_batch, indices, new_images)
    
### Folded indices

In simple cases it's convienient to think of `indices` and `updates` as lists,
but this is not a strict requirement. Instead of a flat `num_updates`, the
`indices` and `updates` can be folded into a `batch_shape`. This `batch_shape`
is all axes of the `indices`, except for the innermost `index_depth` axis.

    
    index_depth = indices.shape[-1]
    batch_shape = indices.shape[:-1]
    
> Note: The one exception is that the `batch_shape` cannot be `[]`. You can't
> update a single index by passing indices with shape `[index_depth]`.
`updates` must have a matching `batch_shape` (the axes before `inner_shape`).

    
    assert updates.shape == batch_shape + inner_shape
    
> Note: The result is equivalent to flattening the `batch_shape` axes of
> `indices` and `updates`. This generalization just avoids the need for
> reshapes when it is more natural to construct "folded" indices and updates.
With this generalization the full shape constraints are:

    
    assert tf.rank(indices) >= 2
    index_depth = indices.shape[-1]
    batch_shape = indices.shape[:-1]
    assert index_depth <= tf.rank(tensor)
    outer_shape = tensor.shape[:index_depth]
    inner_shape = tensor.shape[index_depth:]
    assert updates.shape == batch_shape + inner_shape
    
For example, to draw an `X` on a `(5,5)` matrix start with these indices:

    
    tensor = tf.zeros([5,5])
    indices = tf.constant([
     [[0,0],
      [1,1],
      [2,2],
      [3,3],
      [4,4]],
     [[0,4],
      [1,3],
      [2,2],
      [3,1],
      [4,0]],
    ])
    indices.shape.as_list()  # batch_shape == [2, 5], index_depth == 2
    [2, 5, 2]
    
Here the `indices` do not have a shape of `[num_updates, index_depth]`, but a
shape of `batch_shape+[index_depth]`. Since the `index_depth` is equal to the
rank of `tensor`:

  * `outer_shape` is `(5,5)`
  * `inner_shape` is `()` \- each update is scalar
  * `updates.shape` is `batch_shape + inner_shape == (5,2) + ()`

    
    updates = [
      [1,1,1,1,1],
      [1,1,1,1,1],
    ]
    
Putting this together gives:

    
    tf.tensor_scatter_nd_update(tensor, indices, updates).numpy()
    array([[1., 0., 0., 0., 1.],
           [0., 1., 0., 1., 0.],
           [0., 0., 1., 0., 0.],
           [0., 1., 0., 1., 0.],
           [1., 0., 0., 0., 1.]], dtype=float32)
    
| Args  
---  
`tensor` |  Tensor to copy/update.   
`indices` |  Indices to update.   
`updates` |  Updates to apply at the indices.   
`name` |  Optional name for the operation.   
Returns  
---  
A new tensor with the given shape and updates applied according to the
indices.  
Â© 2020 The TensorFlow Authors. All rights reserved.  
Licensed under the Creative Commons Attribution License 3.0.  
Code samples licensed under the Apache 2.0 License.  
https://www.tensorflow.org/versions/r2.4/api_docs/python/tf/tensor_scatter_nd_update

  *[ISP]: Internet Service Provider
  *[LIFO]: last-in, first-out
  *[FIFO]: first-in, first-out

